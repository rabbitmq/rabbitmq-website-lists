<tt>
On&nbsp;Monday,&nbsp;February&nbsp;17,&nbsp;2014,&nbsp;Matthias&nbsp;Radestock&nbsp;&lt;&lt;a&nbsp;href=&quot;mailto:matthias@rabbitmq.com&quot;&gt;matthias@rabbitmq.com&lt;/a&gt;&gt;&nbsp;wrote:&lt;br&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
On&nbsp;17/02/14&nbsp;08:23,&nbsp;Michael&nbsp;Klishin&nbsp;wrote:&lt;br&gt;<br>
&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
If&nbsp;you&nbsp;don&#39;t&nbsp;run&nbsp;out&nbsp;of&nbsp;RAM&nbsp;and&nbsp;your&nbsp;consumers&nbsp;can&nbsp;keep&nbsp;up&nbsp;even&nbsp;with&nbsp;1&lt;br&gt;<br>
queue,&nbsp;what&nbsp;issues&nbsp;do&nbsp;you&nbsp;have?&lt;br&gt;<br>
&lt;/blockquote&gt;<br>
&lt;br&gt;<br>
&gt;From&nbsp;the&nbsp;original&nbsp;post&lt;br&gt;<br>
&lt;quote&gt;&lt;br&gt;<br>
In&nbsp;periods&nbsp;of&nbsp;high&nbsp;utilization&nbsp;of&nbsp;the&nbsp;cluster,&nbsp;we&nbsp;are&nbsp;noticing&nbsp;frequent&nbsp;partitioning.&nbsp;We&nbsp;have&nbsp;narrowed&nbsp;it&nbsp;down&nbsp;to&nbsp;this&nbsp;particular&nbsp;use&nbsp;case&nbsp;[i.e.&nbsp;extremely&nbsp;large&nbsp;queues]&lt;br&gt;<br>
&lt;/quote&gt;&lt;br&gt;<br>
&lt;br&gt;<br>
Matthias&lt;br&gt;<br>
&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Precisely.&nbsp; Sorry&nbsp;for&nbsp;the&nbsp;tangent.&nbsp;I&nbsp;was&nbsp;obsessing&nbsp;over&nbsp;memory&nbsp;all&nbsp;week.&nbsp; &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;We&nbsp;are&nbsp;seeing&nbsp;multiple&nbsp;clusters&nbsp;both versions&nbsp;3.2.3&nbsp;and&nbsp;3.1.5&nbsp;partition&nbsp;regularly.&nbsp;We&nbsp;haven&#39;t&nbsp;had&nbsp;these&nbsp;issues&nbsp;until&nbsp;we&nbsp;started&nbsp;seeing&nbsp;queues&nbsp;larger&nbsp;than&nbsp;1&nbsp;million&nbsp;messages. &lt;/div&gt;<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I&nbsp;realize&nbsp;this&nbsp;is...&nbsp;Not&nbsp;a&nbsp;good&nbsp;use&nbsp;of&nbsp;RabbitMQ&nbsp;but&nbsp;it&nbsp;is&nbsp;somewhat&nbsp;a&nbsp;necessity. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;We&nbsp;have&nbsp;a&nbsp;three&nbsp;node&nbsp;cluster&nbsp;(2&nbsp;disk&nbsp;1&nbsp;ram).&nbsp;I&nbsp;have&nbsp;disabled&nbsp;paging&nbsp;messages&nbsp;to&nbsp;disk&nbsp;and&nbsp;set&nbsp;vm&nbsp;high&nbsp;memory&nbsp;watermark&nbsp;at&nbsp;around&nbsp;.75.&nbsp;I&#39;ve&nbsp;chosen&nbsp;a&nbsp;three&nbsp;node&nbsp;configuration&nbsp;simply&nbsp;so&nbsp;I&nbsp;can&nbsp;use&nbsp;pause&nbsp;minority.&nbsp;I&nbsp;have&nbsp;also&nbsp;configured&nbsp;an&nbsp;ha&nbsp;policy&nbsp;with&nbsp;mirroring&nbsp;to&nbsp;all&nbsp;nodes&nbsp;and&nbsp;automatic&nbsp;synchronization. &lt;/div&gt;<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;We&nbsp;have&nbsp;a&nbsp;strong&nbsp;requirement&nbsp;for&nbsp;consistency&nbsp;and&nbsp;partition&nbsp;tolerance.&nbsp;This&nbsp;seemed&nbsp;to&nbsp;be&nbsp;the&nbsp;best&nbsp;option.&nbsp;We&nbsp;can&nbsp;tolerate&nbsp;a&nbsp;full&nbsp;RabbitMQ&nbsp;outage&nbsp;but&nbsp;data&nbsp;loss&nbsp;or&nbsp;inconsistency&nbsp;must&nbsp;be&nbsp;avoided. &lt;/div&gt;&lt;div&gt;<br>
&lt;br&gt;&lt;/div&gt;&lt;div&gt;This&nbsp;configuration&nbsp;seemed&nbsp;the&nbsp;safest&nbsp;but&nbsp;we&nbsp;are&nbsp;still&nbsp;experiencing&nbsp;partitions&nbsp;and&nbsp;while&nbsp;they&nbsp;are&nbsp;easily&nbsp;healed,&nbsp;this&nbsp;now&nbsp;leads&nbsp;to&nbsp;queue&nbsp;in&nbsp;availability&nbsp;for&nbsp;long&nbsp;periods&nbsp;of&nbsp;time&nbsp;while&nbsp;the&nbsp;queues&nbsp;sunchronize.&nbsp;If&nbsp;this&nbsp;were&nbsp;attributable&nbsp;to&nbsp;network&nbsp;outages,&nbsp;I&nbsp;would&nbsp;understand,&nbsp;but&nbsp;the&nbsp;machines&nbsp;are&nbsp;in&nbsp;adjacent&nbsp;racks&nbsp;with&nbsp;redundant&nbsp;links&nbsp;to&nbsp;top&nbsp;or&nbsp;rack&nbsp;switches.&lt;span&gt;&lt;/span&gt;&lt;/div&gt;<br>

</tt>
