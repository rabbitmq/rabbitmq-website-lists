<tt>
&lt;div&nbsp;dir=&quot;ltr&quot;&gt;&lt;div&nbsp;class=&quot;gmail_extra&quot;&gt;&lt;div&nbsp;class=&quot;gmail_quote&quot;&gt;On&nbsp;Fri,&nbsp;Oct&nbsp;4,&nbsp;2013&nbsp;at&nbsp;1:54&nbsp;AM,&nbsp;Tim&nbsp;Watson&nbsp;&lt;span&nbsp;dir=&quot;ltr&quot;&gt;&amp;lt;&lt;a&nbsp;href=&quot;mailto:watson.timothy@gmail.com&quot;&nbsp;target=&quot;_blank&quot;&gt;watson.timothy@gmail.com&lt;/a&gt;&amp;gt;&lt;/span&gt;&nbsp;wrote:&lt;br&gt;<br>
&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;&lt;div&nbsp;class=&quot;im&quot;&gt;&amp;gt;&nbsp;All&nbsp;items&nbsp;below&nbsp;were&nbsp;discovered&nbsp;while&nbsp;deploying&nbsp;3.1.5&nbsp;over&nbsp;the&nbsp;past&nbsp;few&nbsp;days.&nbsp;Hosts&nbsp;in&nbsp;question&nbsp;have&nbsp;24&nbsp;sandy&nbsp;bridge&nbsp;HT&nbsp;cores,&nbsp;64GB&nbsp;of&nbsp;RAM,&nbsp;XFS&nbsp;filesystem,&nbsp;running&nbsp;on&nbsp;CentOS&nbsp;6.&nbsp;Cluster&nbsp;is&nbsp;5&nbsp;nodes,&nbsp;with&nbsp;a&nbsp;default&nbsp;HA&nbsp;policy&nbsp;on&nbsp;all&nbsp;queues&nbsp;of&nbsp;exact/3/automatic-sync.&lt;br&gt;<br>
<br>
&amp;gt;&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;That&amp;#39;s&nbsp;a&nbsp;very&nbsp;strong&nbsp;consistency&nbsp;and&nbsp;redundency&nbsp;guarantee&nbsp;for&nbsp;every&nbsp;queue.&nbsp;Do&nbsp;you&nbsp;really&nbsp;need&nbsp;such&nbsp;strong&nbsp;guarantees&nbsp;for&nbsp;all&nbsp;of&nbsp;them?&nbsp;There&nbsp;is&nbsp;a&nbsp;cost&nbsp;to&nbsp;doing&nbsp;ha.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Yes,&nbsp;it&amp;#39;s&nbsp;very&nbsp;important&nbsp;we&nbsp;never&nbsp;lose&nbsp;a&nbsp;message&nbsp;if&nbsp;it&nbsp;gets&nbsp;accepted&nbsp;for&nbsp;delivery.&nbsp;We&amp;#39;re&nbsp;more&nbsp;than&nbsp;willing&nbsp;to&nbsp;pay&nbsp;the&nbsp;overhead&nbsp;in&nbsp;terms&nbsp;of&nbsp;more&nbsp;hardware&nbsp;as&nbsp;necessary.&nbsp;The&nbsp;5&nbsp;node&nbsp;count&nbsp;is&nbsp;just&nbsp;for&nbsp;initial&nbsp;deployment,&nbsp;I&amp;#39;d&nbsp;expect&nbsp;us&nbsp;to&nbsp;double&nbsp;the&nbsp;cluster&nbsp;size&nbsp;every&nbsp;year&nbsp;for&nbsp;the&nbsp;next&nbsp;3-5&nbsp;years&nbsp;to&nbsp;deal&nbsp;with&nbsp;our&nbsp;workload&nbsp;growth,&nbsp;even&nbsp;counting&nbsp;improvements&nbsp;in&nbsp;individual&nbsp;server&nbsp;processing&nbsp;power.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;expected&nbsp;queues&nbsp;to&nbsp;be&nbsp;distributed&nbsp;evenly&nbsp;among&nbsp;cluster&nbsp;machines,&nbsp;instead&nbsp;got&nbsp;all&nbsp;queues&nbsp;on&nbsp;first&nbsp;3&nbsp;machines&nbsp;in&nbsp;the&nbsp;cluster,&nbsp;nothing&nbsp;on&nbsp;the&nbsp;last&nbsp;2.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Distributed&nbsp;evenly&nbsp;in&nbsp;what&nbsp;regard?&nbsp;Randomly,&nbsp;or&nbsp;based&nbsp;on&nbsp;some&nbsp;metric?&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Doesn&amp;#39;t&nbsp;matter.&nbsp;Random&nbsp;or&nbsp;round&nbsp;robin&nbsp;would&nbsp;be&nbsp;sufficient.&nbsp;We&nbsp;use&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;100s&nbsp;of&nbsp;queues,&nbsp;and&nbsp;so&nbsp;even&nbsp;with&nbsp;~10%&nbsp;having&nbsp;a&nbsp;somewhat&nbsp;higher&nbsp;workload,&nbsp;any&nbsp;distribution&nbsp;scheme&nbsp;would&nbsp;balance&nbsp;the&nbsp;load&nbsp;out&nbsp;between&nbsp;machines&nbsp;reasonably&nbsp;evenly.&lt;br&gt;<br>
&lt;br&gt;&lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;expected&nbsp;message&nbsp;reads&nbsp;from&nbsp;a&nbsp;mirror&nbsp;machine&nbsp;for&nbsp;a&nbsp;queue&nbsp;to&nbsp;do&nbsp;the&nbsp;read&nbsp;i/o&nbsp;locally,&nbsp;so&nbsp;as&nbsp;to&nbsp;spread&nbsp;out&nbsp;workload,&nbsp;but&nbsp;it&nbsp;appears&nbsp;to&nbsp;always&nbsp;go&nbsp;to&nbsp;the&nbsp;host&nbsp;where&nbsp;the&nbsp;queue&nbsp;was&nbsp;created.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;That&amp;#39;s&nbsp;expected&nbsp;behaviour.&nbsp;In&nbsp;a&nbsp;master-slave&nbsp;configuration,&nbsp;writes&nbsp;have&nbsp;to&nbsp;go&nbsp;to&nbsp;the&nbsp;master.&nbsp;Odd&nbsp;though&nbsp;it&nbsp;may&nbsp;sound,&nbsp;reads&nbsp;from&nbsp;a&nbsp;queue&nbsp;involve&nbsp;writes,&nbsp;since&nbsp;we&nbsp;have&nbsp;to&nbsp;do&nbsp;accounting&nbsp;(of&nbsp;e.g.,&nbsp; pending&nbsp;ACKs,&nbsp;position&nbsp;in&nbsp;the&nbsp;queue,&nbsp;etc),&nbsp;so&nbsp;all&nbsp;requests&nbsp;are&nbsp;handled&nbsp;by&nbsp;the&nbsp;master.&lt;br&gt;<br>
&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Yeah,&nbsp;I&nbsp;understand&nbsp;the&nbsp;logic&nbsp;around&nbsp;needing&nbsp;to&nbsp;do&nbsp;queue&nbsp;management&nbsp;and&nbsp;requiring&nbsp;locks&nbsp;and&nbsp;writing.&nbsp;It&nbsp;just&nbsp;doesn&amp;#39;t&nbsp;make&nbsp;sense&nbsp;to&nbsp;me&nbsp;that&nbsp;the&nbsp;read&nbsp;can&amp;#39;t&nbsp;happen&nbsp;locally&nbsp;if&nbsp;the&nbsp;data&nbsp;exists&nbsp;locally,&nbsp;after&nbsp;all&nbsp;appropriate&nbsp;queue&nbsp;locks&nbsp;and&nbsp;bookkeeping&nbsp;have&nbsp;completed.&nbsp;I&nbsp;imagine&nbsp;this&nbsp;is&nbsp;just&nbsp;for&nbsp;code&nbsp;simplicity&nbsp;rather&nbsp;than&nbsp;any&nbsp;technical&nbsp;limitation,&nbsp;and&nbsp;it&amp;#39;s&nbsp;something&nbsp;that&nbsp;really&nbsp;isn&amp;#39;t&nbsp;an&nbsp;issue&nbsp;if&nbsp;we&nbsp;can&nbsp;evenly&nbsp;balance&nbsp;queues&nbsp;between&nbsp;cluster&nbsp;hosts.&nbsp;I&nbsp;also&nbsp;imagine&nbsp;it&nbsp;isn&amp;#39;t&nbsp;an&nbsp;issue&nbsp;for&nbsp;people&nbsp;who&nbsp;aren&amp;#39;t&nbsp;trying&nbsp;to&nbsp;send&nbsp;large,&nbsp;persistent,&nbsp;binary&nbsp;messages&nbsp;through&nbsp;the&nbsp;queueing&nbsp;system,&nbsp;since&nbsp;they&nbsp;probably&nbsp;never&nbsp;run&nbsp;into&nbsp;IO&nbsp;limitations.&lt;br&gt;<br>
&lt;br&gt;&lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;this&nbsp;led&nbsp;to&nbsp;a&nbsp;single&nbsp;node&nbsp;with&nbsp;~35k&nbsp;active&nbsp;open&nbsp;filehandles,&nbsp;and&nbsp;4&nbsp;nodes&nbsp;with&nbsp;~90.&nbsp;not&nbsp;an&nbsp;optimum&nbsp;distribution&nbsp;of&nbsp;read&nbsp;workload.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Agreed.&nbsp;Simon&nbsp;or&nbsp;Marthias&nbsp;may&nbsp;be&nbsp;able&nbsp;to&nbsp;elaborate&nbsp;on&nbsp;various&nbsp;things&nbsp;we&amp;#39;re&nbsp;working&nbsp;on&nbsp;to&nbsp;improve&nbsp;workload&nbsp;distribution.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Great!&nbsp;We&amp;#39;re&nbsp;doing&nbsp;some&nbsp;work&nbsp;on&nbsp;our&nbsp;code&nbsp;to&nbsp;manually&nbsp;distribute&nbsp;queues&nbsp;at&nbsp;creation&nbsp;time,&nbsp;but&nbsp;it&amp;#39;d&nbsp;be&nbsp;a&nbsp;lot&nbsp;better&nbsp;if&nbsp;there&nbsp;was&nbsp;a&nbsp;switch&nbsp;to&nbsp;pull&nbsp;on&nbsp;the&nbsp;rabbit&nbsp;end&nbsp;to&nbsp;just&nbsp;make&nbsp;it&nbsp;happen.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;expected&nbsp;that&nbsp;if&nbsp;system&nbsp;a&nbsp;queue&nbsp;was&nbsp;created&nbsp;on&nbsp;is&nbsp;permanently&nbsp;removed&nbsp;(shut&nbsp;down&nbsp;and&nbsp;&amp;quot;rabbitmqctl&nbsp;forget_cluster_node&nbsp;hostname&amp;quot;&amp;#39;d),&nbsp;automatic&nbsp;sync&nbsp;would&nbsp;ensure&nbsp;there&amp;#39;s&nbsp;the&nbsp;right&nbsp;number&nbsp;of&nbsp;copies&nbsp;replicated,&nbsp;but&nbsp;instead&nbsp;it&nbsp;just&nbsp;left&nbsp;every&nbsp;single&nbsp;queue&nbsp;under&nbsp;replicated.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
&lt;/div&gt;That&nbsp;doesn&amp;#39;t&nbsp;sound&nbsp;right.&nbsp;It&amp;#39;s&nbsp;not&nbsp;automatic&nbsp;sync&nbsp;we&amp;#39;re&nbsp;talking&nbsp;here&nbsp;either&nbsp;-&nbsp;that&nbsp;sounds&nbsp;like&nbsp;the&nbsp;policy&nbsp;isn&amp;#39;t&nbsp;getting&nbsp;applied&nbsp;properly.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Hmm...&nbsp;Well,&nbsp;we&amp;#39;re&nbsp;just&nbsp;applying&nbsp;a&nbsp;global&nbsp;policy&nbsp;with&nbsp;the&nbsp;pattern&nbsp;&amp;quot;.*&amp;quot;,&nbsp;and&nbsp;it&nbsp;shows&nbsp;as&nbsp;being&nbsp;applied&nbsp;in&nbsp;the&nbsp;queue&nbsp;information&nbsp;API&nbsp;and&nbsp;on&nbsp;the&nbsp;web&nbsp;page.&nbsp;I&amp;#39;m&nbsp;not&nbsp;sure&nbsp;how&nbsp;to&nbsp;check&nbsp;if&nbsp;it&amp;#39;s&nbsp;fully&nbsp;applied&nbsp;otherwise,&nbsp;so&nbsp;if&nbsp;you&amp;#39;ve&nbsp;got&nbsp;something&nbsp;I&nbsp;can&nbsp;run&nbsp;to&nbsp;check&nbsp;that,&nbsp;I&nbsp;can&nbsp;definitely&nbsp;do&nbsp;some&nbsp;digging.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;when&nbsp;a&nbsp;new&nbsp;policy&nbsp;is&nbsp;applied&nbsp;that&nbsp;defines&nbsp;specific&nbsp;replication&nbsp;nodes,&nbsp;or&nbsp;a&nbsp;number&nbsp;of&nbsp;copies&nbsp;using&nbsp;&amp;#39;exact,&nbsp;and&nbsp;auto-sync&nbsp;is&nbsp;set,&nbsp;sometimes&nbsp;it&nbsp;just&nbsp;syncs&nbsp;the&nbsp;first&nbsp;replica&nbsp;and&nbsp;leaves&nbsp;any&nbsp;others&nbsp;unsynced&nbsp;and&nbsp;calls&nbsp;it&nbsp;job&nbsp;done.&nbsp;This&nbsp;is&nbsp;bad.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
&lt;/div&gt;Can&nbsp;you&nbsp;provide&nbsp;us&nbsp;with&nbsp;a&nbsp;way&nbsp;to&nbsp;reproduce&nbsp;this?&nbsp;How&nbsp;did&nbsp;you&nbsp;detect&nbsp;that&nbsp;the&nbsp;remaining&nbsp;replicas&nbsp;were&nbsp;not&nbsp;sync&amp;#39;ed?&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Detection&nbsp;was&nbsp;just&nbsp;by&nbsp;looking&nbsp;at&nbsp;the&nbsp;queue&nbsp;page&nbsp;in&nbsp;the&nbsp;management&nbsp;web&nbsp;GUI.&nbsp;It&nbsp;shows&nbsp;a&nbsp;big&nbsp;blue&nbsp;+1&nbsp;and&nbsp;a&nbsp;big&nbsp;red&nbsp;+1&nbsp;next&nbsp;to&nbsp;maybe&nbsp;10%&nbsp;of&nbsp;queues&nbsp;after&nbsp;applying&nbsp;the&nbsp;global&nbsp;queue&nbsp;policy&nbsp;after&nbsp;all&nbsp;sync&nbsp;ops&nbsp;complete.&nbsp;If&nbsp;I&nbsp;issue&nbsp;a&nbsp;manual&nbsp;sync&nbsp;operation&nbsp;on&nbsp;all&nbsp;the&nbsp;problem&nbsp;queues,&nbsp;then&nbsp;they&nbsp;correctly&nbsp;finish&nbsp;syncing&nbsp;up&nbsp;the&nbsp;3rd&nbsp;data&nbsp;copy.&nbsp;I&amp;#39;ll&nbsp;see&nbsp;if&nbsp;I&nbsp;can&nbsp;script&nbsp;up&nbsp;a&nbsp;way&nbsp;to&nbsp;reproduce&nbsp;it&nbsp;on&nbsp;clean&nbsp;set&nbsp;of&nbsp;nodes,&nbsp;since&nbsp;I&amp;#39;m&nbsp;trying&nbsp;not&nbsp;to&nbsp;break&nbsp;my&nbsp;prod&nbsp;cluster&nbsp;any&nbsp;more&nbsp;than&nbsp;I&nbsp;have&nbsp;this&nbsp;week.&nbsp;I&amp;#39;ll&nbsp;e-mail&nbsp;the&nbsp;list&nbsp;once&nbsp;I&amp;#39;ve&nbsp;got&nbsp;a&nbsp;reproducible&nbsp;test&nbsp;case.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;Attempted&nbsp;to&nbsp;create&nbsp;small&nbsp;per-queue&nbsp;policies&nbsp;to&nbsp;redistribute&nbsp;messages&nbsp;and&nbsp;then&nbsp;delete&nbsp;the&nbsp;per-queue&nbsp;policies,&nbsp;but&nbsp;this&nbsp;often&nbsp;leads&nbsp;to&nbsp;a&nbsp;inconsistent&nbsp;cluster&nbsp;state&nbsp;where&nbsp;queues&nbsp;continued&nbsp;to&nbsp;show&nbsp;as&nbsp;being&nbsp;part&nbsp;of&nbsp;a&nbsp;policy&nbsp;that&nbsp;was&nbsp;already&nbsp;deleted,&nbsp;attempt&nbsp;to&nbsp;resync,&nbsp;and&nbsp;get&nbsp;stuck,&nbsp;unable&nbsp;to&nbsp;complete&nbsp;or&nbsp;switch&nbsp;back&nbsp;to&nbsp;the&nbsp;global&nbsp;default&nbsp;policy.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
&lt;/div&gt;Again,&nbsp;it&nbsp;would&nbsp;be&nbsp;helpful&nbsp;if&nbsp;you&nbsp;could&nbsp;help&nbsp;us&nbsp;to&nbsp;replicate&nbsp;this.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;This&nbsp;is&nbsp;100%&nbsp;reproducible&nbsp;on&nbsp;our&nbsp;prod&nbsp;cluster.&nbsp;I&amp;#39;ve&nbsp;got&nbsp;a&nbsp;python&nbsp;script&nbsp;that&nbsp;attempts&nbsp;the&nbsp;rebalancing&nbsp;on&nbsp;a&nbsp;cluster,&nbsp;so&nbsp;I&amp;#39;ll&nbsp;add&nbsp;some&nbsp;logic&nbsp;to&nbsp;get&nbsp;it&nbsp;to&nbsp;generate&nbsp;and&nbsp;populate&nbsp;queues&nbsp;to&nbsp;reproduce&nbsp;this&nbsp;on&nbsp;a&nbsp;fresh&nbsp;cluster,&nbsp;and&nbsp;e-mail&nbsp;that&nbsp;out.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;sometimes&nbsp;the&nbsp;cluster&nbsp;refuses&nbsp;to&nbsp;accept&nbsp;any&nbsp;more&nbsp;policy&nbsp;commands.&nbsp;Have&nbsp;to&nbsp;fully&nbsp;shut&nbsp;down&nbsp;and&nbsp;restart&nbsp;the&nbsp;cluster&nbsp;to&nbsp;clear&nbsp;this&nbsp;condition.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;And&nbsp;this.&nbsp;Can&nbsp;you&nbsp;provide&nbsp;a&nbsp;run&nbsp;down&nbsp;of&nbsp;these&nbsp;policies&nbsp;and&nbsp;the&nbsp;order&nbsp;in&nbsp;which&nbsp;you&amp;#39;re&nbsp;trying&nbsp;to&nbsp;apply&nbsp;them?&nbsp;Also,&nbsp;how&nbsp;busy&nbsp;are&nbsp;the&nbsp;queues&nbsp;whilst&nbsp;the&nbsp;policy&nbsp;changes&nbsp;are&nbsp;happening?&nbsp;We&nbsp;may&nbsp;need&nbsp;to&nbsp;extend&nbsp;our&nbsp;test&nbsp;beds&nbsp;to&nbsp;reliably&nbsp;reproduce&nbsp;such&nbsp;problems.&lt;br&gt;<br>
&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;This&nbsp;case&nbsp;happens&nbsp;after&nbsp;attempting&nbsp;a&nbsp;bunch&nbsp;of&nbsp;policy&nbsp;operations&nbsp;from&nbsp;the&nbsp;previous&nbsp;mentioned&nbsp;script,&nbsp;so&nbsp;it&nbsp;should&nbsp;be&nbsp;easy&nbsp;enough&nbsp;to&nbsp;see&nbsp;it&nbsp;in&nbsp;action&nbsp;once&nbsp;I&amp;#39;ve&nbsp;got&nbsp;a&nbsp;script&nbsp;to&nbsp;reproduce&nbsp;the&nbsp;previous&nbsp;issue.&nbsp;We&nbsp;saw&nbsp;this&nbsp;happening&nbsp;with&nbsp;as&nbsp;low&nbsp;as&nbsp;5&nbsp;messages/sec&nbsp;on&nbsp;the&nbsp;whole&nbsp;cluster,&nbsp;so&nbsp;it&nbsp;doesn&amp;#39;t&nbsp;seem&nbsp;to&nbsp;be&nbsp;load&nbsp;related.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
<br>
&amp;gt;&nbsp;-&nbsp;sometimes&nbsp;policies&nbsp;applied&nbsp;to&nbsp;empty&nbsp;and&nbsp;inactive&nbsp;queues&nbsp;don&amp;#39;t&nbsp;get&nbsp;correctly&nbsp;applied,&nbsp;and&nbsp;the&nbsp;queue&nbsp;hangs&nbsp;on&nbsp;&amp;quot;resyncing&nbsp;/&nbsp;100%&amp;quot;.l&lt;br&gt;<br>
&lt;br&gt;<br>
What!?&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Yeah.&nbsp;That&nbsp;was&nbsp;my&nbsp;reaction&nbsp;as&nbsp;well.&nbsp;We&nbsp;saw&nbsp;this&nbsp;after&nbsp;removing&nbsp;the&nbsp;per-queue&nbsp;polices&nbsp;created&nbsp;with&nbsp;the&nbsp;previous&nbsp;mentioned&nbsp;script,&nbsp;after&nbsp;the&nbsp;queues&nbsp;reverted&nbsp;to&nbsp;the&nbsp;global&nbsp;exact/3/autosync&nbsp;policy.&nbsp;I&nbsp;had&nbsp;to&nbsp;actually&nbsp;kill&nbsp;all&nbsp;of&nbsp;my&nbsp;rabbitmq&nbsp;instances&nbsp;as&nbsp;they&nbsp;wouldn&amp;#39;t&nbsp;nicely&nbsp;shut&nbsp;down,&nbsp;and&nbsp;then&nbsp;bring&nbsp;the&nbsp;whole&nbsp;cluster&nbsp;back&nbsp;up&nbsp;to&nbsp;clear&nbsp;this.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;this&nbsp;makes&nbsp;no&nbsp;sense,&nbsp;given&nbsp;the&nbsp;queue&nbsp;is&nbsp;empty,&nbsp;and&nbsp;requires&nbsp;a&nbsp;full&nbsp;cluster&nbsp;restart&nbsp;to&nbsp;clear.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Please&nbsp;provide&nbsp;the&nbsp;commands&nbsp;you&nbsp;invoked&nbsp;to&nbsp;get&nbsp;this&nbsp;to&nbsp;happen.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Again,&nbsp;this&nbsp;are&nbsp;all&nbsp;things&nbsp;noticed&nbsp;after&nbsp;running&nbsp;the&nbsp;script&nbsp;mentioned&nbsp;above&nbsp;to&nbsp;do&nbsp;the&nbsp;per-queue&nbsp;policies.&nbsp;I&nbsp;didn&amp;#39;t&nbsp;intentionally&nbsp;do&nbsp;anything&nbsp;to&nbsp;make&nbsp;these&nbsp;errors&nbsp;occur,&nbsp;but&nbsp;once&nbsp;I&amp;#39;ve&nbsp;got&nbsp;a&nbsp;script&nbsp;to&nbsp;reproduce&nbsp;the&nbsp;first&nbsp;set&nbsp;of&nbsp;errors&nbsp;on&nbsp;a&nbsp;fresh&nbsp;cluster,&nbsp;it&nbsp;should&nbsp;be&nbsp;easy&nbsp;enough&nbsp;to&nbsp;see&nbsp;some&nbsp;of&nbsp;these&nbsp;other&nbsp;issues,&nbsp;since&nbsp;they&nbsp;seem&nbsp;to&nbsp;cascade&nbsp;from&nbsp;the&nbsp;first&nbsp;set&nbsp;of&nbsp;problems.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;I&amp;#39;ve&nbsp;managed&nbsp;to&nbsp;get&nbsp;the&nbsp;cluster&nbsp;into&nbsp;an&nbsp;inconsistent&nbsp;state&nbsp;a&nbsp;/lot/&nbsp;using&nbsp;the&nbsp;HA&nbsp;features,&nbsp;so&nbsp;it&nbsp;feels&nbsp;like&nbsp;they&nbsp;need&nbsp;more&nbsp;automated&nbsp;stress&nbsp;testing&nbsp;and&nbsp;bulletproofing.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;If&nbsp;you&nbsp;can&nbsp;help&nbsp;us&nbsp;repoduce&nbsp;these&nbsp;errors,&nbsp;I&nbsp;can&nbsp;assure&nbsp;you&nbsp;that&nbsp;they&amp;#39;ll&nbsp;get&nbsp;included&nbsp;in&nbsp;our&nbsp;integration&nbsp;tests!&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Great.&nbsp;I&amp;#39;ll&nbsp;get&nbsp;to&nbsp;work&nbsp;on&nbsp;being&nbsp;able&nbsp;to&nbsp;solidly&nbsp;reproduce&nbsp;at&nbsp;least&nbsp;the&nbsp;first&nbsp;set&nbsp;of&nbsp;issues&nbsp;I&nbsp;encountered,&nbsp;and&nbsp;hopefully&nbsp;that&amp;#39;ll&nbsp;lead&nbsp;to&nbsp;a&nbsp;reproduction&nbsp;path&nbsp;for&nbsp;some&nbsp;of&nbsp;the&nbsp;other&nbsp;ones.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;Persistent&nbsp;message&nbsp;storage:&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;-&nbsp;it&nbsp;appears&nbsp;as&nbsp;if&nbsp;messages&nbsp;are&nbsp;put&nbsp;into&nbsp;very&nbsp;small&nbsp;batch&nbsp;files&nbsp;on&nbsp;the&nbsp;filesystem&nbsp;(1-20&nbsp;MB)&lt;br&gt;<br>
&amp;gt;&nbsp;-&nbsp;this&nbsp;causes&nbsp;the&nbsp;filesystem&nbsp;to&nbsp;thrash&nbsp;if&nbsp;your&nbsp;IO&nbsp;isn&amp;#39;t&nbsp;good&nbsp;at&nbsp;random&nbsp;IO&nbsp;(SATA&nbsp;disks)&nbsp;and&nbsp;you&nbsp;have&nbsp;lots&nbsp;of&nbsp;persistent&nbsp;messages&nbsp;(&amp;gt;200k&nbsp;messages&nbsp;500kB-1MB&nbsp;in&nbsp;size)&nbsp;that&nbsp;don&amp;#39;t&nbsp;fit&nbsp;in&nbsp;RAM.&lt;br&gt;<br>
&amp;gt;&nbsp;-&nbsp;this&nbsp;caused&nbsp;CentOS&nbsp;6&nbsp;kernel&nbsp;to&nbsp;kill&nbsp;erlang&nbsp;after&nbsp;stalling&nbsp;the&nbsp;XFS&nbsp;filesystem&nbsp;for&nbsp;&amp;gt;&nbsp;120s.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Iirc&nbsp;this&nbsp;is&nbsp;tuneable,&nbsp;though&nbsp;we&nbsp;don&amp;#39;t&nbsp;recommend&nbsp;changing&nbsp;it.&nbsp;Not&nbsp;at&nbsp;u&nbsp;desk&nbsp;right&nbsp;now&nbsp;though,&nbsp;so&nbsp;I&nbsp;can&amp;#39;t&nbsp;remember&nbsp;the&nbsp;exact&nbsp;details.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;It&nbsp;doesn&amp;#39;t&nbsp;seem&nbsp;to&nbsp;be&nbsp;an&nbsp;issue&nbsp;since&nbsp;we&amp;#39;ve&nbsp;switched&nbsp;to&nbsp;SSDs,&nbsp;so&nbsp;I&amp;#39;m&nbsp;not&nbsp;going&nbsp;to&nbsp;spend&nbsp;a&nbsp;lot&nbsp;of&nbsp;time&nbsp;worrying&nbsp;about&nbsp;it.&nbsp;It&amp;#39;d&nbsp;just&nbsp;be&nbsp;nice&nbsp;to&nbsp;see&nbsp;some&nbsp;supported&nbsp;tuning&nbsp;options&nbsp;for&nbsp;this&nbsp;make&nbsp;a&nbsp;dev&nbsp;roadmap&nbsp;for&nbsp;the&nbsp;future.&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;<br>
&amp;gt;&nbsp;-&nbsp;if&nbsp;a&nbsp;node&nbsp;crashes,&nbsp;Rabbit&nbsp;seems&nbsp;to&nbsp;rescan&nbsp;the&nbsp;entire&nbsp;on-disk&nbsp;datastore&nbsp;before&nbsp;continuing,&nbsp;instead&nbsp;of&nbsp;using&nbsp;some&nbsp;sort&nbsp;of&nbsp;checkpointing&nbsp;or&nbsp;journaling&nbsp;system&nbsp;to&nbsp;quickly&nbsp;recover&nbsp;from&nbsp;a&nbsp;crash.&lt;br&gt;<br>
&amp;gt;&nbsp;-&nbsp;all&nbsp;of&nbsp;above&nbsp;should&nbsp;be&nbsp;solvable&nbsp;by&nbsp;using&nbsp;an&nbsp;existing&nbsp;append-only&nbsp;datastore&nbsp;like&nbsp;eLevelDB&nbsp;or&nbsp;Bitcask.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;On&nbsp;our&nbsp;todo&nbsp;list&nbsp;already,&nbsp;at&nbsp;least&nbsp;for&nbsp;the&nbsp;message&nbsp;store&nbsp;index.&lt;br&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Great,&nbsp;glad&nbsp;to&nbsp;hear&nbsp;it.&lt;br&gt;&lt;br&gt;There&amp;#39;s&nbsp;probably&nbsp;a&nbsp;lot&nbsp;of&nbsp;performance&nbsp;improvements&nbsp;to&nbsp;be&nbsp;had&nbsp;by&nbsp;using&nbsp;something&nbsp;like&nbsp;eleveldb&nbsp;or&nbsp;bitcask,&nbsp;since&nbsp;they&nbsp;do&nbsp;a&nbsp;lot&nbsp;to&nbsp;optimize&nbsp;disk&nbsp;seeks&nbsp;and&nbsp;RAM&nbsp;buffering,&nbsp;but&nbsp;I&nbsp;imagine&nbsp;that&amp;#39;s&nbsp;a&nbsp;fairly&nbsp;ambitious&nbsp;amount&nbsp;of&nbsp;work&nbsp;that&nbsp;isn&amp;#39;t&nbsp;high&nbsp;particularly&nbsp;high&nbsp;priority&nbsp;for&nbsp;you&nbsp;guys.&nbsp;Just&nbsp;a&nbsp;suggestion&nbsp;to&nbsp;think&nbsp;about&nbsp;in&nbsp;the&nbsp;long&nbsp;term,&nbsp;or&nbsp;to&nbsp;put&nbsp;an&nbsp;intern&nbsp;on&nbsp;testing.&nbsp;;)&lt;br&gt;<br>
&lt;br&gt;&lt;/div&gt;&lt;div&gt;In&nbsp;any&nbsp;case,&nbsp;I&amp;#39;m&nbsp;going&nbsp;to&nbsp;get&nbsp;working&nbsp;on&nbsp;reproducible&nbsp;test&nbsp;cases&nbsp;for&nbsp;all&nbsp;the&nbsp;issues&nbsp;we&amp;#39;ve&nbsp;been&nbsp;discussing.&nbsp;I&amp;#39;ll&nbsp;update&nbsp;this&nbsp;thread&nbsp;when&nbsp;I&nbsp;have&nbsp;something&nbsp;concrete&nbsp;for&nbsp;you.&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;<br>
&lt;div&gt;Graeme&lt;br&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;<br>

</tt>
