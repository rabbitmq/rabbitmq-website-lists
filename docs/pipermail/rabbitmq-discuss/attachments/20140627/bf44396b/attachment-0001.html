<tt>
&lt;div&nbsp;dir=&quot;ltr&quot;&gt;Today&nbsp;we&nbsp;had&nbsp;the&nbsp;physical&nbsp;machine&nbsp;of&nbsp;a&nbsp;dedicated&nbsp;node&nbsp;kernel&nbsp;panic&nbsp;(linux&nbsp;centos&nbsp;6)...&nbsp;when&nbsp;that&nbsp;happened&nbsp;the&nbsp;other&nbsp;two&nbsp;nodes&nbsp;in&nbsp;the&nbsp;cluster&nbsp;seemed&nbsp;to&nbsp;choke,&nbsp;and&nbsp;not&nbsp;respond&nbsp;at&nbsp;all.&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&quot;rabbitmqctl&nbsp;cluster_status&quot;&nbsp;on&nbsp;either&nbsp;of&nbsp;the&nbsp;other&nbsp;nodes&nbsp;would&nbsp;hang.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The&nbsp;web&nbsp;management&nbsp;UI&nbsp;didn't&nbsp;respond.&nbsp;&nbsp;I&nbsp;could&nbsp;get&nbsp;a&nbsp;login&nbsp;page&nbsp;to&nbsp;come&nbsp;up&nbsp;but&nbsp;after&nbsp;that&nbsp;it&nbsp;would&nbsp;go&nbsp;back&nbsp;to&nbsp;not&nbsp;responding.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;When&nbsp;the&nbsp;crashed&nbsp;machine&nbsp;came&nbsp;back&nbsp;up,&nbsp;without&nbsp;starting&nbsp;rabbitmq&nbsp;on&nbsp;it,&nbsp;once&nbsp;networking&nbsp;was&nbsp;responding,&nbsp;the&nbsp;other&nbsp;two&nbsp;nodes&nbsp;seemed&nbsp;to&nbsp;free&nbsp;up&nbsp;and&nbsp;start&nbsp;operating&nbsp;normally&nbsp;again.&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;After&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;cluster&nbsp;was&nbsp;operating&nbsp;normally&nbsp;again,&nbsp;we&nbsp;brought&nbsp;down&nbsp;the&nbsp;crashed&nbsp;machine&nbsp;to&nbsp;do&nbsp;a&nbsp;memtest,&nbsp;and&nbsp;we&nbsp;didn't&nbsp;experience&nbsp;the&nbsp;cluster&nbsp;freeze&nbsp;again&nbsp;(rabbitmq&nbsp;was&nbsp;not&nbsp;ever&nbsp;started&nbsp;back&nbsp;up&nbsp;on&nbsp;the&nbsp;failed&nbsp;node).&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;This&nbsp;cluster&nbsp;(we&nbsp;went&nbsp;live&nbsp;with&nbsp;multiple&nbsp;clusters&nbsp;yesterday),&nbsp;is&nbsp;running&nbsp;3&nbsp;physical&nbsp;dedicated&nbsp;machines.&nbsp;&nbsp;All&nbsp;of&nbsp;them&nbsp;are&nbsp;on&nbsp;centos&nbsp;6.&nbsp;&nbsp;RabbitMQ&nbsp;v&nbsp;3.3.2.&nbsp;&nbsp;All&nbsp;nodes&nbsp;are&nbsp;disc&nbsp;nodes.&nbsp;&nbsp;All&nbsp;queues&nbsp;are&nbsp;durable&nbsp;and&nbsp;mirrored.&nbsp;&nbsp;This&nbsp;cluster&nbsp;has&nbsp;one&nbsp;queue,&nbsp;plus&nbsp;1000's&nbsp;of&nbsp;dynamic&nbsp;shovels&nbsp;(which&nbsp;of&nbsp;course&nbsp;includes&nbsp;their&nbsp;own&nbsp;queues&nbsp;on&nbsp;this&nbsp;cluster)&nbsp;connecting&nbsp;to&nbsp;queues&nbsp;on&nbsp;3&nbsp;other&nbsp;clusters&nbsp;with&nbsp;similar&nbsp;setups.&nbsp;&nbsp;Each&nbsp;node&nbsp;has&nbsp;about&nbsp;7gig&nbsp;of&nbsp;disk&nbsp;free&nbsp;on&nbsp;the&nbsp;relevant&nbsp;partition,&nbsp;and&nbsp;48gig&nbsp;of&nbsp;ram&nbsp;with&nbsp;the&nbsp;high_water_mark&nbsp;set&nbsp;to&nbsp;0.9,&nbsp;but&nbsp;even&nbsp;at&nbsp;diminished&nbsp;capacity&nbsp;right&nbsp;now,&nbsp;the&nbsp;most&nbsp;ram&nbsp;used&nbsp;is&nbsp;1.2gig&nbsp;on&nbsp;one&nbsp;node&nbsp;and&nbsp;600meg&nbsp;on&nbsp;the&nbsp;other&nbsp;(these&nbsp;boxes&nbsp;were&nbsp;way&nbsp;over&nbsp;built&nbsp;with&nbsp;short-term&nbsp;growth&nbsp;in&nbsp;mind).&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Sadly,&nbsp;there&nbsp;was&nbsp;nothing&nbsp;in&nbsp;the&nbsp;logs.&nbsp;&nbsp;We&nbsp;realized&nbsp;this&nbsp;might&nbsp;be&nbsp;related&nbsp;to&nbsp;the&nbsp;logging&nbsp;bug&nbsp;fixed&nbsp;in&nbsp;3.3.3,&nbsp;so&nbsp;we&nbsp;just&nbsp;upgraded&nbsp;our&nbsp;dev&nbsp;environment&nbsp;to&nbsp;start&nbsp;the&nbsp;process&nbsp;to&nbsp;deal&nbsp;with&nbsp;that.&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Any&nbsp;thoughts&nbsp;on&nbsp;what&nbsp;the&nbsp;cause&nbsp;of&nbsp;this&nbsp;freeze&nbsp;up&nbsp;could&nbsp;have&nbsp;been?&nbsp;&nbsp;And&nbsp;how&nbsp;to&nbsp;mitigate&nbsp;it?&nbsp;&nbsp;Or&nbsp;any&nbsp;troubleshooting&nbsp;/&nbsp;information&nbsp;gathering&nbsp;we&nbsp;could&nbsp;do&nbsp;if&nbsp;it&nbsp;happens&nbsp;again?&nbsp;&nbsp;It's&nbsp;a&nbsp;scary&nbsp;thing&nbsp;now&nbsp;to&nbsp;have&nbsp;happen&nbsp;on&nbsp;a&nbsp;friday&nbsp;afternoon.&nbsp;&nbsp;We&nbsp;were&nbsp;counting&nbsp;on&nbsp;three&nbsp;node&nbsp;clusters&nbsp;getting&nbsp;us&nbsp;through&nbsp;if&nbsp;there&nbsp;was&nbsp;an&nbsp;outage&nbsp;of&nbsp;a&nbsp;node&nbsp;during&nbsp;the&nbsp;weekend...&nbsp;but&nbsp;now&nbsp;we're&nbsp;all&nbsp;afraid&nbsp;to&nbsp;go&nbsp;home&nbsp;for&nbsp;the&nbsp;weekend!&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thanks!&lt;/div&gt;&lt;div&gt;Dan.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;
</tt>
