<tt>
&lt;div&nbsp;dir=&quot;ltr&quot;&gt;Cheers!&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; I&#39;m&nbsp;in&nbsp;the&nbsp;process&nbsp;of&nbsp;building&nbsp;a&nbsp;rabbitmq&nbsp;cluster,&nbsp;and&nbsp;before&nbsp;going&nbsp;to&nbsp;production&nbsp;I&#39;d&nbsp;like&nbsp;to&nbsp;produce&nbsp;some&nbsp;benchmarks,&nbsp;and&nbsp;run&nbsp;stresstests.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Unfortunately&nbsp;rabitmq&nbsp;3.1.3&nbsp;(stable&nbsp;ubuntu&nbsp;13.10),&nbsp;and&nbsp;3.2.2&nbsp;(stable&nbsp;ubuntu&nbsp;13.10&nbsp;+&nbsp;&lt;a&nbsp;href=&quot;http://rabbitmq.com&quot;&gt;rabbitmq.com&lt;/a&gt;&nbsp;repository)&nbsp;crashes&nbsp;miserably&nbsp;every&nbsp;time.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; I&#39;ve&nbsp;got&nbsp;3&nbsp;nodes,&nbsp;called&nbsp;node1-2-3,&nbsp;but&nbsp;don&#39;t&nbsp;get&nbsp;confused,&nbsp;node1&nbsp;is&nbsp;for&nbsp;stresstesting&nbsp;only,&nbsp;and&nbsp;node2+node3&nbsp;are&nbsp;in&nbsp;a&nbsp;rabbitmq&nbsp;cluster.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; All&nbsp;nodes&nbsp;have&nbsp;16G&nbsp;of&nbsp;ram&nbsp;and&nbsp;about&nbsp;100G&nbsp;of&nbsp;disk&nbsp;space.&nbsp;ERLANG16B1.&nbsp;Hipe&nbsp;is&nbsp;installed,&nbsp;but&nbsp;not&nbsp;enabled&nbsp;in&nbsp;the&nbsp;configuration&nbsp;as&nbsp;it&#39;s&nbsp;marked&nbsp;as&nbsp;experimental,&nbsp;and&nbsp;I&#39;d&nbsp;like&nbsp;to&nbsp;get&nbsp;stability&nbsp;first,&nbsp;performance&nbsp;is&nbsp;secondary,&nbsp;although&nbsp;that&nbsp;would&nbsp;be&nbsp;nice&nbsp;too.&nbsp;:)&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; For&nbsp;benchmarking&nbsp;I&#39;d&nbsp;do&nbsp;a&nbsp;lot&nbsp;of&nbsp;test,&nbsp;most&nbsp;of&nbsp;them&nbsp;do&nbsp;not&nbsp;fail,&nbsp;so&nbsp;we&nbsp;will&nbsp;skip&nbsp;those&nbsp;for&nbsp;now.&nbsp;:)&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; I&nbsp;run&nbsp;perftest&nbsp;to&nbsp;&quot;attack&quot;&nbsp;the&nbsp;cluster&nbsp;like&nbsp;this:&lt;/div&gt;&lt;div&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt; consumer:&nbsp;<br>
/perftest/rabbitmq-java-client-bin-3.2.2//runjava.sh&nbsp;<br>
com.rabbitmq.examples.PerfTest&nbsp;-h&nbsp;<br>
&#39;amqp://benchmark:xxx@node3/benchmark&#39;&nbsp;--consumers&nbsp;5&nbsp;<br>
--producers&nbsp;0&nbsp;-u&nbsp;benchmarkq&nbsp;-p&nbsp;--cmessages&nbsp;10000&nbsp;--multiAckEvery&nbsp;100&nbsp;2&gt;&amp;1 &lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;publisher:&nbsp;<br>
perftest/rabbitmq-java-client-bin-3.2.2//runjava.sh&nbsp;<br>
com.rabbitmq.examples.PerfTest&nbsp;-h&nbsp;<br>
&#39;amqp://benchmark:xxx@node2/benchmark&#39;&nbsp;--consumers&nbsp;0&nbsp;<br>
--producers&nbsp;200&nbsp;--routingKey&nbsp;&#39;a.FOOBAR.c&#39;&nbsp;--exchange&nbsp;&#39;benchmark.topic&#39;&nbsp;<br>
-p&nbsp;-u&nbsp;benchmarkq&nbsp;--size&nbsp;5242880&nbsp;--pmessages&nbsp;50  &nbsp;2&gt;&amp;1 &lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; So&nbsp;from&nbsp;node1&nbsp;I&nbsp;publish&nbsp;10000&nbsp;messages&nbsp;(5mbytes&nbsp;each)&nbsp;to&nbsp;a&nbsp;queue&nbsp;on&nbsp;node2,&nbsp;and&nbsp;simultaneously&nbsp;I&nbsp;consume&nbsp;10000&nbsp;messages&nbsp;through&nbsp;node3.&nbsp;(The&nbsp;client&nbsp;is&nbsp;on&nbsp;node1,&nbsp;which&nbsp;is&nbsp;still&nbsp;NOT&nbsp;PART&nbsp;of&nbsp;the&nbsp;rabbitmq&nbsp;cluster,&nbsp;it&nbsp;just&nbsp;has&nbsp;a&nbsp;name&nbsp;like&nbsp;node1)&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; For&nbsp;the&nbsp;tests&nbsp;I&nbsp;use&nbsp;a&nbsp;HA&nbsp;policy&nbsp;to&nbsp;duplicate&nbsp;the&nbsp;messages&nbsp;on&nbsp;both&nbsp;nodes,&nbsp;but&nbsp;it&nbsp;should&nbsp;not&nbsp;make&nbsp;a&nbsp;big&nbsp;difference&nbsp;in&nbsp;network&nbsp;transfer,&nbsp;as&nbsp;the&nbsp;consumer&nbsp;connects&nbsp;to&nbsp;node3,&nbsp;so&nbsp;everything&nbsp;(50Gbyte&nbsp;of&nbsp;data)&nbsp;must&nbsp;go&nbsp;around,&nbsp;but&nbsp;it&nbsp;surely&nbsp;makes&nbsp;the&nbsp;things&nbsp;slower&nbsp;as&nbsp;node3&nbsp;has&nbsp;to&nbsp;write&nbsp;everything&nbsp;to&nbsp;disk.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; The&nbsp;nodes&nbsp;have&nbsp;both&nbsp;management&nbsp;plugin&nbsp;enabled,&nbsp;but&nbsp;have&nbsp;no&nbsp;special&nbsp;configuration,&nbsp;so&nbsp;no&nbsp;shovel&nbsp;no&nbsp;federation,&nbsp;they&nbsp;are&nbsp;disk&nbsp;nodes,&nbsp;high&nbsp;memory&nbsp;watermark&nbsp;set&nbsp;to&nbsp;0.4.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;-------------------------&lt;/div&gt;<br>
<br>
&lt;div&gt; Crash&nbsp;type&nbsp;1.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Previously&nbsp;I&#39;ve&nbsp;reported&nbsp;this&nbsp;bug&nbsp;on&nbsp;irc&nbsp;to&nbsp;bob235,&nbsp;who&nbsp;got&nbsp;the&nbsp;logs&nbsp;for&nbsp;that&nbsp;crash&nbsp;and&nbsp;responded&nbsp;as&nbsp;it&#39;s&nbsp;a&nbsp;real&nbsp;bug&nbsp;in&nbsp;the&nbsp;software,&nbsp;and&nbsp;asked&nbsp;me&nbsp;to&nbsp;write&nbsp;down&nbsp;the&nbsp;specifics,&nbsp;so&nbsp;here&nbsp;we&nbsp;are.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; He&nbsp;acknowledged&nbsp;this&nbsp;as&nbsp;a&nbsp;real&nbsp;bug &lt;/div&gt;&lt;div&gt; (&quot;{gm,find_prefix_common_suffix,2,[]}&quot;&nbsp;in&nbsp;the&nbsp;stacktrace)&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt; (trace&nbsp;can&nbsp;be&nbsp;found&nbsp;at: &lt;a&nbsp;href=&quot;http://corpweb.dunakanyar.net/petrosdump/rabbitcrashlogs.tgz&quot;&gt;http://corpweb.dunakanyar.net/petrosdump/rabbitcrashlogs.tgz&lt;/a&gt;&nbsp;)&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; These&nbsp;logs&nbsp;shows&nbsp;the&nbsp;death&nbsp;of&nbsp;a&nbsp;node,&nbsp;and&nbsp;the&nbsp;sasl&nbsp;log&nbsp;has&nbsp;the&nbsp;details...&nbsp;The&nbsp;&quot;attack&quot;&nbsp;was&nbsp;like&nbsp;the&nbsp;above,&nbsp;200&nbsp;producers.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; One&nbsp;thing&nbsp;to&nbsp;note&nbsp;is&nbsp;that&nbsp;the&nbsp;network&nbsp;links&nbsp;were&nbsp;unbalanced:&lt;/div&gt;<br>
<br>
&lt;div&gt; -&nbsp;node1-&gt;node2&nbsp; 1000mbit&lt;/div&gt;&lt;div&gt; -&nbsp;node2-&gt;node3&nbsp;100mbit&lt;/div&gt;&lt;div&gt; -&nbsp;node3-&gt;node1&nbsp;100mbit&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; But&nbsp;this&nbsp;is&nbsp;not&nbsp;a&nbsp;reason&nbsp;for&nbsp;rabbitmq&nbsp;to&nbsp;stop&nbsp;working.&nbsp;:)&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;-------------------------&lt;/div&gt;<br>
<br>
&lt;div&gt; Crash&nbsp;type&nbsp;2.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Situation:&nbsp;as&nbsp;the&nbsp;consumers&nbsp;have&nbsp;a&nbsp;slower&nbsp;network&nbsp;link,&nbsp;disk&nbsp;runs&nbsp;out&nbsp;on&nbsp;node2,&nbsp;and&nbsp;while&nbsp;I&nbsp;had&nbsp;the&nbsp;default&nbsp;setting&nbsp;of&nbsp;(I&nbsp;think&nbsp;50mbytes),&nbsp;rabbitmq&nbsp;run&nbsp;out&nbsp;of&nbsp;disk&nbsp;space,&nbsp;and&nbsp;CRASHED!. &lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; I&nbsp;don&#39;t&nbsp;think&nbsp;high&nbsp;availability&nbsp;has&nbsp;a&nbsp;part&nbsp;of&nbsp;&quot;if&nbsp;(last_err==E_NOSPC)&nbsp;die();&quot;&nbsp;or&nbsp;something,&nbsp;so&nbsp;rabbitmq&nbsp;should&nbsp;work&nbsp;like&nbsp;it&#39;s&nbsp;documented: &lt;a&nbsp;href=&quot;http://www.rabbitmq.com/memory.html&quot;&gt;http://www.rabbitmq.com/memory.html&lt;/a&gt;&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Flow&nbsp;control&nbsp;should&nbsp;kick&nbsp;in&nbsp;whenever&nbsp;a&nbsp;disk&nbsp;capacity&nbsp;problem&nbsp;is&nbsp;detected,&nbsp;and&nbsp;publishers&nbsp;should&nbsp;stop&nbsp;for&nbsp;a&nbsp;while,&nbsp;and&nbsp;consumers&nbsp;will&nbsp;help&nbsp;the&nbsp;server&nbsp;to&nbsp;breath...&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; 2.1:&nbsp;If&nbsp;the&nbsp;server&nbsp;does&nbsp;not&nbsp;work&nbsp;like&nbsp;it&#39;s&nbsp;documented,&nbsp;then&nbsp;it&#39;s&nbsp;a&nbsp;bug.&lt;/div&gt;<br>
<br>
&lt;div&gt; 2.2:&nbsp;If&nbsp;the&nbsp;server&#39;s&nbsp;default&nbsp;free&nbsp;space&nbsp;limit&nbsp;(50mybte)&nbsp;does&nbsp;not&nbsp;make&nbsp;running&nbsp;a&nbsp;stable&nbsp;service,&nbsp;then&nbsp;it&#39;s&nbsp;not&nbsp;a&nbsp;good&nbsp;default&nbsp;setting&lt;/div&gt;&lt;div&gt; 2.3&nbsp;If&nbsp;I&nbsp;have&nbsp;50gbyte&nbsp;of&nbsp;total&nbsp;data&nbsp;(10000&nbsp;pcs&nbsp;of&nbsp;5mbyte&nbsp;messages)&nbsp;in&nbsp;a&nbsp;single&nbsp;queue&nbsp;(and&nbsp;It&#39;s&nbsp;even&nbsp;consumed&nbsp;while&nbsp;published,&nbsp;so&nbsp;there&#39;s&nbsp;never&nbsp;a&nbsp;moment&nbsp;where&nbsp;every&nbsp;message&nbsp;is&nbsp;on&nbsp;the&nbsp;server),&nbsp;I&nbsp;don&#39;t&nbsp;believe&nbsp;90gbyte&nbsp;of&nbsp;disk&nbsp;space&nbsp;should&nbsp;be&nbsp;allocated,&nbsp;and&nbsp;the&nbsp;server&nbsp;should&nbsp;crash.&nbsp;This&nbsp;is&nbsp;not&nbsp;a&nbsp;documented&nbsp;feature&nbsp;I&nbsp;think,&nbsp;so&nbsp;maybe&nbsp;the&nbsp;documentation&nbsp;needs&nbsp;to&nbsp;get&nbsp;extended.&lt;/div&gt;<br>
<br>
&lt;div&gt; 2.4&nbsp;Could&nbsp;we&nbsp;get&nbsp;a&nbsp;documentation&nbsp;in&nbsp;a&nbsp;backward&nbsp;accessible&nbsp;way,&nbsp;so&nbsp;for&nbsp;example&nbsp;&lt;a&nbsp;href=&quot;http://rabbitmq.com/doc/3.1.8/flowcontrol.html&quot;&gt;rabbitmq.com/doc/3.1.8/flowcontrol.html&lt;/a&gt;&nbsp;?&nbsp; &nbsp;This&nbsp;would&nbsp;make&nbsp;things&nbsp;easier&nbsp;when&nbsp;someone&nbsp;does&nbsp;not&nbsp;use&nbsp;the&nbsp;latest.&lt;/div&gt;<br>
<br>
&lt;div&gt; 2.5&nbsp;(as&nbsp;a&nbsp;side&nbsp;note&nbsp;to&nbsp;default&nbsp;settings:&nbsp;guest:guest&nbsp;as&nbsp;admin,&nbsp;default&nbsp;open&nbsp;to&nbsp;everyone&nbsp;may&nbsp;not&nbsp;be&nbsp;the&nbsp;best&nbsp;setting&nbsp;too,&nbsp;as&nbsp;users&nbsp;tend&nbsp;to&nbsp;leave&nbsp;this&nbsp;enabled,&nbsp;then&nbsp;fail&nbsp;a&nbsp;few&nbsp;months&nbsp;later)&nbsp; :)&nbsp;)&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;<br>
&lt;div&gt;<br>
 So&nbsp;the&nbsp;flow&nbsp;control&nbsp;does&nbsp;not&nbsp;work&nbsp;well&nbsp;for&nbsp;out&nbsp;of&nbsp;disk&nbsp;space&nbsp;situations,&nbsp;even&nbsp;if&nbsp;the&nbsp;amount&nbsp;of&nbsp;data&nbsp;never&nbsp;reaches&nbsp;the&nbsp;disk&nbsp;space&nbsp;available,&nbsp;the&nbsp;server&nbsp;can&nbsp;crash,&nbsp;and&nbsp;this&nbsp;is&nbsp;not&nbsp;nice,&nbsp;the&nbsp;server&nbsp;is&nbsp;not&nbsp;failsafe.&lt;/div&gt;&lt;div&gt;<br>
<br>
&lt;br&gt;&lt;/div&gt;&lt;div&gt;--------------------&lt;/div&gt;&lt;div&gt;Crash&nbsp;type&nbsp;3.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Situation:&nbsp;Out&nbsp;of&nbsp;memory.&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt; I&#39;ve&nbsp;got&nbsp;16Gbytes&nbsp;of&nbsp;ram&nbsp;in&nbsp;each&nbsp;node.&nbsp;The&nbsp;watermark&nbsp;is&nbsp;set&nbsp;to&nbsp;7.8Gbyte,&nbsp;so&nbsp;we&nbsp;should&nbsp;never&nbsp;have&nbsp;memory&nbsp;problems.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; If&nbsp;I&nbsp;start&nbsp;the&nbsp;perftest&nbsp;with&nbsp;10&nbsp;publishers&nbsp;(1000messages&nbsp;x&nbsp;5mbytes),&nbsp;and&nbsp;5&nbsp;consumers,&nbsp;the&nbsp;rabbitmq&nbsp;server&nbsp;keeps&nbsp;allocating&nbsp;ram.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; 3.1&nbsp;Management&nbsp;plugin&nbsp;tells&nbsp;me&nbsp;that&nbsp;it&#39;s&nbsp;using&nbsp;for&nbsp;example&nbsp;9&nbsp;gigabytes&nbsp;(HOW!?),&nbsp;but&nbsp;in&nbsp;real&nbsp;the&nbsp;os&nbsp;shows&nbsp;me,&nbsp;that&nbsp;it&#39;s&nbsp;over&nbsp;12gbytes. &lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; As&nbsp;the&nbsp;perftest&nbsp;is&nbsp;on&nbsp;the&nbsp;way,&nbsp;the&nbsp;server&nbsp;keeps&nbsp;allocating&nbsp;more&nbsp;and&nbsp;more&nbsp;memory.&nbsp;We&#39;re&nbsp;swapping&nbsp;now,&nbsp;as&nbsp;rabbitmq&nbsp;is&nbsp;using&nbsp;over&nbsp;20gigabytes&nbsp;of&nbsp;memory.&nbsp;(Still&nbsp;has&nbsp;the&nbsp;watermark&nbsp;at&nbsp;7.8!)&lt;/div&gt;&lt;div&gt;&lt;br&gt;<br>
<br>
&lt;/div&gt;&lt;div&gt; And&nbsp;at&nbsp;this&nbsp;moment&nbsp;the&nbsp;lighning&nbsp;strikes: &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;cat&nbsp;/var/log/rabbitmq/startup_err &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&quot;Crash&nbsp;dump&nbsp;was&nbsp;written&nbsp;to:&nbsp;erl_crash.dump&lt;/div&gt;&lt;div&gt;eheap_alloc:&nbsp;Cannot&nbsp;allocate&nbsp;8162366936&nbsp;bytes&nbsp;of&nbsp;memory&nbsp;(of&nbsp;type&nbsp;&quot;old_heap&quot;).&lt;/div&gt;<br>
<br>
&lt;div&gt;Aborted&nbsp;(core&nbsp;dumped)&quot;&lt;/div&gt;&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt; WTF&nbsp;(What&nbsp;a&nbsp;terrible&nbsp;failure&nbsp;-&nbsp;after&nbsp;Android&nbsp;SDK&nbsp;:D) &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; 3.2&nbsp;So&nbsp;the&nbsp;server&nbsp;configured&nbsp;to&nbsp;use&nbsp;7.8gbytes&nbsp;memory&nbsp;max&nbsp;tried&nbsp;to&nbsp;allocate&nbsp;8&nbsp;gigabytes&nbsp;over&nbsp;the&nbsp;already&nbsp;allocated&nbsp;20gigabytes.&nbsp;This&nbsp;is&nbsp;surely&nbsp;a&nbsp;bug.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; 3.3&nbsp;flow&nbsp;control&nbsp;kicked&nbsp;in,&nbsp;as&nbsp;the&nbsp;logs&nbsp;show,&nbsp;but&nbsp;memory&nbsp;usage&nbsp;did&nbsp;not&nbsp;lower...&nbsp;and&nbsp;running&nbsp;out&nbsp;of&nbsp;memory&nbsp;is&nbsp;not&nbsp;something&nbsp;that&nbsp;is&nbsp;out&nbsp;of&nbsp;control&nbsp;and&nbsp;a&nbsp;failsafe&nbsp;server&nbsp;&quot;could&nbsp;not&nbsp;handle&quot;&nbsp;like&nbsp;a&nbsp;kill&nbsp;-9.&nbsp;It&nbsp;is&nbsp;a&nbsp;bug&nbsp;to&nbsp;let&nbsp;it&nbsp;go&nbsp;over&nbsp;3&nbsp;times&nbsp;the&nbsp;allowed&nbsp;memory&nbsp;limit.&lt;/div&gt;<br>
<br>
&lt;div&gt; 3.4&nbsp;and&nbsp;it&#39;s&nbsp;an&nbsp;another&nbsp;bug&nbsp;not&nbsp;to&nbsp;handle&nbsp;the&nbsp;E_NOMEM&nbsp;situation. &lt;/div&gt;&lt;div&gt; 3.5&nbsp;writing&nbsp;a&nbsp;19gigabyte&nbsp;erl_crash.dump&nbsp;takes&nbsp;a&nbsp;lot&nbsp;of&nbsp;time&nbsp;:),&nbsp;and&nbsp;it&#39;s&nbsp;not&nbsp;even&nbsp;a&nbsp;binary&nbsp;memorydump&nbsp;with&nbsp;all&nbsp;the&nbsp;data,&nbsp;it&#39;s&nbsp;just&nbsp;the&nbsp;stacktrace&nbsp;:D&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; The&nbsp;full&nbsp;logs&nbsp;are&nbsp;here: &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;a&nbsp;href=&quot;http://corpweb.dunakanyar.net/petrosdump/rabbitmqoutofmemory20140113.tar.bz2&quot;&gt;http://corpweb.dunakanyar.net/petrosdump/rabbitmqoutofmemory20140113.tar.bz2&lt;/a&gt;&nbsp;(1.3gigabytes,&nbsp;contains&nbsp;the&nbsp;full&nbsp;19gbyte&nbsp;stacktrace&nbsp;too,&nbsp;watch&nbsp;out&nbsp;when&nbsp;uncompressing)&lt;br&gt;<br>
<br>
&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; I&#39;ll&nbsp;remove&nbsp;this&nbsp;file&nbsp;at&nbsp;2014-02-01.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;(I&nbsp;know&nbsp;the&nbsp;db&nbsp;contains&nbsp;the&nbsp;users,&nbsp;passes,&nbsp;but&nbsp;they&nbsp;are&nbsp;worthless&nbsp;as&nbsp;it&#39;s&nbsp;just&nbsp;a&nbsp;drop-rebuild&nbsp;machine,&nbsp;even&nbsp;the&nbsp;cookie&nbsp;means&nbsp;nothing)&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; &lt;br&gt;&lt;/div&gt;&lt;div&gt;-------&lt;/div&gt;&lt;div&gt;Crash&nbsp;type&nbsp;4.&lt;/div&gt;&lt;div&gt; This&nbsp;happened&nbsp;a&nbsp;few&nbsp;times&nbsp;too,&nbsp;even&nbsp;after&nbsp;fixing&nbsp;the&nbsp;100m&nbsp;network&nbsp;bottleneck&nbsp;between&nbsp;node3&nbsp;and&nbsp;others.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Sometimes&nbsp;the&nbsp;server&nbsp;goes&nbsp;down&nbsp;with&nbsp;the&nbsp;message &lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&quot;Absurdly&nbsp;large&nbsp;distribution&nbsp;output&nbsp;data&nbsp;buffer&nbsp;(2427628186&nbsp;bytes)&nbsp;passed.&lt;/div&gt;&lt;div&gt;Aborted&nbsp;(core&nbsp;dumped)&quot;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; I&nbsp;can&#39;t&nbsp;find&nbsp;an&nbsp;erl_crash.dump&nbsp;in&nbsp;this&nbsp;case.&lt;/div&gt;&lt;div&gt;<br>
<br>
&lt;br&gt;&lt;/div&gt;&lt;div&gt; But&nbsp;I&nbsp;have&nbsp;a&nbsp;log&nbsp;for&nbsp;this&nbsp;too: &lt;a&nbsp;href=&quot;http://corpweb.dunakanyar.net/petrosdump/cluster_node2_20140109_163509.tar.bz2&quot;&gt;http://corpweb.dunakanyar.net/petrosdump/cluster_node2_20140109_163509.tar.bz2&lt;/a&gt;&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; The&nbsp;test&nbsp;was&nbsp;something&nbsp;similar,&nbsp;probably&nbsp;200&nbsp;publishers&nbsp;*&nbsp;50&nbsp;messages&nbsp;each&nbsp;5mbytes. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; So&nbsp;total&nbsp;data&nbsp;amount&nbsp;is&nbsp;always&nbsp;50gigabytes,&nbsp;the&nbsp;total&nbsp;message&nbsp;count&nbsp;is&nbsp;10.000,&nbsp;I&nbsp;just&nbsp;play&nbsp;with&nbsp;the&nbsp;producer&nbsp;count&nbsp;sometimes.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;--- &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Questions:&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; -&nbsp;What&nbsp;am&nbsp;I&nbsp;doing&nbsp;wrong?&nbsp;Shouldn&#39;t&nbsp;be&nbsp;this&nbsp;&quot;10000&nbsp;messages&nbsp;from&nbsp;publishers&nbsp;to&nbsp;consumers&quot;&nbsp;a&nbsp;simple&nbsp;usecase?&nbsp;Why&nbsp;does&nbsp;the&nbsp;message&nbsp;size&nbsp;make&nbsp;difference?&nbsp;(This&nbsp;is&nbsp;just&nbsp;the&nbsp;size&nbsp;of&nbsp;my&nbsp;local&nbsp;mailbox,&nbsp;and&nbsp;thunderbird&nbsp;does&nbsp;not&nbsp;crash.&nbsp;:)&nbsp;)&lt;/div&gt;<br>
<br>
&lt;div&gt; -&nbsp;How&nbsp;could&nbsp;I&nbsp;help&nbsp;to&nbsp;determine&nbsp;and&nbsp;remove&nbsp;the&nbsp;factors&nbsp;that&nbsp;keep&nbsp;rabbitmq&nbsp;crashing?&lt;/div&gt;&lt;div&gt; -&nbsp;What&nbsp;can&nbsp;I&nbsp;do&nbsp;to&nbsp;deliver&nbsp;50&nbsp;pcs&nbsp;of&nbsp;message&nbsp;from&nbsp;each&nbsp;of&nbsp;200&nbsp;workers?&lt;/div&gt;&lt;div&gt; -&nbsp;Which&nbsp;is&nbsp;the&nbsp;&quot;pivotal&nbsp;preferred&nbsp;stable&nbsp;version&quot;?&nbsp;And&nbsp;which&nbsp;is&nbsp;the&nbsp;&quot;pivotal&nbsp;preferred&nbsp;stable&nbsp;linux&nbsp;distribution&quot;?&lt;/div&gt;<br>
<br>
&lt;div&gt; -&nbsp;Does&nbsp;rabbitmq&nbsp;have&nbsp;a&nbsp;ticket&nbsp;system&nbsp;somewhere?&lt;/div&gt;&lt;div&gt; -&nbsp;Where&nbsp;to&nbsp;report&nbsp;bugs&nbsp;to?&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; &nbsp;I&nbsp;can&nbsp;reproduce&nbsp;some&nbsp;of&nbsp;these,&nbsp;but&nbsp;it&nbsp;looks&nbsp;random,&nbsp;which&nbsp;problem&nbsp;kills&nbsp;the&nbsp;server&nbsp;faster.&nbsp;:)&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; &nbsp;Of&nbsp;course&nbsp;I&#39;ve&nbsp;raised&nbsp;the&nbsp;disk&nbsp;free&nbsp;limit,&nbsp;so&nbsp;I&nbsp;don&#39;t&nbsp;hit&nbsp;that&nbsp;problem&nbsp;today.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; &nbsp;As&nbsp;these&nbsp;are&nbsp;testing&nbsp;machines&nbsp;currently&nbsp;it&nbsp;is&nbsp;possible&nbsp;to&nbsp;give&nbsp;a&nbsp;developer&nbsp;access&nbsp;to&nbsp;them&nbsp;if&nbsp;it&nbsp;helps.&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Please&nbsp;help&nbsp;me&nbsp;get&nbsp;a&nbsp;failsafe&nbsp;rabbitmqnode&nbsp;to&nbsp;build&nbsp;a&nbsp;failsafe&nbsp;rabbitmq&nbsp;cluster&nbsp;from.&nbsp;:)&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Any&nbsp;ideas?&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Thank&nbsp;you!&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt; Peter&nbsp;Kopias&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;ps.:&nbsp;I&nbsp;started&nbsp;with&nbsp;3.1.3,&nbsp;but&nbsp;I&nbsp;figured&nbsp;that&nbsp;reporing&nbsp;crashes&nbsp;in&nbsp;a&nbsp;year&nbsp;old&nbsp;release&nbsp;does&nbsp;not&nbsp;interest&nbsp;the&nbsp;devs,&nbsp;so&nbsp;I&nbsp;switched&nbsp;to&nbsp;3.2.2,&nbsp;as&nbsp;it&nbsp;shows&nbsp;a&nbsp;lot&nbsp;of&nbsp;crashes&nbsp;fixed&nbsp;in&nbsp;the&nbsp;changelog.&nbsp;All&nbsp;the&nbsp;above&nbsp;happened&nbsp;to&nbsp;3.2.2&nbsp;serverd&nbsp;by&nbsp;&lt;a&nbsp;href=&quot;http://rabbitmq.com&quot;&gt;rabbitmq.com&lt;/a&gt;&nbsp;...&lt;/div&gt;<br>
<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;<br>

</tt>
