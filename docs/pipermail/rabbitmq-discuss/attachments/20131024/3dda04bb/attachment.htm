<tt>
&lt;p&nbsp;dir=&quot;ltr&quot;&gt;&amp;quot;single&nbsp;node&nbsp;with&nbsp;~35k&nbsp;active&nbsp;open&nbsp;filehandles&amp;quot;&lt;/p&gt;<br>
&lt;p&nbsp;dir=&quot;ltr&quot;&gt;I&nbsp;assume,&nbsp;all&nbsp;those&nbsp;file&nbsp;handles&nbsp;point&nbsp;to&nbsp;same&nbsp;content?&nbsp;If&nbsp;this&nbsp;would&nbsp;be&nbsp;the&nbsp;case,&nbsp;programmers&nbsp;should&nbsp;go&nbsp;back&nbsp;to&nbsp;school,&nbsp;learn&nbsp;from&nbsp;scratch.&lt;/p&gt;<br>
&lt;p&nbsp;dir=&quot;ltr&quot;&gt;Where&nbsp;does&nbsp;this&nbsp;come&nbsp;from?&lt;/p&gt;<br>
&lt;div&nbsp;class=&quot;gmail_quote&quot;&gt;Am&nbsp;03.10.2013&nbsp;23:04&nbsp;schrieb&nbsp;&amp;quot;Graeme&nbsp;N&amp;quot;&nbsp;&amp;lt;&lt;a&nbsp;href=&quot;mailto:graeme@sudo.ca&quot;&gt;graeme@sudo.ca&lt;/a&gt;&amp;gt;:&lt;br&nbsp;type=&quot;attribution&quot;&gt;&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;<br>
&lt;div&nbsp;dir=&quot;ltr&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;Hey&nbsp;everyone,&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;I&amp;#39;ve&nbsp;recently&nbsp;been&nbsp;doing&nbsp;a&nbsp;deployment&nbsp;of&nbsp;a&nbsp;5&nbsp;node&nbsp;rabbit&nbsp;cluster,&nbsp;and&nbsp;found&nbsp;some&nbsp;rough&nbsp;edges&nbsp;I&nbsp;thought&nbsp;I&nbsp;should&nbsp;share.&nbsp;I&nbsp;realize&nbsp;many&nbsp;of&nbsp;these&nbsp;are&nbsp;feature&nbsp;reqs,&nbsp;but&nbsp;I&amp;#39;m&nbsp;hoping&nbsp;that&nbsp;I&nbsp;just&nbsp;haven&amp;#39;t&nbsp;discovered&nbsp;the&nbsp;proper&nbsp;configuration&nbsp;to&nbsp;deal&nbsp;with&nbsp;some&nbsp;of&nbsp;these&nbsp;issues,&nbsp;or&nbsp;have&nbsp;misunderstood&nbsp;Rabbit&amp;#39;s&nbsp;behaviour.&nbsp;If&nbsp;not,&nbsp;hopefully&nbsp;they&nbsp;can&nbsp;become&nbsp;feature&nbsp;req&nbsp;items&nbsp;that&amp;#39;ll&nbsp;make&nbsp;a&nbsp;dev&nbsp;schedule&nbsp;at&nbsp;some&nbsp;point.&lt;br&gt;<br>
<br>
<br>
&lt;br&gt;&lt;/div&gt;&lt;div&gt;All&nbsp;items&nbsp;below&nbsp;were&nbsp;discovered&nbsp;while&nbsp;deploying&nbsp;3.1.5&nbsp;over&nbsp;the&nbsp;past&nbsp;few&nbsp;days.&nbsp;Hosts&nbsp;in&nbsp;question&nbsp;have&nbsp;24&nbsp;sandy&nbsp;bridge&nbsp;HT&nbsp;cores,&nbsp;64GB&nbsp;of&nbsp;RAM,&nbsp;XFS&nbsp;filesystem,&nbsp;running&nbsp;on&nbsp;CentOS&nbsp;6.&nbsp;Cluster&nbsp;is&nbsp;5&nbsp;nodes,&nbsp;with&nbsp;a&nbsp;default&nbsp;HA&nbsp;policy&nbsp;on&nbsp;all&nbsp;queues&nbsp;of&nbsp;exact/3/automatic-sync.&lt;br&gt;<br>
<br>
<br>
&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;HA&nbsp;/&nbsp;Clustering:&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;-&nbsp;expected&nbsp;queues&nbsp;to&nbsp;be&nbsp;distributed&nbsp;evenly&nbsp;among&nbsp;cluster&nbsp;machines,&nbsp;instead&nbsp;got&nbsp;all&nbsp;queues&nbsp;on&nbsp;first&nbsp;3&nbsp;machines&nbsp;in&nbsp;the&nbsp;cluster,&nbsp;nothing&nbsp;on&nbsp;the&nbsp;last&nbsp;2.&lt;br&gt;&lt;/div&gt;&lt;div&gt;-&nbsp;expected&nbsp;message&nbsp;reads&nbsp;from&nbsp;a&nbsp;mirror&nbsp;machine&nbsp;for&nbsp;a&nbsp;queue&nbsp;to&nbsp;do&nbsp;the&nbsp;read&nbsp;i/o&nbsp;locally,&nbsp;so&nbsp;as&nbsp;to&nbsp;spread&nbsp;out&nbsp;workload,&nbsp;but&nbsp;it&nbsp;appears&nbsp;to&nbsp;always&nbsp;go&nbsp;to&nbsp;the&nbsp;host&nbsp;where&nbsp;the&nbsp;queue&nbsp;was&nbsp;created.&lt;br&gt;<br>
<br>
<br>
&lt;/div&gt;&lt;div&gt;-&nbsp;this&nbsp;led&nbsp;to&nbsp;a&nbsp;single&nbsp;node&nbsp;with&nbsp;~35k&nbsp;active&nbsp;open&nbsp;filehandles,&nbsp;and&nbsp;4&nbsp;nodes&nbsp;with&nbsp;~90.&nbsp;not&nbsp;an&nbsp;optimum&nbsp;distribution&nbsp;of&nbsp;read&nbsp;workload.&lt;br&gt;&lt;/div&gt;&lt;div&gt;-&nbsp;expected&nbsp;that&nbsp;if&nbsp;system&nbsp;a&nbsp;queue&nbsp;was&nbsp;created&nbsp;on&nbsp;is&nbsp;permanently&nbsp;removed&nbsp;(shut&nbsp;down&nbsp;and&nbsp;&amp;quot;rabbitmqctl&nbsp;forget_cluster_node&nbsp;hostname&amp;quot;&amp;#39;d),&nbsp;automatic&nbsp;sync&nbsp;would&nbsp;ensure&nbsp;there&amp;#39;s&nbsp;the&nbsp;right&nbsp;number&nbsp;of&nbsp;copies&nbsp;replicated,&nbsp;but&nbsp;instead&nbsp;it&nbsp;just&nbsp;left&nbsp;every&nbsp;single&nbsp;queue&nbsp;under&nbsp;replicated.&lt;br&gt;<br>
<br>
&lt;/div&gt;&lt;div&gt;-&nbsp;when&nbsp;a&nbsp;new&nbsp;policy&nbsp;is&nbsp;applied&nbsp;that&nbsp;defines&nbsp;specific&nbsp;replication&nbsp;nodes,&nbsp;or&nbsp;a&nbsp;number&nbsp;of&nbsp;copies&nbsp;using&nbsp;&amp;#39;exact,&nbsp;and&nbsp;auto-sync&nbsp;is&nbsp;set,&nbsp;sometimes&nbsp;it&nbsp;just&nbsp;syncs&nbsp;the&nbsp;first&nbsp;replica&nbsp;and&nbsp;leaves&nbsp;any&nbsp;others&nbsp;unsynced&nbsp;and&nbsp;calls&nbsp;it&nbsp;job&nbsp;done.&nbsp;This&nbsp;is&nbsp;bad.&lt;br&gt;<br>
<br>
<br>
&lt;/div&gt;&lt;div&gt;-&nbsp;had&nbsp;to&nbsp;add&nbsp;a&nbsp;new&nbsp;global&nbsp;HA&nbsp;policy&nbsp;and&nbsp;delete&nbsp;the&nbsp;existing&nbsp;one&nbsp;before&nbsp;rabbit&nbsp;fixed&nbsp;my&nbsp;queue&nbsp;replication.&lt;br&gt;&lt;/div&gt;&lt;div&gt;-&nbsp;Attempted&nbsp;to&nbsp;create&nbsp;small&nbsp;per-queue&nbsp;policies&nbsp;to&nbsp;redistribute&nbsp;messages&nbsp;and&nbsp;then&nbsp;delete&nbsp;the&nbsp;per-queue&nbsp;policies,&nbsp;but&nbsp;this&nbsp;often&nbsp;leads&nbsp;to&nbsp;a&nbsp;inconsistent&nbsp;cluster&nbsp;state&nbsp;where&nbsp;queues&nbsp;continued&nbsp;to&nbsp;show&nbsp;as&nbsp;being&nbsp;part&nbsp;of&nbsp;a&nbsp;policy&nbsp;that&nbsp;was&nbsp;already&nbsp;deleted,&nbsp;attempt&nbsp;to&nbsp;resync,&nbsp;and&nbsp;get&nbsp;stuck,&nbsp;unable&nbsp;to&nbsp;complete&nbsp;or&nbsp;switch&nbsp;back&nbsp;to&nbsp;the&nbsp;global&nbsp;default&nbsp;policy.&lt;br&gt;<br>
<br>
-&nbsp;sometimes&nbsp;the&nbsp;cluster&nbsp;refuses&nbsp;to&nbsp;accept&nbsp;any&nbsp;more&nbsp;policy&nbsp;commands.&nbsp;Have&nbsp;to&nbsp;fully&nbsp;shut&nbsp;down&nbsp;and&nbsp;restart&nbsp;the&nbsp;cluster&nbsp;to&nbsp;clear&nbsp;this&nbsp;condition.&lt;br&gt;&lt;/div&gt;&lt;div&gt;-&nbsp;sometimes&nbsp;policies&nbsp;applied&nbsp;to&nbsp;empty&nbsp;and&nbsp;inactive&nbsp;queues&nbsp;don&amp;#39;t&nbsp;get&nbsp;correctly&nbsp;applied,&nbsp;and&nbsp;the&nbsp;queue&nbsp;hangs&nbsp;on&nbsp;&amp;quot;resyncing&nbsp;/&nbsp;100%&amp;quot;.&nbsp;this&nbsp;makes&nbsp;no&nbsp;sense,&nbsp;given&nbsp;the&nbsp;queue&nbsp;is&nbsp;empty,&nbsp;and&nbsp;requires&nbsp;a&nbsp;full&nbsp;cluster&nbsp;restart&nbsp;to&nbsp;clear.&lt;br&gt;<br>
<br>
&lt;/div&gt;&lt;div&gt;-&nbsp;would&nbsp;like&nbsp;to&nbsp;see&nbsp;a&nbsp;tool&nbsp;to&nbsp;redistribute&nbsp;queues&nbsp;amongst&nbsp;available&nbsp;cluster&nbsp;machines&nbsp;according&nbsp;to&nbsp;HA&nbsp;policy.&nbsp;Ideally&nbsp;something&nbsp;that&nbsp;happens&nbsp;automatically&nbsp;on&nbsp;queue&nbsp;creation,&nbsp;cluster&nbsp;membership&nbsp;and&nbsp;policy&nbsp;changes,&nbsp;but&nbsp;would&nbsp;take&nbsp;something&nbsp;manual&nbsp;I&nbsp;could&nbsp;run&nbsp;out&nbsp;of&nbsp;cron.&lt;br&gt;<br>
<br>
&lt;/div&gt;-&nbsp;&nbsp;I&amp;#39;ve&nbsp;managed&nbsp;to&nbsp;get&nbsp;the&nbsp;cluster&nbsp;into&nbsp;an&nbsp;inconsistent&nbsp;state&nbsp;a&nbsp;/lot/&nbsp;<br>
using&nbsp;the&nbsp;HA&nbsp;features,&nbsp;so&nbsp;it&nbsp;feels&nbsp;like&nbsp;they&nbsp;need&nbsp;more&nbsp;<br>
automated&nbsp;stress&nbsp;testing&nbsp;and&nbsp;bulletproofing.&lt;br&gt;&lt;br&gt;Persistent&nbsp;message&nbsp;storage:&lt;br&gt;&lt;/div&gt;&lt;br&gt;-&nbsp;it&nbsp;appears&nbsp;as&nbsp;if&nbsp;messages&nbsp;are&nbsp;put&nbsp;into&nbsp;very&nbsp;small&nbsp;batch&nbsp;files&nbsp;on&nbsp;the&nbsp;filesystem&nbsp;(1-20&nbsp;MB)&lt;br&gt;<br>
&lt;/div&gt;-&nbsp;this&nbsp;causes&nbsp;the&nbsp;filesystem&nbsp;to&nbsp;thrash&nbsp;if&nbsp;your&nbsp;IO&nbsp;isn&amp;#39;t&nbsp;good&nbsp;at&nbsp;random&nbsp;IO&nbsp;(SATA&nbsp;disks)&nbsp;and&nbsp;you&nbsp;have&nbsp;lots&nbsp;of&nbsp;persistent&nbsp;messages&nbsp;(&amp;gt;200k&nbsp;messages&nbsp;500kB-1MB&nbsp;in&nbsp;size)&nbsp;that&nbsp;don&amp;#39;t&nbsp;fit&nbsp;in&nbsp;RAM.&lt;br&gt;-&nbsp;this&nbsp;caused&nbsp;CentOS&nbsp;6&nbsp;kernel&nbsp;to&nbsp;kill&nbsp;erlang&nbsp;after&nbsp;stalling&nbsp;the&nbsp;XFS&nbsp;filesystem&nbsp;for&nbsp;&amp;gt;&nbsp;120s.&lt;br&gt;<br>
<br>
<br>
&lt;/div&gt;&lt;div&gt;-&nbsp;if&nbsp;a&nbsp;node&nbsp;crashes,&nbsp;Rabbit&nbsp;seems&nbsp;to&nbsp;rescan&nbsp;the&nbsp;entire&nbsp;on-disk&nbsp;datastore&nbsp;before&nbsp;continuing,&nbsp;instead&nbsp;of&nbsp;using&nbsp;some&nbsp;sort&nbsp;of&nbsp;checkpointing&nbsp;or&nbsp;journaling&nbsp;system&nbsp;to&nbsp;quickly&nbsp;recover&nbsp;from&nbsp;a&nbsp;crash.&lt;br&gt;&lt;/div&gt;&lt;div&gt;-&nbsp;all&nbsp;of&nbsp;above&nbsp;should&nbsp;be&nbsp;solvable&nbsp;by&nbsp;using&nbsp;an&nbsp;existing&nbsp;append-only&nbsp;datastore&nbsp;like&nbsp;eLevelDB&nbsp;or&nbsp;Bitcask.&lt;br&gt;<br>
<br>
<br>
&lt;/div&gt;&lt;div&gt;-&nbsp;we&nbsp;solved&nbsp;for&nbsp;now&nbsp;by&nbsp;using&nbsp;SSDs,&nbsp;but&nbsp;this&nbsp;bumps&nbsp;up&nbsp;the&nbsp;cost&nbsp;of&nbsp;each&nbsp;RMQ&nbsp;node,&nbsp;and&nbsp;doesn&amp;#39;t&nbsp;solve&nbsp;the&nbsp;node&nbsp;crash&nbsp;recovery&nbsp;problem,&nbsp;just&nbsp;speeds&nbsp;up&nbsp;the&nbsp;process&nbsp;somewhat.&lt;br&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Web&nbsp;API:&lt;br&gt;&lt;/div&gt;&lt;div&gt;<br>
<br>
-&nbsp;API&nbsp;seems&nbsp;to&nbsp;block&nbsp;when&nbsp;cluster&nbsp;is&nbsp;busy,&nbsp;even&nbsp;for&nbsp;informational&nbsp;GETs,&nbsp;so&nbsp;you&nbsp;can&amp;#39;t&nbsp;determine&nbsp;what&amp;#39;s&nbsp;going&nbsp;on&nbsp;with&nbsp;the&nbsp;cluster.&lt;br&gt;&lt;/div&gt;&lt;div&gt;-&nbsp;Some&nbsp;API&nbsp;operations&nbsp;seem&nbsp;to&nbsp;block&nbsp;until&nbsp;they&nbsp;complete&nbsp;(like&nbsp;putting&nbsp;a&nbsp;new&nbsp;policy),&nbsp;while&nbsp;others&nbsp;return&nbsp;immediately&nbsp;even&nbsp;though&nbsp;they&amp;#39;re&nbsp;definitely&nbsp;not&nbsp;completed&nbsp;yet&nbsp;(like&nbsp;deleting&nbsp;a&nbsp;policy).&nbsp;It&amp;#39;s&nbsp;not&nbsp;documented&nbsp;which&nbsp;have&nbsp;which&nbsp;behaviour,&nbsp;or&nbsp;why&nbsp;they&nbsp;don&amp;#39;t&nbsp;just&nbsp;all&nbsp;block&nbsp;until&nbsp;op&nbsp;is&nbsp;completed.&lt;br&gt;<br>
<br>
&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Hopefully&nbsp;you&nbsp;guys&nbsp;can&nbsp;educate&nbsp;me&nbsp;on&nbsp;what&nbsp;I&amp;#39;m&nbsp;doing&nbsp;wrong&nbsp;in&nbsp;some&nbsp;of&nbsp;these&nbsp;scenarios,&nbsp;or&nbsp;how&nbsp;to&nbsp;mitigate&nbsp;some&nbsp;of&nbsp;these&nbsp;issues.&nbsp;Any&nbsp;issue&nbsp;that&nbsp;requires&nbsp;taking&nbsp;down&nbsp;and&nbsp;restarting&nbsp;the&nbsp;cluster&nbsp;to&nbsp;fix&nbsp;is&nbsp;especially&nbsp;troubling.&lt;br&gt;<br>
<br>
<br>
&lt;br&gt;Thanks,&lt;br&gt;&lt;/div&gt;&lt;div&gt;Graeme&lt;br&gt;&lt;br&gt;<br>
&lt;/div&gt;&lt;/div&gt;<br>
&lt;br&gt;_______________________________________________&lt;br&gt;<br>
rabbitmq-discuss&nbsp;mailing&nbsp;list&lt;br&gt;<br>
&lt;a&nbsp;href=&quot;mailto:rabbitmq-discuss@lists.rabbitmq.com&quot;&gt;rabbitmq-discuss@lists.rabbitmq.com&lt;/a&gt;&lt;br&gt;<br>
&lt;a&nbsp;href=&quot;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&quot;&nbsp;target=&quot;_blank&quot;&gt;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&lt;/a&gt;&lt;br&gt;<br>
&lt;br&gt;&lt;/blockquote&gt;&lt;/div&gt;<br>

</tt>
