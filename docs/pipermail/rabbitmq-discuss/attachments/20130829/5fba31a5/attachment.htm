<tt>
&lt;div&nbsp;dir=&quot;ltr&quot;&gt;Something&nbsp;we&nbsp;did&nbsp;for&nbsp;high&nbsp;latency&nbsp;systems&nbsp;was&nbsp;to&nbsp;publish&nbsp;to&nbsp;a&nbsp;fanout&nbsp;exchange&nbsp;that&amp;#39;s&nbsp;bound&nbsp;to&nbsp;an&nbsp;x-concurrent-hash&nbsp;exchange.&nbsp; Then&nbsp;we&nbsp;multiplex&nbsp;the&nbsp;queues,&nbsp;so&nbsp;instead&nbsp;of&nbsp;&amp;quot;myqueue&amp;quot;&nbsp;we&nbsp;create&nbsp;10&nbsp;queues&nbsp;&amp;quot;myqueue1&amp;quot;...&nbsp;&amp;quot;myqueue10&amp;quot;.&nbsp; As&nbsp;long&nbsp;as&nbsp;ordering&nbsp;isn&amp;#39;t&nbsp;an&nbsp;issue,&nbsp;this&nbsp;allows&nbsp;you&nbsp;to&nbsp;get&nbsp;a&nbsp;greatly&nbsp;increased&nbsp;through-put.&nbsp; But&nbsp;then&nbsp;you&nbsp;have&nbsp;to&nbsp;consume&nbsp;from&nbsp;multiple&nbsp;queues&nbsp;not&nbsp;a&nbsp;single&nbsp;queue.&nbsp; An&nbsp;example&nbsp;of&nbsp;where&nbsp;we&nbsp;use&nbsp;this&nbsp;technique&nbsp;is&nbsp;when&nbsp;dealing&nbsp;with&nbsp;physical&nbsp;location&nbsp;differences&nbsp;(e.g.&nbsp;Chicago&nbsp;to&nbsp;Dallas&nbsp;would&nbsp;be&nbsp;an&nbsp;example)&nbsp;where&nbsp;latency&nbsp;is&nbsp;fairly&nbsp;high.&nbsp; We&nbsp;use&nbsp;shovels&nbsp;(btw,&nbsp;remember&nbsp;to&nbsp;set&nbsp;a&nbsp;prefetch&nbsp;OTHER&nbsp;than&nbsp;0&nbsp;-&nbsp;we&nbsp;hit&nbsp;major&nbsp;performance&nbsp;issues&nbsp;otherwise&nbsp;related&nbsp;to&nbsp;network&nbsp;latency)&nbsp;to&nbsp;transmit&nbsp;from&nbsp;each&nbsp;of&nbsp;the&nbsp;10&nbsp;queues&nbsp;back&nbsp;to&nbsp;a&nbsp;single&nbsp;exchange/queue&nbsp;on&nbsp;the&nbsp;remote&nbsp;side.&lt;div&gt;<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Jason&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&nbsp;class=&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;div&nbsp;class=&quot;gmail_quote&quot;&gt;On&nbsp;Wed,&nbsp;Aug&nbsp;28,&nbsp;2013&nbsp;at&nbsp;4:57&nbsp;AM,&nbsp;Tim&nbsp;Watson&nbsp;&lt;span&nbsp;dir=&quot;ltr&quot;&gt;&amp;lt;&lt;a&nbsp;href=&quot;mailto:tim@rabbitmq.com&quot;&nbsp;target=&quot;_blank&quot;&gt;tim@rabbitmq.com&lt;/a&gt;&amp;gt;&lt;/span&gt;&nbsp;wrote:&lt;br&gt;<br>
&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;Junius,&lt;br&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;&lt;br&gt;<br>
On&nbsp;23&nbsp;Aug&nbsp;2013,&nbsp;at&nbsp;03:14,&nbsp;Junius&nbsp;Wang&nbsp;wrote:&lt;br&gt;<br>
&amp;gt;&nbsp;Before&nbsp;every&nbsp;test&nbsp;(&nbsp;after&nbsp;a&nbsp;new&nbsp;RabbitMQ&nbsp;node&nbsp;added/removed&nbsp;to&nbsp;the&nbsp;cluster):&lt;br&gt;<br>
&amp;gt;&nbsp;1).we&nbsp;reset&nbsp;the&nbsp;policy&nbsp;to&nbsp;&amp;quot;exactly&amp;quot;&nbsp;on&nbsp;2&nbsp;nodes.&nbsp;2).&nbsp;declared&nbsp;10&nbsp;queues&nbsp;using&lt;br&gt;<br>
&amp;gt;&nbsp;rabbitmqctl&nbsp;and&nbsp;force&nbsp;them&nbsp;distributed&nbsp;evenly&nbsp;on&nbsp;all&nbsp;nodes.&nbsp; &lt;a&nbsp;href=&quot;http://e.g.in&quot;&nbsp;target=&quot;_blank&quot;&gt;e.g.in&lt;/a&gt;&nbsp;a&nbsp;2&nbsp;node&lt;br&gt;<br>
&amp;gt;&nbsp;cluster,5&nbsp;queues&nbsp;resides&nbsp;on&nbsp;rabbit1,&nbsp;another&nbsp;5&nbsp;resides&nbsp;on&nbsp;rabbit2.&nbsp;3-3-4&nbsp;for&lt;br&gt;<br>
&amp;gt;&nbsp;3&nbsp;node&nbsp;cluster&nbsp;,3-3-2-2&nbsp;for&nbsp;4&nbsp;node&nbsp;cluster.&nbsp; &nbsp;3)&nbsp;declare&nbsp;a&nbsp;&amp;#39;topic&amp;quot;&nbsp;exchange&lt;br&gt;<br>
&amp;gt;&nbsp;4)&nbsp;binding&nbsp;the&nbsp;10&nbsp;queues&nbsp;to&nbsp;the&nbsp;exchange&nbsp;with&nbsp;10&nbsp;different&nbsp;routing&nbsp;keys.&lt;br&gt;<br>
&amp;gt;&nbsp;Publishers&nbsp;publish&nbsp;messages&nbsp;with&nbsp;the&nbsp;10&nbsp;routing&nbsp;keys&nbsp;by&nbsp;turns.&nbsp;Thus&nbsp;all&lt;br&gt;<br>
&amp;gt;&nbsp;queues&nbsp;receive&nbsp;the&nbsp;same&nbsp;number&nbsp;of&nbsp;messages.&lt;br&gt;<br>
&amp;gt;&nbsp;Is&nbsp;the&nbsp;number&nbsp;of&nbsp;queues&nbsp;large&nbsp;enough?&nbsp; we&nbsp;can&nbsp;declare&nbsp;more&nbsp;queues&nbsp;in&nbsp;test&lt;br&gt;<br>
&amp;gt;&nbsp;but&nbsp;I&nbsp;don&amp;#39;t&nbsp;think&nbsp;there&nbsp;will&nbsp;be&nbsp;too&nbsp;many&nbsp;queues&nbsp;in&nbsp;production.&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Let&amp;#39;s&nbsp;re-visit&nbsp;the&nbsp;point&nbsp;Michael&nbsp;made&nbsp;about&nbsp;adding&nbsp;more&nbsp;queues:&lt;br&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;&lt;br&gt;<br>
&amp;quot;the&nbsp;way&nbsp;you&nbsp;extend&nbsp;capacity&nbsp;with&nbsp;RabbitMQ&nbsp;is&nbsp;by&nbsp;adding&nbsp;nodes&nbsp;and&nbsp;using&nbsp;more&nbsp;queues&amp;quot;&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;In&nbsp;any&nbsp;messaging&nbsp;system&nbsp;where&nbsp;queues&nbsp;exhibit&nbsp;FIFO&nbsp;behaviour,&nbsp;each&nbsp;queue&nbsp;is&nbsp;a&nbsp;concurrency&nbsp;bottleneck,&nbsp;because&nbsp;it&nbsp;can&nbsp;handle&nbsp;only&nbsp;1&nbsp;message&nbsp;at&nbsp;a&nbsp;time.&nbsp;If&nbsp;the&nbsp;queue&nbsp;handled&nbsp;more&nbsp;than&nbsp;1&nbsp;inbound&nbsp;message&nbsp;at&nbsp;a&nbsp;time,&nbsp;it&nbsp;would&nbsp;not&nbsp;be&nbsp;possible&nbsp;to&nbsp;enforce&nbsp;the&nbsp;FIFO&nbsp;characteristics,&nbsp;since&nbsp;without&nbsp;some&nbsp;kind&nbsp;of&nbsp;concurrency&nbsp;control&nbsp;the&nbsp;ordering&nbsp;would&nbsp;be&nbsp;non-deterministic.&nbsp;The&nbsp;question&nbsp;of&nbsp;whether&nbsp;or&nbsp;not&nbsp;the&nbsp;number&nbsp;of&nbsp;queues&nbsp;is&nbsp;&amp;quot;large&nbsp;enough&amp;quot;&nbsp;is&nbsp;application&nbsp;specific.&nbsp;Where&nbsp;you&nbsp;need&nbsp;FIFO&nbsp;ordering,&nbsp;you&amp;#39;ll&nbsp;want&nbsp;a&nbsp;single&nbsp;queue.&nbsp;Where&nbsp;you&nbsp;are&nbsp;processing&nbsp;messages&nbsp;separately&nbsp;and/or&nbsp;do&nbsp;not&nbsp;require&nbsp;ordering&nbsp;(e.g.,&nbsp;between&nbsp;messages&nbsp;of&nbsp;different&nbsp;&amp;quot;types&amp;quot;)&nbsp;then&nbsp;you&nbsp;can&nbsp;use&nbsp;separate&nbsp;queues:&nbsp;this&nbsp;is&nbsp;a&nbsp;design&nbsp;choice&nbsp;that&nbsp;only&nbsp;you&nbsp;can&nbsp;make.&nbsp;The&nbsp;point&nbsp;Michael&nbsp;made&nbsp;is&nbsp;that&nbsp;the&nbsp;more&nbsp;queues&nbsp;you&nbsp;shove&nbsp;messages&nbsp;into,&nbsp;the&nbsp;better&nbsp;the&nbsp;concurrency&nbsp;will&nbsp;be,&nbsp;which&nbsp;in&nbsp;turn&nbsp;may&nbsp;yield&nbsp;performance&nbsp;benefits&nbsp;in&nbsp;terms&nbsp;of&nbsp;throughput.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
As&nbsp;to&nbsp;the&nbsp;point&nbsp;about&nbsp;&amp;quot;adding&nbsp;nodes&amp;quot;&nbsp;-&nbsp;a&nbsp;single&nbsp;RabbitMQ&nbsp;node&nbsp;has&nbsp;a&nbsp;finite&nbsp;processing&nbsp;capacity,&nbsp;dependent&nbsp;on&nbsp;available&nbsp;operating&nbsp;system&nbsp;resources&nbsp;and&nbsp;client&nbsp;usage&nbsp;patterns.&nbsp;If&nbsp;you&nbsp;work&nbsp;out&nbsp;the&nbsp;maximum&nbsp;capacity&nbsp;&amp;quot;n&amp;quot;&nbsp;of&nbsp;a&nbsp;single&nbsp;node&nbsp;in&nbsp;your&nbsp;application,&nbsp;where&nbsp;the&nbsp;combination&nbsp;of&nbsp;available&nbsp;system&nbsp;resources&nbsp;and&nbsp;application&nbsp;load&nbsp;on&nbsp;the&nbsp;broker&nbsp;is&nbsp;at&nbsp;its&nbsp;peak,&nbsp;then&nbsp;by&nbsp;adding&nbsp;a&nbsp;second&nbsp;node,&nbsp;you&nbsp;might&nbsp;increase&nbsp;the&nbsp;system&amp;#39;s&nbsp;overall&nbsp;capacity&nbsp;to&nbsp;something&nbsp;close&nbsp;to&nbsp;2n.&nbsp;There&nbsp;are&nbsp;important&nbsp;caveats&nbsp;here&nbsp;though,&nbsp;as&nbsp;Michael&nbsp;has&nbsp;already&nbsp;pointed&nbsp;out:&nbsp;clients&nbsp;can&nbsp;connect&nbsp;to&nbsp;any&nbsp;node&nbsp;in&nbsp;a&nbsp;cluster,&nbsp;but&nbsp;if&nbsp;the&nbsp;queue&nbsp;with&nbsp;which&nbsp;they&amp;#39;re&nbsp;communicating&nbsp;resides&nbsp;on&nbsp;another&nbsp;node,&nbsp;messages&nbsp;(both&nbsp;in&nbsp;and&nbsp;out&nbsp;of&nbsp;the&nbsp;broker)&nbsp;must&nbsp;be&nbsp;transmitted&nbsp;between&nbsp;nodes.&lt;br&gt;<br>
<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;&lt;br&gt;<br>
&amp;gt;&nbsp;publishers&nbsp;connect&nbsp;to&nbsp;the&nbsp;cluster&nbsp;via&nbsp;load&nbsp;balancer(AWS&nbsp;ELB)&nbsp;as&nbsp;well&nbsp;as&lt;br&gt;<br>
&amp;gt;&nbsp;consumers,&nbsp;they&nbsp;don&amp;#39;t&nbsp;know&nbsp;to&nbsp;which&nbsp;node&nbsp;they&nbsp;are&nbsp;connect.&nbsp; So&nbsp;we&nbsp;may&nbsp;not&lt;br&gt;<br>
&amp;gt;&nbsp;decrease&nbsp;the&nbsp;intra-cluster&nbsp;traffic.&nbsp;But&nbsp; we&nbsp;can&nbsp;try&nbsp;some&nbsp;high&nbsp;bandwidth&lt;br&gt;<br>
&amp;gt;&nbsp;instances.&nbsp;Does&nbsp;this&nbsp;help?&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Increased&nbsp;bandwidth&nbsp;might&nbsp;be&nbsp;helpful,&nbsp;but&nbsp;the&nbsp;points&nbsp;Michael&nbsp;made&nbsp;will&nbsp;still&nbsp;stand.&nbsp;There&nbsp;is&nbsp;always&nbsp;going&nbsp;to&nbsp;be&nbsp;&amp;quot;some&amp;quot;&nbsp;additional&nbsp;overhead&nbsp;due&nbsp;to&nbsp;inter-node&nbsp;communications,&nbsp;because&nbsp;network&nbsp;latency&nbsp;is&nbsp;the&nbsp;dominating&nbsp;factor&nbsp;here.&nbsp;Think&nbsp;about&nbsp;a&nbsp;classic&nbsp;2-tier&nbsp;application:&nbsp;the&nbsp;biggest&nbsp;&amp;quot;cost&amp;quot;&nbsp;is&nbsp;usually&nbsp;communicating&nbsp;with&nbsp;the&nbsp;backend&nbsp;(e.g.,&nbsp;a&nbsp;database&nbsp;or&nbsp;some&nbsp;such),&nbsp;and&nbsp;*that*&nbsp;cost&nbsp;is&nbsp;due&nbsp;to&nbsp;the&nbsp;network&nbsp;round&nbsp;trips&nbsp;more&nbsp;often&nbsp;than&nbsp;not.&lt;br&gt;<br>
<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;&lt;br&gt;<br>
&amp;gt;&nbsp;Another&nbsp;question&nbsp;is&nbsp;that&nbsp;we&nbsp;will&nbsp;have&nbsp;a&nbsp;queue&nbsp;with&nbsp;lots&nbsp;of&nbsp;messages&lt;br&gt;<br>
&amp;gt;&nbsp;handled,&nbsp;perhaps&nbsp;5000/sec(size&nbsp;of&nbsp;2K),&nbsp;which&nbsp;is&nbsp;about&nbsp;90%&nbsp;of&nbsp;the&nbsp;total&lt;br&gt;<br>
&amp;gt;&nbsp;messages&nbsp;handled&nbsp;by&nbsp;the&nbsp;cluster.&nbsp; It&amp;#39;s&nbsp;hard&nbsp;for&nbsp;us&nbsp;to&nbsp;split&nbsp;it&nbsp;into&nbsp;multiple&lt;br&gt;<br>
&amp;gt;&nbsp;queues,&nbsp;it&amp;#39;s&nbsp;really&nbsp;a&nbsp;big&nbsp;design&nbsp;change&nbsp;which&nbsp;we&nbsp;try&nbsp;to&nbsp;avoid.&nbsp;From&nbsp;your&lt;br&gt;<br>
&amp;gt;&nbsp;comments,&nbsp;even&nbsp;if&nbsp;messages&nbsp;are&nbsp;synchronized&nbsp;on&nbsp;only&nbsp;2&nbsp;nodes&nbsp;,&nbsp;hardly&nbsp;we&nbsp;can&lt;br&gt;<br>
&amp;gt;&nbsp;get&nbsp;linear&nbsp;scaling&nbsp;,&nbsp;right?&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;That&amp;#39;s&nbsp;correct.&nbsp;The&nbsp;queue&nbsp;must&nbsp;handle&nbsp;messages&nbsp;in&nbsp;FIFO&nbsp;order&nbsp;and&nbsp;has&nbsp;an&nbsp;upper&nbsp;bound&nbsp;on&nbsp;how&nbsp;fast&nbsp;it&nbsp;can&nbsp;receive&nbsp;from&nbsp;and/or&nbsp;deliver&nbsp;messages&nbsp;to&nbsp;its&nbsp;clients.&lt;br&gt;<br>
&lt;div&nbsp;class=&quot;im&quot;&gt;&lt;br&gt;<br>
&amp;gt;&nbsp;If&nbsp;so,&nbsp;is&nbsp;the&nbsp;network&nbsp;traffic&nbsp;the&nbsp;major&nbsp;factor&nbsp;impacting&nbsp;the&nbsp;performance?&nbsp;If&nbsp;so&nbsp;,we&nbsp;can&nbsp;try&nbsp;some&nbsp;high&nbsp;bandwidth&nbsp;instances&lt;br&gt;<br>
&amp;gt;&nbsp;or&nbsp;using&nbsp;AWS&nbsp;instance&nbsp;group.&nbsp;Hopes&nbsp;that&nbsp;will&nbsp;get&nbsp;better&nbsp;performance.&lt;br&gt;<br>
&lt;br&gt;<br>
&lt;/div&gt;Yes&nbsp;and&nbsp;no.&nbsp;If&nbsp;the&nbsp;clients&nbsp;interacting&nbsp;with&nbsp;the&nbsp;queue&nbsp;are&nbsp;connected&nbsp;to&nbsp;another&nbsp;node,&nbsp;then&nbsp;the&nbsp;cost&nbsp;of&nbsp;inter-node&nbsp;traffic&nbsp;will&nbsp;have&nbsp;an&nbsp;impact.&nbsp;A&nbsp;equally&nbsp;important&nbsp;potential&nbsp;cost&nbsp;you&nbsp;must&nbsp;be&nbsp;aware&nbsp;of&nbsp;though,&nbsp;is&nbsp;that&nbsp;of&nbsp;mirroring&nbsp;the&nbsp;queue&nbsp;across&nbsp;even&nbsp;two&nbsp;cluster&nbsp;nodes.&nbsp;When&nbsp;a&nbsp;queue&nbsp;is&nbsp;mirrored,&nbsp;the&nbsp;master&nbsp;queue&nbsp;process&nbsp;cannot&nbsp;take&nbsp;full&nbsp;responsibility&nbsp;for&nbsp;a&nbsp;message&nbsp;until&nbsp;that&nbsp;message&nbsp;has&nbsp;been&nbsp;accepted&nbsp;by&nbsp;all&nbsp;replicas.&nbsp;Even&nbsp;if&nbsp;the&nbsp;mirroring&nbsp;policy&nbsp;is&nbsp;set&nbsp;to&nbsp;&amp;quot;exactly&nbsp;2&nbsp;nodes&amp;quot;,&nbsp;this&nbsp;still&nbsp;means&nbsp;that&nbsp;a&nbsp;message&nbsp;delivered&nbsp;to&nbsp;the&nbsp;queue&nbsp;on&nbsp;node-1&nbsp;must&nbsp;also&nbsp;be&nbsp;transmitted&nbsp;to&nbsp;node-2.&nbsp;If&nbsp;the&nbsp;message&nbsp;is&nbsp;&amp;#39;persistent&amp;#39;,&nbsp;then&nbsp;both&nbsp;nodes&nbsp;must&nbsp;flush&nbsp;to&nbsp;disk.&nbsp;If&nbsp;confirms&nbsp;are&nbsp;in&nbsp;use,&nbsp;no&nbsp;confirm&nbsp;can&nbsp;be&nbsp;issued&nbsp;until&nbsp;both&nbsp;nodes&nbsp;have&nbsp;received&nbsp;the&nbsp;message,&nbsp;flushed&nbsp;it&nbsp;to&nbsp;disk&nbsp;and&nbsp;the&nbsp;master&nbsp;*knows*&nbsp;that&nbsp;this&nbsp;has&nbsp;taken&nbsp;place.&nbsp;That&amp;#39;s&nbsp;a&nbsp;lot&nbsp;of&nbsp;work&nbsp;per-message.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
So&nbsp;as&nbsp;Michael&nbsp;said,&nbsp;HA/Mirroring&nbsp;is&nbsp;not&nbsp;a&nbsp;feature&nbsp;that&nbsp;helps&nbsp;performance&nbsp;-&nbsp;it&nbsp;is&nbsp;a&nbsp;feature&nbsp;that&nbsp;increases&nbsp;resiliency&nbsp;in&nbsp;case&nbsp;of&nbsp;node&nbsp;failures.&nbsp;Adding&nbsp;bandwidth&nbsp;is&nbsp;only&nbsp;part&nbsp;of&nbsp;the&nbsp;story.&nbsp;Even&nbsp;with&nbsp;more&nbsp;bandwidth,&nbsp;there&nbsp;is&nbsp;still&nbsp;going&nbsp;to&nbsp;be&nbsp;higher&nbsp;latency,&nbsp;not&nbsp;only&nbsp;because&nbsp;of&nbsp;the&nbsp;amount&nbsp;of&nbsp;inter-node&nbsp;traffic,&nbsp;but&nbsp;also&nbsp;because&nbsp;each&nbsp;node&nbsp;has&nbsp;to&nbsp;complete&nbsp;some&nbsp;work&nbsp;in&nbsp;order&nbsp;to&nbsp;keep&nbsp;the&nbsp;replicas&nbsp;in&nbsp;sync.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
I&nbsp;would&nbsp;suggest&nbsp;that&nbsp;you&nbsp;look&nbsp;very&nbsp;hard&nbsp;at&nbsp;that&nbsp;&amp;quot;queue&nbsp;with&nbsp;lots&nbsp;of&nbsp;messages&nbsp;handled&amp;quot;&nbsp;and&nbsp;see&nbsp;if&nbsp;you&nbsp;can&nbsp;split&nbsp;it&nbsp;up.&nbsp;Imagine&nbsp;if&nbsp;you&nbsp;were&nbsp;writing&nbsp;to&nbsp;a&nbsp;database&nbsp;and&nbsp;you&nbsp;had&nbsp;this&nbsp;one&nbsp;giant&nbsp;table&nbsp;that&nbsp;almost&nbsp;every&nbsp;transaction&nbsp;had&nbsp;to&nbsp;write&nbsp;to.&nbsp;This&nbsp;would&nbsp;become&nbsp;a&nbsp;bottleneck&nbsp;quickly&nbsp;and&nbsp;you&amp;#39;d&nbsp;look&nbsp;to&nbsp;split&nbsp;it&nbsp;up&nbsp;if&nbsp;possible,&nbsp;or&nbsp;if&nbsp;not&nbsp;then&nbsp;at&nbsp;least&nbsp;to&nbsp;partition&nbsp;it&nbsp;based&nbsp;on&nbsp;suitable&nbsp;indexes.&nbsp;We&nbsp;don&amp;#39;t&nbsp;have&nbsp;a&nbsp;feature&nbsp;for&nbsp;&amp;quot;partitioning&nbsp;a&nbsp;queue&nbsp;over&nbsp;multiple&nbsp;nodes&amp;quot;&nbsp;yet,&nbsp;so&nbsp;you&amp;#39;ll&nbsp;have&nbsp;to&nbsp;think&nbsp;about&nbsp;partitioning&nbsp;the&nbsp;messages&nbsp;yourself.&nbsp;If&nbsp;you&nbsp;can&nbsp;do&nbsp;that,&nbsp;and&nbsp;therefore&nbsp;increase&nbsp;the&nbsp;number&nbsp;of&nbsp;queues&nbsp;that&nbsp;are&nbsp;able&nbsp;to&nbsp;concurrently&nbsp;handle&nbsp;messages,&nbsp;you&nbsp;should&nbsp;get&nbsp;a&nbsp;decent&nbsp;performance&nbsp;increase.&lt;br&gt;<br>
<br>
&lt;br&gt;<br>
HTH.&lt;br&gt;<br>
&lt;span&nbsp;class=&quot;HOEnZb&quot;&gt;&lt;font&nbsp;color=&quot;#888888&quot;&gt;&lt;br&gt;<br>
Tim&lt;br&gt;<br>
&lt;/font&gt;&lt;/span&gt;&lt;div&nbsp;class=&quot;HOEnZb&quot;&gt;&lt;div&nbsp;class=&quot;h5&quot;&gt;&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;-----Original&nbsp;Message-----&lt;br&gt;<br>
&amp;gt;&nbsp;From:&nbsp;&lt;a&nbsp;href=&quot;mailto:rabbitmq-discuss-bounces@lists.rabbitmq.com&quot;&gt;rabbitmq-discuss-bounces@lists.rabbitmq.com&lt;/a&gt;&lt;br&gt;<br>
&amp;gt;&nbsp;[mailto:&lt;a&nbsp;href=&quot;mailto:rabbitmq-discuss-bounces@lists.rabbitmq.com&quot;&gt;rabbitmq-discuss-bounces@lists.rabbitmq.com&lt;/a&gt;]&nbsp;On&nbsp;Behalf&nbsp;Of&nbsp;Michael&lt;br&gt;<br>
&amp;gt;&nbsp;Klishin&lt;br&gt;<br>
&amp;gt;&nbsp;Sent:&nbsp;Thursday,&nbsp;August&nbsp;22,&nbsp;2013&nbsp;4:56&nbsp;PM&lt;br&gt;<br>
&amp;gt;&nbsp;To:&nbsp;Discussions&nbsp;about&nbsp;RabbitMQ&lt;br&gt;<br>
&amp;gt;&nbsp;Subject:&nbsp;Re:&nbsp;[rabbitmq-discuss]&nbsp;questions&nbsp;about&nbsp;RabbitMQ&nbsp;linear&nbsp;scalability&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;Junius&nbsp;Wang:&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&amp;gt;&nbsp;1.&nbsp; &nbsp; &nbsp; &nbsp;The&nbsp;throughput&nbsp;of&nbsp;two&nbsp;node&nbsp;cluster&nbsp;is&nbsp;50%-60%&nbsp;worse&nbsp;than&nbsp;a&nbsp;single&lt;br&gt;<br>
&amp;gt;&nbsp;node&nbsp;broker.&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;With&nbsp;mirroring,&nbsp;messages&nbsp;have&nbsp;to&nbsp;be&nbsp;copied&nbsp;to&nbsp;multiple&nbsp;(N&nbsp;or,&nbsp;depending&nbsp;on&lt;br&gt;<br>
&amp;gt;&nbsp;configuration,&nbsp;even&nbsp;all)&nbsp;nodes&nbsp;in&nbsp;the&nbsp;cluster.&nbsp;That&nbsp;obviously&nbsp;takes&nbsp;more&lt;br&gt;<br>
&amp;gt;&nbsp;time&nbsp;than&nbsp;not&nbsp;copying&nbsp;anything.&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&amp;gt;&nbsp;2.&nbsp; &nbsp; &nbsp; &nbsp;Adding&nbsp;more&nbsp;node&nbsp;did&nbsp;have&nbsp;improvement&nbsp;on&nbsp;throughput&nbsp;but&nbsp;we&nbsp;only&lt;br&gt;<br>
&amp;gt;&nbsp;got&nbsp;25%&nbsp;improvement(throughput&nbsp;of&nbsp;3&nbsp;node&nbsp;cluster&nbsp;is&nbsp;25%&nbsp;better&nbsp;than&nbsp;2&nbsp;node&lt;br&gt;<br>
&amp;gt;&nbsp;cluster.&nbsp;4&nbsp;node&nbsp;cluster&nbsp;is&nbsp;25%&nbsp;better&nbsp;than&nbsp;3&nbsp;node&nbsp;cluster&nbsp;too).&nbsp;What&nbsp;we&lt;br&gt;<br>
&amp;gt;&nbsp;expected&nbsp;is&nbsp;a&nbsp;45-degree&nbsp;line,&nbsp;that&nbsp;means&nbsp;when&nbsp;2&nbsp;nodes&nbsp;are&nbsp;used&nbsp;the&lt;br&gt;<br>
&amp;gt;&nbsp;throughput&nbsp;is&nbsp;double.&nbsp;With&nbsp;3&nbsp;nodes,&nbsp;then&nbsp;triple.&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;You&nbsp;are&nbsp;not&nbsp;providing&nbsp;any&nbsp;details&nbsp;about&nbsp;your&nbsp;workload&nbsp;but&nbsp;the&nbsp;way&nbsp;you&nbsp;extend&lt;br&gt;<br>
&amp;gt;&nbsp;capacity&nbsp;with&nbsp;RabbitMQ&nbsp;is&nbsp;by&nbsp;adding&nbsp;nodes&nbsp;and&nbsp;using&nbsp;more&nbsp;queues.&nbsp;Queue&lt;br&gt;<br>
&amp;gt;&nbsp;mirroring&nbsp;is&nbsp;an&nbsp;HA&nbsp;feature,&nbsp;which&nbsp;means&nbsp;copying&nbsp;more&nbsp;data&nbsp;across&nbsp;the&lt;br&gt;<br>
&amp;gt;&nbsp;cluster.&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;Maximum&nbsp;throughput&nbsp;and&nbsp;highest&nbsp;availability&nbsp;are&nbsp;largely&nbsp;at&nbsp;odds&nbsp;with&nbsp;each&lt;br&gt;<br>
&amp;gt;&nbsp;other,&nbsp;so&nbsp;you&nbsp;need&nbsp;to&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;1.&nbsp;Use&nbsp;multiple&nbsp;queues&nbsp;(and&nbsp;the&nbsp;number&nbsp;should&nbsp;grow&nbsp;with&nbsp;the&nbsp;number&nbsp;of&nbsp;nodes)&lt;br&gt;<br>
&amp;gt;&nbsp;2.&nbsp;Choose&nbsp;what&nbsp;queues&nbsp;to&nbsp;mirror.&nbsp;Likely&nbsp;not&nbsp;all&nbsp;queues&nbsp;are&nbsp;equally&nbsp;important&lt;br&gt;<br>
&amp;gt;&nbsp;to&nbsp;your&nbsp;system,&nbsp;so&nbsp;you&nbsp;can&nbsp;make&nbsp;some&nbsp;of&nbsp;them&nbsp;non-HA.&lt;br&gt;<br>
&amp;gt;&nbsp;3.&nbsp;Configure&nbsp;mirroring&nbsp;to,&nbsp;say,&nbsp;2&nbsp;nodes&nbsp;instead&nbsp;of&nbsp;&amp;quot;all&amp;quot;.&lt;br&gt;<br>
&amp;gt;&nbsp;4.&nbsp;If&nbsp;you&nbsp;know&nbsp;what&nbsp;node&nbsp;is&nbsp;the&nbsp;master&nbsp;for&nbsp;a&nbsp;particular&nbsp;queue&nbsp;(e.g.&nbsp;was&lt;br&gt;<br>
&amp;gt;&nbsp;declared&nbsp;on&nbsp;that&nbsp;node),&lt;br&gt;<br>
&amp;gt;&nbsp; &nbsp; &nbsp;make&nbsp;your&nbsp;clients&nbsp;connect&nbsp;there.&nbsp;It&nbsp;will&nbsp;decrease&nbsp;intra-cluster&lt;br&gt;<br>
&amp;gt;&nbsp;traffic.&lt;br&gt;<br>
&amp;gt;&nbsp;--&lt;br&gt;<br>
&amp;gt;&nbsp;MK&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&lt;br&gt;<br>
&amp;gt;&nbsp;_______________________________________________&lt;br&gt;<br>
&amp;gt;&nbsp;rabbitmq-discuss&nbsp;mailing&nbsp;list&lt;br&gt;<br>
&amp;gt;&nbsp;&lt;a&nbsp;href=&quot;mailto:rabbitmq-discuss@lists.rabbitmq.com&quot;&gt;rabbitmq-discuss@lists.rabbitmq.com&lt;/a&gt;&lt;br&gt;<br>
&amp;gt;&nbsp;&lt;a&nbsp;href=&quot;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&quot;&nbsp;target=&quot;_blank&quot;&gt;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&lt;/a&gt;&lt;br&gt;<br>
&lt;br&gt;<br>
_______________________________________________&lt;br&gt;<br>
rabbitmq-discuss&nbsp;mailing&nbsp;list&lt;br&gt;<br>
&lt;a&nbsp;href=&quot;mailto:rabbitmq-discuss@lists.rabbitmq.com&quot;&gt;rabbitmq-discuss@lists.rabbitmq.com&lt;/a&gt;&lt;br&gt;<br>
&lt;a&nbsp;href=&quot;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&quot;&nbsp;target=&quot;_blank&quot;&gt;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&lt;/a&gt;&lt;br&gt;<br>
&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;br&nbsp;clear=&quot;all&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;--&nbsp;&lt;br&gt;Jason&nbsp;McIntosh&lt;br&gt;&lt;a&nbsp;href=&quot;http://mcintosh.poetshome.com/blog/&quot;&gt;http://mcintosh.poetshome.com/blog/&lt;/a&gt;&lt;br&gt;573-424-7612<br>
&lt;/div&gt;<br>

</tt>
