<tt>
&lt;div&nbsp;dir=&quot;ltr&quot;&gt;On&nbsp;your&nbsp;clustering&nbsp;and&nbsp;clients&nbsp;connecting&nbsp;transparently&nbsp;-&nbsp;I&nbsp;highly&nbsp;recommend&nbsp;a&nbsp;load&nbsp;balancer&nbsp;in&nbsp;front&nbsp;of&nbsp;your&nbsp;Rabbit&nbsp;servers.&nbsp; With&nbsp;tcp-half&nbsp;open&nbsp;monitoring&nbsp;on&nbsp;the&nbsp;rabbit&nbsp;port,&nbsp;you&nbsp;can&nbsp;tell&nbsp;pretty&nbsp;quickly&nbsp;when&nbsp;a&nbsp;node/site&nbsp;goes&nbsp;down,&nbsp;and&nbsp;then&nbsp;get&nbsp;failover&nbsp;to&nbsp;one&nbsp;of&nbsp;the&nbsp;other&nbsp;nodes/clusters.&nbsp; With&nbsp;clustering&nbsp;and&nbsp;mirrored&nbsp;queues&nbsp;and&nbsp;by&nbsp;using&nbsp;publisher&nbsp;confirms&nbsp;you&#39;ll&nbsp;avoid&nbsp;data&nbsp;loss&nbsp;this&nbsp;way.&nbsp; You&nbsp;CAN&nbsp;get&nbsp;data&nbsp;duplication&nbsp;though.&nbsp; But&nbsp;I&#39;d&nbsp;only&nbsp;recommend&nbsp;clustering&nbsp;over&nbsp;a&nbsp;really&nbsp;reliable&nbsp;link.&nbsp; If&nbsp;you&#39;re&nbsp;going&nbsp;across&nbsp;a&nbsp;WAN&nbsp;-&nbsp;use&nbsp;shovel/federations&nbsp;to&nbsp;replicate&nbsp;messages&nbsp;to&nbsp;rabbit&nbsp;clusters&nbsp;on&nbsp;the&nbsp;other&nbsp;side,&nbsp;vs.&nbsp;trying&nbsp;to&nbsp;do&nbsp;cross-wan&nbsp;clusters.&nbsp; You&nbsp;could&nbsp;for&nbsp;run&nbsp;a&nbsp;cluster&nbsp;in&nbsp;each&nbsp;site&nbsp;and&nbsp;use&nbsp;federation&nbsp;to&nbsp;send&nbsp;messages&nbsp;to&nbsp;the&nbsp;other&nbsp;cluster&nbsp;as&nbsp;needed.&nbsp; If&nbsp;any&nbsp;given&nbsp;site&nbsp;goes&nbsp;down,&nbsp;your&nbsp;load&nbsp;balancer&nbsp;could&nbsp;switch&nbsp;traffic&nbsp;to&nbsp;the&nbsp;other&nbsp;cluster.&nbsp; There&#39;s&nbsp;still&nbsp;a&nbsp;chance&nbsp;for&nbsp;downtime,&nbsp;but&nbsp;it&#39;s&nbsp;pretty&nbsp;minimal.&nbsp; We&nbsp;use&nbsp;this&nbsp;to&nbsp;redirect&nbsp;traffic&nbsp;to&nbsp;any&nbsp;given&nbsp;node&nbsp;in&nbsp;the&nbsp;cluster&nbsp;right&nbsp;now&nbsp;so&nbsp;if&nbsp;a&nbsp;single&nbsp;node&nbsp;fails,&nbsp;the&nbsp;load&nbsp;balancers&nbsp;pull&nbsp;that&nbsp;node&nbsp;out&nbsp;of&nbsp;service&nbsp;automatically.&lt;div&gt;<br>
&lt;br&gt;&lt;/div&gt;&lt;div&gt;Regarding&nbsp;question&nbsp;2&nbsp;-&nbsp;if&nbsp;you&nbsp;design&nbsp;it&nbsp;right,&nbsp;using&nbsp;confirms&nbsp;(the&nbsp;default&nbsp;in&nbsp;most&nbsp;clients&nbsp;as&nbsp;i&nbsp;understand&nbsp;it),&nbsp;and&nbsp;use&nbsp;persistent&nbsp;messages,&nbsp;you&#39;ll&nbsp;never&nbsp;get&nbsp;message&nbsp;loss&nbsp;with&nbsp;a&nbsp;mirrored&nbsp;queue,&nbsp;unless&nbsp;ALL&nbsp;servers&nbsp;completely&nbsp;crash&nbsp;and&nbsp;hard&nbsp;drives&nbsp;die.&nbsp; At&nbsp;least&nbsp;this&nbsp;is&nbsp;my&nbsp;understanding&nbsp;:)&lt;div&gt;<br>
&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;Last&nbsp;point&nbsp;-&nbsp;you&nbsp;may&nbsp;want&nbsp;to&nbsp;do&nbsp;manual&nbsp;handling&nbsp;of&nbsp;this&nbsp;situation&nbsp;if&nbsp;it&#39;s&nbsp;that&nbsp;much&nbsp;of&nbsp;concern.&nbsp; e.g.&nbsp;let&nbsp;the&nbsp;nodes&nbsp;remain&nbsp;partitioned,&nbsp;let&nbsp;all&nbsp;messages&nbsp;empty&nbsp;(again,&nbsp;remember&nbsp;there&nbsp;would&nbsp;be&nbsp;duplicates&nbsp;possible),&nbsp;then&nbsp;restrict&nbsp;access&nbsp;to&nbsp;the&nbsp;bad&nbsp;nodes&nbsp;to&nbsp;anything&nbsp;but&nbsp;your&nbsp;consumer&nbsp;processes,&nbsp;shut&nbsp;down&nbsp;the&nbsp;&quot;bad&quot;&nbsp;nodes&nbsp;and&nbsp;bring&nbsp;them&nbsp;back&nbsp;up.&nbsp; They&#39;d&nbsp;not&nbsp;have&nbsp;any&nbsp;messages,&nbsp;and&nbsp;they&#39;d&nbsp;get&nbsp;their&nbsp;queues/exchanges&nbsp;from&nbsp; your&nbsp;&quot;master&quot;&nbsp;node&nbsp;that&nbsp;was&nbsp;good&nbsp;when&nbsp;they&nbsp;come&nbsp;back&nbsp;up.&nbsp; In&nbsp;the&nbsp;case&nbsp;of&nbsp;a&nbsp;load&nbsp;balancer&nbsp;in&nbsp;front,&nbsp;you&nbsp;could&nbsp;use&nbsp;your&nbsp;load&nbsp;balancer&nbsp;to&nbsp;control&nbsp;this&nbsp;very&nbsp;effectively.&lt;/div&gt;<br>
&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Definitely&nbsp;read&nbsp;through&nbsp;partitioning&nbsp;and&nbsp;reliability&nbsp;documentation&nbsp;and&nbsp;actually&nbsp;try&nbsp;these&nbsp;scenarios:&lt;/div&gt;&lt;div&gt;&lt;a&nbsp;href=&quot;https://www.rabbitmq.com/partitions.html&quot;&gt;https://www.rabbitmq.com/partitions.html&lt;/a&gt;&lt;br&gt;<br>
&lt;/div&gt;&lt;div&gt;&lt;a&nbsp;href=&quot;https://www.rabbitmq.com/reliability.html&quot;&gt;https://www.rabbitmq.com/reliability.html&lt;/a&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Jason&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&nbsp;class=&quot;gmail_extra&quot;&gt;<br>
&lt;br&gt;&lt;br&gt;&lt;div&nbsp;class=&quot;gmail_quote&quot;&gt;On&nbsp;Thu,&nbsp;May&nbsp;22,&nbsp;2014&nbsp;at&nbsp;8:04&nbsp;AM,&nbsp;Steffen&nbsp;Daniel&nbsp;Jensen&nbsp;&lt;span&nbsp;dir=&quot;ltr&quot;&gt;&lt;&lt;a&nbsp;href=&quot;mailto:steffen.daniel.jensen@gmail.com&quot;&nbsp;target=&quot;_blank&quot;&gt;steffen.daniel.jensen@gmail.com&lt;/a&gt;&gt;&lt;/span&gt;&nbsp;wrote:&lt;br&gt;<br>
&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:0&nbsp;0&nbsp;0&nbsp;.8ex;border-left:1px&nbsp;#ccc&nbsp;solid;padding-left:1ex&quot;&gt;&lt;div&nbsp;dir=&quot;ltr&quot;&gt;&lt;div&gt;We&nbsp;have&nbsp;two&nbsp;data&nbsp;centers&nbsp;connected&nbsp;closely&nbsp;by&nbsp;LAN.&nbsp;&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;We&nbsp;are&nbsp;interested&nbsp;in&nbsp;a&nbsp;*reliable&nbsp;cluster*&nbsp;setup.&nbsp;It&nbsp;must&nbsp;be a&nbsp;cluster&nbsp;because&nbsp;we&nbsp;want&nbsp;clients&nbsp;to&nbsp;be&nbsp;able&nbsp;to&nbsp;connect&nbsp;to&nbsp;each&nbsp;node&nbsp;transparently.&nbsp;Federation&nbsp;is&nbsp;not&nbsp;an&nbsp;option.&lt;/div&gt;<br>
&lt;div&gt;&lt;div&gt;1.&nbsp;It&nbsp;happens&nbsp;that&nbsp;the&nbsp;firewall/switch&nbsp;is&nbsp;restarted,&nbsp;and&nbsp;maybe&nbsp;a&nbsp;few&nbsp;ping&nbsp;messages&nbsp;are&nbsp;lost.&lt;/div&gt;&lt;div&gt;2.&nbsp;The&nbsp;setup&nbsp;should&nbsp;survive&nbsp;data&nbsp;center&nbsp;crash&lt;/div&gt;&lt;/div&gt;&lt;div&gt;3.&nbsp;All&nbsp;queues&nbsp;are&nbsp;durable&nbsp;and&nbsp;mirrored,&nbsp;all&nbsp;messages&nbsp;are&nbsp;persisted,&nbsp;all&nbsp;publishes&nbsp;are&nbsp;confirmed&lt;/div&gt;<br>
&lt;div&gt; &lt;/div&gt;&lt;div&gt;There&nbsp;are&nbsp;3&nbsp;cluster-recovery&nbsp;settings&lt;/div&gt;&lt;div&gt;a)&nbsp;&lt;span&gt;ignore:&lt;/span&gt;&nbsp;A&nbsp;cross&nbsp;data&nbsp;center&nbsp;network&nbsp;break-down&nbsp;would&nbsp;cause&nbsp;message&nbsp;loss&nbsp;on&nbsp;the&nbsp;node&nbsp;that&nbsp;is&nbsp;restarted&nbsp;In&nbsp;order&nbsp;to&nbsp;rejoin.&lt;/div&gt;&lt;div&gt;b)&nbsp;&lt;span&gt;pause_minority:&lt;/span&gt;&nbsp;If&nbsp;we&nbsp;choose&nbsp;the&nbsp;same&nbsp;number&nbsp;of&nbsp;nodes&nbsp;in&nbsp;each&nbsp;data&nbsp;center,&nbsp;the&nbsp;whole&nbsp;cluster&nbsp;will&nbsp;pause.&nbsp;If&nbsp;we&nbsp;don&#39;t,&nbsp;only&nbsp;the&nbsp;data&nbsp;center&nbsp;with&nbsp;the&nbsp;most&nbsp;nodes&nbsp;can&nbsp;survive.&nbsp;&lt;/div&gt;<br>
&lt;div&gt;c)&nbsp;auto_heal:&nbsp;If&nbsp;the&nbsp;cluster&nbsp;decides&nbsp;network&nbsp;partitioning,&nbsp;there&nbsp;is&nbsp;a&nbsp;potential&nbsp;of&nbsp;message&nbsp;loss, when&nbsp;joining.&lt;/div&gt;&lt;div&gt;[I&nbsp;would&nbsp;really&nbsp;like&nbsp;a&nbsp;resync-setting&nbsp;similar&nbsp;to&nbsp;the&nbsp;one&nbsp;described&nbsp;below]&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;<br>
Question&nbsp;1:&nbsp;Is it even&nbsp;possible&nbsp;to&nbsp;have&nbsp;a&nbsp;fully&nbsp;reliable&nbsp;setup&nbsp;in&nbsp;such&nbsp;a&nbsp;setting?&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;In&nbsp;reality&nbsp;we&nbsp;probably&nbsp;won&#39;t&nbsp;have&nbsp;actual&nbsp;network&nbsp;partitions,&nbsp;and&nbsp;it&nbsp;will&nbsp;most probably&nbsp;only&nbsp;be&nbsp;a&nbsp;very&nbsp;short&nbsp;network downtime.&lt;/div&gt;<br>
&lt;div&gt; &lt;/div&gt;&lt;div&gt;Question&nbsp;2: Is&nbsp;it&nbsp;possible&nbsp;to&nbsp;adjust&nbsp;how&nbsp;long&nbsp;it&nbsp;takes&nbsp;rabbitmq&nbsp;to&nbsp;decide&nbsp;&quot;node&nbsp;down&quot;?&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;It&nbsp;is&nbsp;much&nbsp;better to&nbsp;have&nbsp;a&nbsp;halted&nbsp;rabbitmq&nbsp;for some&nbsp;seconds&nbsp;than&nbsp;to&nbsp;have&nbsp;message&nbsp;loss.&lt;/div&gt;<br>
&lt;div&gt; &lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;Question&nbsp;3:&nbsp;Assume&nbsp;that&nbsp;we&nbsp;are&nbsp;using&nbsp;the&nbsp;ignore&nbsp;setting,&nbsp;and&nbsp;that&nbsp;we&nbsp;have&nbsp;only&nbsp;two&nbsp;nodes&nbsp;in&nbsp;the&nbsp;cluster.&nbsp;Would&nbsp;the&nbsp;following&nbsp;be&nbsp;a&nbsp;full&nbsp;recovery&nbsp;with&nbsp;zero&nbsp;message&nbsp;loss?&nbsp;&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;0.&nbsp;Decide&nbsp;which&nbsp;node&nbsp;survives,&nbsp;Ns,&nbsp;and&nbsp;which&nbsp;should&nbsp;be&nbsp;restarted,&nbsp;Nr.&lt;/div&gt;<br>
&lt;div&gt;1.&nbsp;Refuse&nbsp;all&nbsp;connections&nbsp;to&nbsp;Nr&nbsp;except&nbsp;from&nbsp;a&nbsp;special&nbsp;recovery&nbsp;application.&nbsp;(One&nbsp;could&nbsp;change&nbsp;the&nbsp;ip,&nbsp;so&nbsp;all&nbsp;running&nbsp;services&nbsp;can&#39;t&nbsp;connect or&nbsp;similar)&lt;/div&gt;&lt;div&gt;2.&nbsp;Consume&nbsp;and&nbsp;republish&nbsp;all&nbsp;message&nbsp;from&nbsp;Nr&nbsp;to&nbsp;Ns.&lt;/div&gt;<br>
&lt;div&gt;3.&nbsp;Restart&nbsp;Nr&lt;/div&gt;&lt;div&gt;Then&nbsp;the&nbsp;cluster&nbsp;should&nbsp;be&nbsp;up-and-running&nbsp;again.&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;Since&nbsp;all&nbsp;queues&nbsp;are&nbsp;mirrored,&nbsp;all&nbsp;messages&nbsp;published&nbsp;in&nbsp;the&nbsp;partition&nbsp;time&nbsp;is&nbsp;preserved.&nbsp;If&nbsp;a&nbsp;certain&nbsp;service&nbsp;lives&nbsp;only&nbsp;in&nbsp;the&nbsp;one&nbsp;data&nbsp;center,&nbsp;messages&nbsp;will&nbsp;pile&nbsp;up&nbsp;in&nbsp;the&nbsp;other&nbsp;(if&nbsp;there&nbsp;are&nbsp;any&nbsp;publishes).&lt;/div&gt;<br>
&lt;div&gt; &lt;/div&gt;&lt;div&gt;If&nbsp;you&nbsp;have&nbsp;any&nbsp;other&nbsp;suggestions,&nbsp;I&nbsp;would&nbsp;be&nbsp;very&nbsp;interested&nbsp;to&nbsp;hear&nbsp;them.&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;div&gt;I&nbsp;would&nbsp;be&nbsp;really&nbsp;sad&nbsp;to&nbsp;find&nbsp;it&nbsp;necessary&nbsp;to&nbsp;choose&nbsp;Tibco&nbsp;ESB&nbsp;over&nbsp;RabbitMQ,&nbsp;for&nbsp;this&nbsp;reason.&lt;/div&gt;&lt;div&gt; &lt;/div&gt;<br>
&lt;div&gt;Thank&nbsp;you,&lt;/div&gt;&lt;div&gt;--&nbsp;Steffen&lt;/div&gt;&lt;/div&gt;&lt;br&gt;_______________________________________________&lt;br&gt;<br>
rabbitmq-discuss&nbsp;mailing&nbsp;list&lt;br&gt;<br>
&lt;a&nbsp;href=&quot;mailto:rabbitmq-discuss@lists.rabbitmq.com&quot;&gt;rabbitmq-discuss@lists.rabbitmq.com&lt;/a&gt;&lt;br&gt;<br>
&lt;a&nbsp;href=&quot;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&quot;&nbsp;target=&quot;_blank&quot;&gt;https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss&lt;/a&gt;&lt;br&gt;<br>
&lt;br&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;br&nbsp;clear=&quot;all&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;--&nbsp;&lt;br&gt;&lt;div&nbsp;dir=&quot;ltr&quot;&gt;Jason&nbsp;McIntosh&lt;br&gt;&lt;a&nbsp;href=&quot;https://github.com/jasonmcintosh/&quot;&nbsp;target=&quot;_blank&quot;&gt;https://github.com/jasonmcintosh/&lt;/a&gt;&lt;br&gt;573-424-7612&lt;/div&gt;<br>
&lt;/div&gt;<br>

</tt>
