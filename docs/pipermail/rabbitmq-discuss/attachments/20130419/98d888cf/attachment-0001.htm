<tt>
We&nbsp;have&nbsp;also&nbsp;recently&nbsp;upgraded&nbsp;to&nbsp;3.0.4&nbsp;and&nbsp;have&nbsp;since&nbsp;then&nbsp;had&nbsp;2&nbsp;outages.&nbsp;In&nbsp;the&nbsp;one&nbsp;case&nbsp;the&nbsp;service&nbsp;was&nbsp;running&nbsp;but&nbsp;non&nbsp;functional.&nbsp;The&nbsp;logs&nbsp;didn't&nbsp;have&nbsp;errors,&nbsp;but&nbsp;at&nbsp;a&nbsp;certain&nbsp;point&nbsp;just&nbsp;stopped&nbsp;receiving&nbsp;new&nbsp;connections.&nbsp;We&nbsp;had&nbsp;to&nbsp;restart&nbsp;the&nbsp;service&nbsp;and&nbsp;all&nbsp;was&nbsp;well&nbsp;until&nbsp;about&nbsp;a&nbsp;week&nbsp;later&nbsp;when&nbsp;there&nbsp;were&nbsp;a&nbsp;lot&nbsp;of&nbsp;heaped&nbsp;up&nbsp;messages&nbsp;server&nbsp;side&nbsp;but&nbsp;clients&nbsp;could&nbsp;not&nbsp;connect&nbsp;to&nbsp;the&nbsp;queue&nbsp;anymore.&nbsp;(server&nbsp;actively&nbsp;refused&nbsp;connection&nbsp;message&nbsp;from&nbsp;the&nbsp;client&nbsp;side).&nbsp;We&nbsp;will&nbsp;be&nbsp;downgrading&nbsp;to&nbsp;2.8.x&nbsp;in&nbsp;the&nbsp;mean&nbsp;time.&lt;br&gt;&lt;br&gt;On&nbsp;Friday,&nbsp;April&nbsp;12,&nbsp;2013&nbsp;8:36:22&nbsp;PM&nbsp;UTC+2,&nbsp;Matt&nbsp;Wise&nbsp;wrote:&lt;blockquote&nbsp;class=&quot;gmail_quote&quot;&nbsp;style=&quot;margin:&nbsp;0;margin-left:&nbsp;0.8ex;border-left:&nbsp;1px&nbsp;#ccc&nbsp;solid;padding-left:&nbsp;1ex;&quot;&gt;We've&nbsp;been&nbsp;running&nbsp;RabbitMQ&nbsp;2.8.x&nbsp;in&nbsp;production&nbsp;in&nbsp;Amazon&nbsp;for&nbsp;about&nbsp;16&nbsp;months&nbsp;now&nbsp;without&nbsp;very&nbsp;many&nbsp;issues.&nbsp;Last&nbsp;week&nbsp;we&nbsp;ran&nbsp;into&nbsp;an&nbsp;issue&nbsp;where&nbsp;our&nbsp;2.8.5&nbsp;cluster&nbsp;nodes&nbsp;hit&nbsp;their&nbsp;high-memory-limit&nbsp;and&nbsp;stopped&nbsp;processing&nbsp;jobs,&nbsp;effectively&nbsp;taking&nbsp;down&nbsp;our&nbsp;entire&nbsp;Celery&nbsp;task&nbsp;queue.&nbsp;We&nbsp;decided&nbsp;to&nbsp;upgrade&nbsp;the&nbsp;software&nbsp;to&nbsp;3.0.4&nbsp;(which&nbsp;had&nbsp;been&nbsp;running&nbsp;in&nbsp;staging&nbsp;for&nbsp;a&nbsp;few&nbsp;weeks,&nbsp;as&nbsp;a&nbsp;single&nbsp;instance,&nbsp;without&nbsp;issue)&nbsp;and&nbsp;at&nbsp;the&nbsp;same&nbsp;time&nbsp;beef&nbsp;up&nbsp;the&nbsp;size&nbsp;and&nbsp;redundancy&nbsp;of&nbsp;our&nbsp;farm&nbsp;to&nbsp;3&nbsp;machines&nbsp;that&nbsp;were&nbsp;m1.larges.&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Old&nbsp;Farm:&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server1:&nbsp;m1.small,&nbsp;2.8.5,&nbsp;us-west-1c&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server2:&nbsp;m1.small,&nbsp;2.8.5,&nbsp;us-west-1c&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;New&nbsp;Farm:&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server1:&nbsp;m1.large,&nbsp;3.0.4,&nbsp;us-west-1a&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server2:&nbsp;m1.large,&nbsp;3.0.4,&nbsp;us-west-1c&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server3:&nbsp;m1.large,&nbsp;3.0.4,&nbsp;us-west-1c&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Since&nbsp;creating&nbsp;the&nbsp;new&nbsp;server&nbsp;farm&nbsp;though&nbsp;we've&nbsp;had&nbsp;3&nbsp;outages.&nbsp;In&nbsp;the&nbsp;first&nbsp;two&nbsp;outages&nbsp;we&nbsp;received&nbsp;a&nbsp;Network&nbsp;Partition&nbsp;Split,&nbsp;and&nbsp;effectively&nbsp;all&nbsp;3&nbsp;of&nbsp;the&nbsp;systems&nbsp;decided&nbsp;to&nbsp;run&nbsp;their&nbsp;own&nbsp;queues&nbsp;independently&nbsp;of&nbsp;the&nbsp;other&nbsp;servers.&nbsp;This&nbsp;was&nbsp;the&nbsp;first&nbsp;time&nbsp;we'd&nbsp;ever&nbsp;seen&nbsp;this&nbsp;failure,&nbsp;ever.&nbsp;In&nbsp;the&nbsp;most&nbsp;recent&nbsp;failure&nbsp;we&nbsp;had&nbsp;2&nbsp;machines&nbsp;split&nbsp;off,&nbsp;and&nbsp;the&nbsp;3rd&nbsp;rabbitmq&nbsp;service&nbsp;effectively&nbsp;became&nbsp;unresponsive&nbsp;entirely.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;For&nbsp;sanity&nbsp;sake,&nbsp;at&nbsp;this&nbsp;point&nbsp;we've&nbsp;backed&nbsp;down&nbsp;to&nbsp;the&nbsp;following&nbsp;configuration:&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;New-New&nbsp;Farm:&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server1:&nbsp;m1.large,&nbsp;2.8.5,&nbsp;us-west-1c&lt;/div&gt;&lt;div&gt;&amp;nbsp;&nbsp;server2:&nbsp;m1.large,&nbsp;2.8.5,&nbsp;us-west-1a&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Up&nbsp;until&nbsp;recently&nbsp;though&nbsp;I&nbsp;had&nbsp;felt&nbsp;extremely&nbsp;comfortable&nbsp;with&nbsp;RabbitMQ's&nbsp;clustering&nbsp;technology&nbsp;and&nbsp;reliability...&nbsp;now&nbsp;...&nbsp;not&nbsp;so&nbsp;much.&nbsp;Has&nbsp;anyone&nbsp;else&nbsp;seen&nbsp;similar&nbsp;behaviors?&nbsp;Is&nbsp;it&nbsp;simply&nbsp;due&nbsp;to&nbsp;the&nbsp;fact&nbsp;that&nbsp;we're&nbsp;running&nbsp;cross-zone&nbsp;now&nbsp;in&nbsp;Amazon,&nbsp;or&nbsp;is&nbsp;it&nbsp;more&nbsp;likely&nbsp;the&nbsp;3&nbsp;servers&nbsp;that&nbsp;caused&nbsp;the&nbsp;problem?&nbsp;Or&nbsp;the&nbsp;3.0.x&nbsp;upgrade?&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;--Matt&lt;/div&gt;&lt;/blockquote&gt;
</tt>
