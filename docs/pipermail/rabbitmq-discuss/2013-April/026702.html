<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] Durability and consumer acknowledgement extremely slow
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Durability%20and%20consumer%20acknowledgement%0A%20extremely%20slow&In-Reply-To=%3C51790C96.2050100%40rabbitmq.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="026670.html">
   <LINK REL="Next"  HREF="026732.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] Durability and consumer acknowledgement extremely slow</H1>
    <B>Simon MacMullen</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Durability%20and%20consumer%20acknowledgement%0A%20extremely%20slow&In-Reply-To=%3C51790C96.2050100%40rabbitmq.com%3E"
       TITLE="[rabbitmq-discuss] Durability and consumer acknowledgement extremely slow">simon at rabbitmq.com
       </A><BR>
    <I>Thu Apr 25 11:59:34 BST 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="026670.html">[rabbitmq-discuss] Durability and consumer acknowledgement	extremely slow
</A></li>
        <LI>Next message: <A HREF="026732.html">[rabbitmq-discuss] Durability and consumer acknowledgement extremely slow
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26702">[ date ]</a>
              <a href="thread.html#26702">[ thread ]</a>
              <a href="subject.html#26702">[ subject ]</a>
              <a href="author.html#26702">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi Karl. I suspect you are not really seeing a bottleneck with 
acknowledgements, but rather an optimisation in autoack mode. When you 
publish a persistent message to an empty queue with a non-blocked 
autoack consumer RabbitMQ will not persist the message to disc - there's 
no point. The message can go straight to the consumer, and then it's 
gone; it can never be requeued.

So I suspect that's the difference you're seeing. And I'm afraid 5-8k 
msg/s is roughly what I would expect for persistent messages on a 
reasonable machine.

Cheers, Simon

On 24/04/13 17:26, Karl Rieb wrote:
&gt;<i> Hi,
</I>&gt;<i>
</I>&gt;<i> I am trying to improve the message throughput for a RabbitMQ queue in an
</I>&gt;<i> Amazon cloud instance and am noticing a *significant* drop in
</I>&gt;<i> performance when enabling acknowledgements for consumer of a durable
</I>&gt;<i> queue (with persisted messages).  The real problem is that the
</I>&gt;<i> bottleneck appears to be on the rabbit node and not with the consumers,
</I>&gt;<i> so adding more consumers does not improve the throughput (or help drain
</I>&gt;<i> the queue any quicker).  As a matter of fact, adding new consumers will
</I>&gt;<i> just slow down existing consumers so everyone ends up consuming at a
</I>&gt;<i> slower rate, preventing overall throughput from changing.
</I>&gt;<i>
</I>&gt;<i> Trying to do batch acknowledgements using the Multiple flag helps a bit
</I>&gt;<i> (8k msgs/s vs 5.5k msgs/s) but not much compared to the initial drop.
</I>&gt;<i>   It is only when I turn on *auto_ack* for the consumers that I see the
</I>&gt;<i> performance shoot *way *back up and when I start seeing a linear
</I>&gt;<i> increase in throughput as I add more consumers.
</I>&gt;<i>
</I>&gt;<i> Is this expected behavior?  Is there a way to configure the rabbit node
</I>&gt;<i> so it doesn't hit this bottleneck with acknowledgements?
</I>&gt;<i>
</I>&gt;<i> Here is the sample code I'm using to test the throughput:
</I>&gt;<i>
</I>&gt;<i> Publisher:
</I>&gt;<i>
</I>&gt;<i>     #!/usr/bin/python
</I>&gt;<i>
</I>&gt;<i>     import pika
</I>&gt;<i>
</I>&gt;<i>     creds = pika.PlainCredentials('guest','guest')
</I>&gt;<i>     conn  =
</I>&gt;<i>     pika.BlockingConnection(pika.ConnectionParameters(host='10.10.1.123', credentials=creds))
</I>&gt;<i>     chan  = conn.channel()
</I>&gt;<i>
</I>&gt;<i>     while True:
</I>&gt;<i>     chan.basic_publish(exchange='simple_exchange',
</I>&gt;<i>     routing_key='simple_queue', body='',
</I>&gt;<i>     properties=pika.BasicProperties(delivery_mode=2))
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Consumer:
</I>&gt;<i>
</I>&gt;<i>       #!/usr/bin/python
</I>&gt;<i>
</I>&gt;<i>     import pika
</I>&gt;<i>
</I>&gt;<i>     def callback(chan, method, properties, body):
</I>&gt;<i>          chan.basic_ack(delivery_tag=method.delivery_tag, multiple=False)
</I>&gt;<i>
</I>&gt;<i>     creds = pika.PlainCredentials('guest','guest')
</I>&gt;<i>     conn  =
</I>&gt;<i>     pika.BlockingConnection(pika.ConnectionParameters(host='10.10.1.123', credentials=creds))
</I>&gt;<i>     chan  = conn.channel()
</I>&gt;<i>
</I>&gt;<i>     chan.basic_consume(callback, queue='simple_queue', no_ack=False)
</I>&gt;<i>     chan.basic_qos(prefetch_count=1000)
</I>&gt;<i>     chan.start_consuming()
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> I spawn multiple processes for the producers and multiple for the
</I>&gt;<i> consumer (so there is no python interpreter locking issues since each
</I>&gt;<i> runs in its own interpreter instance).  I'm using an an Amazon
</I>&gt;<i> *c1.xlarge *(8 virtual cores and &quot;high&quot; IO) Ubuntu 12.04 LTS instance
</I>&gt;<i> with RabbitMQ version 3.0.4-1 and an Amazon ephemeral disk (in
</I>&gt;<i> production we would use an EBS volume instead).  The queue is marked
</I>&gt;<i> *Durable* and my messages all use *delivery_mode* 2 (persist).
</I>&gt;<i>
</I>&gt;<i> Below are the performance numbers.  For each test I use 2 publishers
</I>&gt;<i> processes and 6 consumer processes (where 3 different machines host 2
</I>&gt;<i> consumers each).  The producers and consumers are all on *separate*
</I>&gt;<i> machines from the rabbit node.  Throughput measurements were done using
</I>&gt;<i> the RabbitMQ management UI and linux utility top.  Python was compiled
</I>&gt;<i> to pyc files before running.
</I>&gt;<i>
</I>&gt;<i> *no_ack = True:*
</I>&gt;<i>      rate = 24,000/s
</I>&gt;<i>      single consumer CPU   =  65%
</I>&gt;<i>      single publisher CPU  =  80% (flow control enabled and being enforced)
</I>&gt;<i>      (beam.smp) rabbit CPU = 400% (of 800%, 8 cores) -&gt; 0.0%wa 11.5%sy
</I>&gt;<i>
</I>&gt;<i> *no_ack = False (manual acks per message):*
</I>&gt;<i>      rate =  5,500/s
</I>&gt;<i>      single consumer CPU   =  20%
</I>&gt;<i>      single publisher CPU  =  20% (flow control enabled and being enforced)
</I>&gt;<i>      (beam.smp) rabbit CPU = 300% (of 800%, 8 cores) -&gt; 4.5%wa 10.0%sy
</I>&gt;<i> The most notable difference besides the throughput are the I/O waits
</I>&gt;<i> when ACKs are enabled (4.5% vs 0.0%).  This leads me to believe that the
</I>&gt;<i> rabbit node is being bottlenecked by performing I/O operations for ACK
</I>&gt;<i> bookkeeping.  The I/O doesn't appear to be a problem for persisting the
</I>&gt;<i> published messages since I'm *guessing* that rabbit is buffering those
</I>&gt;<i> and syncing them to disk in batches.  Does this mean the
</I>&gt;<i> acknowledgements are not also being buffered before synced with disk?
</I>&gt;<i>   Can I configure the rabbit node to change this behavior to help speed
</I>&gt;<i> up the acknowledgements?   I'm not using transactions in the example
</I>&gt;<i> code above, so I don't need any strict guarantees that ACKs were written
</I>&gt;<i> to disk before returning.
</I>&gt;<i>
</I>&gt;<i> Thanks,
</I>&gt;<i> Karl
</I>&gt;<i>
</I>&gt;<i> P.S. I wrote the same sample consumer code in Ruby to see if there was a
</I>&gt;<i> difference (in case there was a Python issue), but the numbers were
</I>&gt;<i> about the same.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> rabbitmq-discuss mailing list
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbitmq-discuss at lists.rabbitmq.com</A>
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;<i>
</I>

-- 
Simon MacMullen
RabbitMQ, VMware
</PRE>









<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="026670.html">[rabbitmq-discuss] Durability and consumer acknowledgement	extremely slow
</A></li>
	<LI>Next message: <A HREF="026732.html">[rabbitmq-discuss] Durability and consumer acknowledgement extremely slow
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26702">[ date ]</a>
              <a href="thread.html#26702">[ thread ]</a>
              <a href="subject.html#26702">[ subject ]</a>
              <a href="author.html#26702">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
