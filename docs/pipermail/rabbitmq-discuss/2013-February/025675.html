<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] Consumer Clients as Tomcat Web Applications and Work Queue Configuration
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Consumer%20Clients%20as%20Tomcat%20Web%20Applications%0A%20and%20Work%20Queue%20Configuration&In-Reply-To=%3CCAC_yGUTp4DWn8sBEBxKPo%2BfDTyoOWrDrUy99GpqSkZDSVgn1Eg%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="025551.html">
   <LINK REL="Next"  HREF="025443.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] Consumer Clients as Tomcat Web Applications and Work Queue Configuration</H1>
    <B>Kevin Behr</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Consumer%20Clients%20as%20Tomcat%20Web%20Applications%0A%20and%20Work%20Queue%20Configuration&In-Reply-To=%3CCAC_yGUTp4DWn8sBEBxKPo%2BfDTyoOWrDrUy99GpqSkZDSVgn1Eg%40mail.gmail.com%3E"
       TITLE="[rabbitmq-discuss] Consumer Clients as Tomcat Web Applications and Work Queue Configuration">behrk2 at gmail.com
       </A><BR>
    <I>Tue Feb 26 19:05:58 GMT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="025551.html">[rabbitmq-discuss] Consumer Clients as Tomcat Web Applications	and Work Queue Configuration
</A></li>
        <LI>Next message: <A HREF="025443.html">[rabbitmq-discuss] newbie question: message with delay
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#25675">[ date ]</a>
              <a href="thread.html#25675">[ thread ]</a>
              <a href="subject.html#25675">[ subject ]</a>
              <a href="author.html#25675">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Steve,

Thanks for your continued help in the matter, you've done a great job at
answering my questions.  I'm going to take your advice and run both
performance and stress tests, starting with a prefetch count of 1, and then
increasing to 2, and so on.  I think that the tests will help to highlight
the integrity of my pipeline (i.e. bottlenecks, network latency, etc).

Thanks again!

Kevin


On Mon, Feb 18, 2013 at 11:25 AM, Steve Powell &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">steve at rabbitmq.com</A>&gt; wrote:

&gt;<i> Kevin,
</I>&gt;<i> I'm glad you managed to 'solve' your problem.
</I>&gt;<i>
</I>&gt;<i> The acknowledgement (ACK) is to tell the RabbitMQ Server that the client is
</I>&gt;<i> willing to take responsibility for the message: RabbitMQ will not deliver
</I>&gt;<i> this
</I>&gt;<i> message to anyone again.  If the channel closes/stops/dissappears and a
</I>&gt;<i> message sent to it hasn't been acknowledged, or if a message is rejected
</I>&gt;<i> (with
</I>&gt;<i> requeue)  (NACKed with requeue), then RabbitMQ will attempt to deliver the
</I>&gt;<i> message again.  That may or may not be to the same consumer, channel or
</I>&gt;<i> client.  The prefetch count is a way of limiting the number of messages
</I>&gt;<i> RabbitMQ Server will allow unACKed (and unNACKed) on a channel.
</I>&gt;<i>
</I>&gt;<i> Let's say you have one consumer on one channel on each client, and you have
</I>&gt;<i> two clients, A and B. Let's also say that A and B have prefetch count set
</I>&gt;<i> to 2.
</I>&gt;<i> We'll assume that A and B are served in that order by the RabbitMQ server.
</I>&gt;<i>
</I>&gt;<i> If 5 messages arrive on the queue (m1..5) then the messages will arrive on
</I>&gt;<i> the
</I>&gt;<i> QueueingConsumer client queues on A and B as follows:
</I>&gt;<i>
</I>&gt;<i> A:  [m1, m3]
</I>&gt;<i> B:  [m2, m4]
</I>&gt;<i> and message m5 will stay in the server.
</I>&gt;<i>
</I>&gt;<i> I presume both A and B will see a message and remove it from the
</I>&gt;<i> QueueingConsumer queue:
</I>&gt;<i>
</I>&gt;<i> A:  [m3]  (processing m1)
</I>&gt;<i> B:  [m4]  (processing m2)
</I>&gt;<i>
</I>&gt;<i> When the first message is processed successfully (either m1 on A, or m2 on
</I>&gt;<i> B)
</I>&gt;<i> and the client sends ACK back to the server, then message m5 can be sent to
</I>&gt;<i> the client who ACKed first, let's say it is B:
</I>&gt;<i>
</I>&gt;<i> A:  [m3]  (processing m1)
</I>&gt;<i> B:  [m4, m5]   (not processing a message)
</I>&gt;<i>
</I>&gt;<i> But note that B might have already started processing m4 by this time (it
</I>&gt;<i> more probably would have, since the message m4 is already in the client),
</I>&gt;<i> so it would soon (or already) look like this:
</I>&gt;<i>
</I>&gt;<i> A:  [m3]  (processing m1)
</I>&gt;<i> B:  [m5]  (processing m4)
</I>&gt;<i>
</I>&gt;<i> As messages arrive on the queue in the server, they are sent to either A
</I>&gt;<i> or B
</I>&gt;<i> as they successfully (or otherwise) finish processing the previous
</I>&gt;<i> messages.
</I>&gt;<i>
</I>&gt;<i> Let's say that B finishes first again (ACKnowledging m4), but that  no more
</I>&gt;<i> messages arrive on the queue before then.  We then get into this situation:
</I>&gt;<i>
</I>&gt;<i> A:  [m3]  (processing m1)
</I>&gt;<i> B:  []  (processing m5)
</I>&gt;<i>
</I>&gt;<i> B has an empty queue.
</I>&gt;<i>
</I>&gt;<i> If B again finishes first (a fast and efficient machine, or a lightweight
</I>&gt;<i> message m5?) we could see this situation:
</I>&gt;<i>
</I>&gt;<i> A:  [m3]  (processing m1)
</I>&gt;<i> B:  []  (idle, waiting on internal queue)
</I>&gt;<i>
</I>&gt;<i> A might take a very long time to process message m1, and m3 has to wait
</I>&gt;<i> until
</I>&gt;<i> A has finished (or aborts) before it will be processed, even though B is
</I>&gt;<i> unoccupied and available.
</I>&gt;<i>
</I>&gt;<i> The effects of the prefetch count &gt;1 are therefore three-fold:
</I>&gt;<i>
</I>&gt;<i> 1) The internal client queues never hold more than prefetch-count messages,
</I>&gt;<i>    all excess messages 'back-up' in the server.
</I>&gt;<i>
</I>&gt;<i> 2) Some messages can get 'caught' behind a message that takes a long time
</I>&gt;<i> to
</I>&gt;<i>    process: waiting indefinitely, even though there may be servers ready
</I>&gt;<i> and
</I>&gt;<i>    waiting to process them.
</I>&gt;<i>
</I>&gt;<i> 3) Messages can be sent to a client while it is processing the previous
</I>&gt;<i>    message, so the network transfer time can overlap processing time,
</I>&gt;<i> reducing
</I>&gt;<i>    the time spent idle in the client when there are messages to process.
</I>&gt;<i>
</I>&gt;<i> Notice that there is a (potential) price to pay for the benefit in item 3.
</I>&gt;<i>  If
</I>&gt;<i> you reduce the time spent waiting for network transfer time while idle, you
</I>&gt;<i> run the risk of processing some messages very late indeed.
</I>&gt;<i>
</I>&gt;<i> As soon as you have multiple clients listening to the same queue you cannot
</I>&gt;<i> guarantee the order of processing of messages, whatever you set the
</I>&gt;<i> prefetch
</I>&gt;<i> counts to.
</I>&gt;<i>
</I>&gt;<i> You should be able to see what happens (can happen) with more clients, or
</I>&gt;<i> with
</I>&gt;<i> a larger (or unlimited) prefetch count.
</I>&gt;<i>
</I>&gt;<i> Whether you wish to use a setting like this very much depends upon how
</I>&gt;<i> large
</I>&gt;<i> the network latency is, what the risk of message failures are, and what the
</I>&gt;<i> rate of message arrival might be (it will doubtless vary in any case).
</I>&gt;<i>
</I>&gt;<i> A prefetch count of 2 and a variable number of clients works reasonably
</I>&gt;<i> well:
</I>&gt;<i>
</I>&gt;<i> - when order of message processing is not critical;
</I>&gt;<i> - when message processing is relatively reliable, and not too variable; and
</I>&gt;<i> - when the network transfer time is not too large per message (e.g.
</I>&gt;<i> messages are
</I>&gt;<i>   reasonably small).
</I>&gt;<i>
</I>&gt;<i> Outside of these parameters I recommend you run some careful tests.
</I>&gt;<i>
</I>&gt;<i> In fact, let me recommend you run some careful performance and stress
</I>&gt;<i> tests in
</I>&gt;<i> any case.  It is notoriously difficult to predict where the bottlenecks
</I>&gt;<i> will
</I>&gt;<i> be in any particular configuration.  These bottlenecks will move depending
</I>&gt;<i> upon load, and sometimes when the load changes (up or down).
</I>&gt;<i>
</I>&gt;<i> It may be, with low to medium message rates, that all this fuss makes no
</I>&gt;<i> difference over a prefetch count of 1.  You should check this before
</I>&gt;<i> investing
</I>&gt;<i> in complex (and potentially buggy) code. You should also ensure that you
</I>&gt;<i> really need many processing clients and that you fully understand the
</I>&gt;<i> interactions between the processing that these clients undertake -- it
</I>&gt;<i> is possible that there are mutual blocks or other serialisation effects
</I>&gt;<i> that will have an impact upon the service rates (or even success)
</I>&gt;<i> of your clients.
</I>&gt;<i>
</I>&gt;<i> I hope I have begun to answer your question.  Good luck.
</I>&gt;<i>   Steve Powell
</I>&gt;<i> [*M*: +44-7815-838-558; *H*:+44-1962-775-598]
</I>&gt;<i> *Links: SpringSource &lt;<A HREF="http://www.springsource.org/">http://www.springsource.org/</A>&gt;** **(**a division of *
</I>&gt;<i> *VMware* &lt;<A HREF="http://www.vmware.com/">http://www.vmware.com/</A>&gt;*),*
</I>&gt;<i>
</I>&gt;<i> * **Virgo* &lt;<A HREF="http://www.eclipse.org/virgo">http://www.eclipse.org/virgo</A>&gt;*, *
</I>&gt;<i> *RabbitMQ &lt;<A HREF="http://www.rabbitmq.com">http://www.rabbitmq.com</A>&gt;.*
</I>&gt;<i> *-----------------------------------------------------------------------*
</I>&gt;<i>   Good design:
</I>&gt;<i>    is innovative, useful, aesthetic;
</I>&gt;<i>    is understandable, unobtrusive, honest;
</I>&gt;<i>    is long-lasting, thorough, environmentally friendly;
</I>&gt;<i>     and *is as little design as possible.*
</I>&gt;<i> *Copyright Dieter Rams, amended March 2003; October 2009; and August 2012*
</I>&gt;<i>
</I>&gt;<i> On 12 Feb 2013, at 19:28, Kevin Behr &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">behrk2 at gmail.com</A>&gt; wrote:
</I>&gt;<i>
</I>&gt;<i> Regarding your statement above, I had not taken that into consideration.
</I>&gt;<i>  I've coded my clients to  only acknowledge upon successful processing of
</I>&gt;<i> the delivered message.  If one of our many web services or datastore
</I>&gt;<i> connections goes down, then unacknowledged messages will start piling up
</I>&gt;<i> quickly, and the messaging and queuing pipeline will essentially come to a
</I>&gt;<i> halt. I would expect this to be the correct behavior, however, as I don't
</I>&gt;<i> want to throw out messages until someone discovers that a service is down.
</I>&gt;<i>
</I>&gt;<i> As I will be using QueuingConsumer, I'm now wondering *how *I should
</I>&gt;<i> determine *what *to set my prefetch counts to.  Do you have any further
</I>&gt;<i> advice on this?  I'm still a little confused there.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20130226/a733857b/attachment.htm">http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20130226/a733857b/attachment.htm</A>&gt;
</PRE>




<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="025551.html">[rabbitmq-discuss] Consumer Clients as Tomcat Web Applications	and Work Queue Configuration
</A></li>
	<LI>Next message: <A HREF="025443.html">[rabbitmq-discuss] newbie question: message with delay
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#25675">[ date ]</a>
              <a href="thread.html#25675">[ thread ]</a>
              <a href="subject.html#25675">[ subject ]</a>
              <a href="author.html#25675">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
