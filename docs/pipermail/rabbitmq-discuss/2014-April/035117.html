<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] Possible memory leak in the management plugin
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Possible%20memory%20leak%20in%20the%20management%20plugin&In-Reply-To=%3C534521F1.9030707%40rabbitmq.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="035108.html">
   <LINK REL="Next"  HREF="035142.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] Possible memory leak in the management plugin</H1>
    <B>Simon MacMullen</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Possible%20memory%20leak%20in%20the%20management%20plugin&In-Reply-To=%3C534521F1.9030707%40rabbitmq.com%3E"
       TITLE="[rabbitmq-discuss] Possible memory leak in the management plugin">simon at rabbitmq.com
       </A><BR>
    <I>Wed Apr  9 11:33:21 BST 2014</I>
    <P><UL>
        <LI>Previous message: <A HREF="035108.html">[rabbitmq-discuss] Possible memory leak in the management plugin
</A></li>
        <LI>Next message: <A HREF="035142.html">[rabbitmq-discuss] Possible memory leak in the management plugin
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#35117">[ date ]</a>
              <a href="thread.html#35117">[ thread ]</a>
              <a href="subject.html#35117">[ subject ]</a>
              <a href="author.html#35117">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 09/04/14 06:15, Pavel wrote:
&gt;<i> Hello,
</I>
Hello. And first of all, thanks for a very detailed email!

&gt;<i> It
</I>&gt;<i> was quickly identified that mgmt_db is a culprit, so we disabled management
</I>&gt;<i> plugin and attempted to reproduce the issue in the lab.
</I>
Based on what you've said below, it looks like the fine grained stats 
are causing the problem, so if you want to re-enable mgmt you might want 
to add

{rabbitmq_management_agent, [{force_fine_statistics, false}]}

in your configuration - this will stop mgmt showing message rates but I 
think it would remove the performance issue you're seeing.

&gt;<i> Q1: There are three ets tables (as far as I understand) that have million+
</I>&gt;<i> items and keep growing rapidly (5754973, 5759070 and 5763167). What are
</I>&gt;<i> those?
</I>
Those numbers are just generated IDs, so they will be different for each 
run of the database. But in your case they are:

5754973 - aggregated_stats
5759070 - aggregated_stats_index
5763167 - old_stats

In (very) short, aggregated_stats contains all the stats for things 
which have history, aggregated_stats_index provides some alternate 
indexes to provide fast lookups into aggregated_stats, and old_stats 
retains the (N-1) version of the raw stats emitted by channels (so that 
we can get the delta to the previous one when aggregating).

All of these I would expect to see having a size that is proportional to 
the number of distinct (channel -&gt; exchange), (channel -&gt; exchange -&gt; 
queue) and (queue -&gt; channel) publish and deliver events. So with large 
fanout between channels and exchanges I would expect to see lots of 
records in these tables.

However, I would not expect to see the tables grow indefinitely. They 
should reach a stable size, and then drop back to (comparatively near) 
zero once all the channels are closed (they will still contain 
per-queue, per-exchange, per-vhost records).

So are they growing without bound? What happens when you close all the 
channels?

&gt;<i> Q2: Total memory used by ets storage (~120Mb) is far less than total memory
</I>&gt;<i> used by rabbit_mgmt_db process (~3.5Gb). Is there any way to inspect what
</I>&gt;<i> consumes the most of the memory?
</I>
That's the weird part. The vast majority of data for the mgmt DB should 
be in those ETS tables; the process memory should only include the 
database's state record (which you printed with sys:get_status/1, it's 
tiny), its stack (also tiny) and any garbage that hasn't yet been collected.

Hmm. That last one is an interesting question. What does

rabbitmqctl eval 
'erlang:garbage_collect(global:whereis_name(rabbit_mgmt_db)).'

do to your memory use?

The database will be generating heaps (sorry) of garbage in your test; 
Erlang's GC should be kicking in frequently but I wonder if somehow it 
is not.

&gt;<i> It was noticed also, that in the configuration described above (1000
</I>&gt;<i> exchanges x 1 queue each) stats events for publishing channels become very
</I>&gt;<i> big.
</I>
Yes, again they contain data for each exchange / queue the channel 
publishes to.

&gt;<i> It also seems that increasing collect_statistics_interval from default
</I>&gt;<i> 5 seconds to 60 seconds helps - memory still goes up after first iteration,
</I>&gt;<i> but then stays within ~2Gb (test was running for at least 1 hour with no
</I>&gt;<i> crash).
</I>
That makes sense.

&gt;<i> Here is the output of fprof analysis for ~5 seconds of the rabbit_mgmt_db
</I>&gt;<i> process during the test at a slower publish rate:
</I>&gt;<i> <A HREF="https://gist.github.com/maisenovich/10226881">https://gist.github.com/maisenovich/10226881</A>
</I>&gt;<i>
</I>&gt;<i> Q3: Is there any better way to profile Erlang/Rabbit? Attempts to use fprof
</I>&gt;<i> under load that is causing the crash didn't work so far.
</I>
Yeah, fprof has a huge performance impact since it is of the &quot;trace 
every function call&quot; school. I am not aware of a sampling profiler for 
Erlang I'm afraid.

&gt;<i> We are suspecting that the issue is caused by massive channel_stats events
</I>&gt;<i> which at the high enough rate causing Erlang GC to start falling behind. The
</I>&gt;<i> function invocation below (from linked profile) is particularly raising
</I>&gt;<i> questions:
</I>
That seems sort of plausible. But Erlang does not GC like Java - each 
process has its own heap and the GC takes place &quot;within&quot; that process, 
it's not a separate thread or anything, so it shouldn&#8217;t be able to fall 
behind just because a process is busy. I am still very suspicious of GC 
in your case though, forcing a manual GC as above would be very helpful.

&gt;<i> Q4: Any better theories on what might be causing the unbounded memory
</I>&gt;<i> growth? Any GC tuning you would recommend to attempt?
</I>
Not yet.

I'd like to verify that GC is the problem first, then we can look at how 
to do it better.

Cheers, Simon

-- 
Simon MacMullen
RabbitMQ, Pivotal
</PRE>












































<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="035108.html">[rabbitmq-discuss] Possible memory leak in the management plugin
</A></li>
	<LI>Next message: <A HREF="035142.html">[rabbitmq-discuss] Possible memory leak in the management plugin
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#35117">[ date ]</a>
              <a href="thread.html#35117">[ thread ]</a>
              <a href="subject.html#35117">[ subject ]</a>
              <a href="author.html#35117">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
