<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] Feature Req / Bug list
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Feature%20Req%20/%20Bug%20list&In-Reply-To=%3C874A73DF-5283-4508-8AD2-43BC53DC9196%40rabbitmq.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="030655.html">
   <LINK REL="Next"  HREF="030726.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] Feature Req / Bug list</H1>
    <B>Tim Watson</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Feature%20Req%20/%20Bug%20list&In-Reply-To=%3C874A73DF-5283-4508-8AD2-43BC53DC9196%40rabbitmq.com%3E"
       TITLE="[rabbitmq-discuss] Feature Req / Bug list">watson.timothy at gmail.com
       </A><BR>
    <I>Fri Oct  4 09:54:20 BST 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="030655.html">[rabbitmq-discuss] Feature Req / Bug list
</A></li>
        <LI>Next message: <A HREF="030726.html">[rabbitmq-discuss] Feature Req / Bug list
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#30667">[ date ]</a>
              <a href="thread.html#30667">[ thread ]</a>
              <a href="subject.html#30667">[ subject ]</a>
              <a href="author.html#30667">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 3 Oct 2013, at 22:03, Graeme N &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">graeme at sudo.ca</A>&gt; wrote:
&gt;<i> 
</I>&gt;<i> All items below were discovered while deploying 3.1.5 over the past few days. Hosts in question have 24 sandy bridge HT cores, 64GB of RAM, XFS filesystem, running on CentOS 6. Cluster is 5 nodes, with a default HA policy on all queues of exact/3/automatic-sync.
</I>&gt;<i> 
</I>
That's a very strong consistency and redundency guarantee for every queue. Do you really need such strong guarantees for all of them? There is a cost to doing ha.

&gt;<i> HA / Clustering:
</I>&gt;<i> 
</I>&gt;<i> - expected queues to be distributed evenly among cluster machines, instead got all queues on first 3 machines in the cluster, nothing on the last 2.
</I>
Distributed evenly in what regard? Randomly, or based on some metric?

&gt;<i> - expected message reads from a mirror machine for a queue to do the read i/o locally, so as to spread out workload, but it appears to always go to the host where the queue was created.
</I>
That's expected behaviour. In a master-slave configuration, writes have to go to the master. Odd though it may sound, reads from a queue involve writes, since we have to do accounting (of e.g.,  pending ACKs, position in the queue, etc), so all requests are handled by the master.

&gt;<i> - this led to a single node with ~35k active open filehandles, and 4 nodes with ~90. not an optimum distribution of read workload.
</I>
Agreed. Simon or Marthias may be able to elaborate on various things we're working on to improve workload distribution.

&gt;<i> - expected that if system a queue was created on is permanently removed (shut down and &quot;rabbitmqctl forget_cluster_node hostname&quot;'d), automatic sync would ensure there's the right number of copies replicated, but instead it just left every single queue under replicated.
</I>
That doesn't sound right. It's not automatic sync we're talking here either - that sounds like the policy isn't getting applied properly.

&gt;<i> - when a new policy is applied that defines specific replication nodes, or a number of copies using 'exact, and auto-sync is set, sometimes it just syncs the first replica and leaves any others unsynced and calls it job done. This is bad.
</I>
Can you provide us with a way to reproduce this? How did you detect that the remaining replicas were not sync'ed?

&gt;<i> - Attempted to create small per-queue policies to redistribute messages and then delete the per-queue policies, but this often leads to a inconsistent cluster state where queues continued to show as being part of a policy that was already deleted, attempt to resync, and get stuck, unable to complete or switch back to the global default policy.
</I>
Again, it would be helpful if you could help us to replicate this. 

&gt;<i> - sometimes the cluster refuses to accept any more policy commands. Have to fully shut down and restart the cluster to clear this condition.
</I>
And this. Can you provide a run down of these policies and the order in which you're trying to apply them? Also, how busy are the queues whilst the policy changes are happening? We may need to extend our test beds to reliably reproduce such problems.

&gt;<i> - sometimes policies applied to empty and inactive queues don't get correctly applied, and the queue hangs on &quot;resyncing / 100%&quot;.l
</I>
What!?

&gt;<i> this makes no sense, given the queue is empty, and requires a full cluster restart to clear.
</I>
Please provide the commands you invoked to get this to happen.

&gt;<i> - I've managed to get the cluster into an inconsistent state a /lot/ using the HA features, so it feels like they need more automated stress testing and bulletproofing.
</I>
If you can help us repoduce these errors, I can assure you that they'll get included in our integration tests!

&gt;<i> 
</I>&gt;<i> Persistent message storage:
</I>&gt;<i> 
</I>&gt;<i> - it appears as if messages are put into very small batch files on the filesystem (1-20 MB)
</I>&gt;<i> - this causes the filesystem to thrash if your IO isn't good at random IO (SATA disks) and you have lots of persistent messages (&gt;200k messages 500kB-1MB in size) that don't fit in RAM.
</I>&gt;<i> - this caused CentOS 6 kernel to kill erlang after stalling the XFS filesystem for &gt; 120s.
</I>
Iirc this is tuneable, though we don't recommend changing it. Not at u desk right now though, so I can't remember the exact details.

&gt;<i> - if a node crashes, Rabbit seems to rescan the entire on-disk datastore before continuing, instead of using some sort of checkpointing or journaling system to quickly recover from a crash.
</I>&gt;<i> - all of above should be solvable by using an existing append-only datastore like eLevelDB or Bitcask.
</I>
On our todo list already, at least for the message store index.

&gt;<i> Hopefully you guys can educate me on what I'm doing wrong in some of these scenarios, or how to mitigate some of these issues. Any issue that requires taking down and restarting the cluster to fix is especially troubling.
</I>&gt;<i> 
</I>&gt;<i> Thanks,
</I>&gt;<i> Graeme
</I>&gt;<i> 
</I>&gt;<i> _______________________________________________
</I>&gt;<i> rabbitmq-discuss mailing list
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbitmq-discuss at lists.rabbitmq.com</A>
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I></PRE>

















<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="030655.html">[rabbitmq-discuss] Feature Req / Bug list
</A></li>
	<LI>Next message: <A HREF="030726.html">[rabbitmq-discuss] Feature Req / Bug list
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#30667">[ date ]</a>
              <a href="thread.html#30667">[ thread ]</a>
              <a href="subject.html#30667">[ subject ]</a>
              <a href="author.html#30667">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
