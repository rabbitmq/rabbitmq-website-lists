<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] Crash with RabbitMQ 3.1.5
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Crash%20with%20RabbitMQ%203.1.5&In-Reply-To=%3CCANM2fuuZy1TiuC2%2B60EiEQ2L7%3Dcern3J-4iCZ%3DwbD3t3vJFczw%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="031049.html">
   <LINK REL="Next"  HREF="031059.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] Crash with RabbitMQ 3.1.5</H1>
    <B>David Harrison</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Crash%20with%20RabbitMQ%203.1.5&In-Reply-To=%3CCANM2fuuZy1TiuC2%2B60EiEQ2L7%3Dcern3J-4iCZ%3DwbD3t3vJFczw%40mail.gmail.com%3E"
       TITLE="[rabbitmq-discuss] Crash with RabbitMQ 3.1.5">dave.l.harrison at gmail.com
       </A><BR>
    <I>Wed Oct 16 16:29:01 BST 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="031049.html">[rabbitmq-discuss] Crash with RabbitMQ 3.1.5
</A></li>
        <LI>Next message: <A HREF="031059.html">[rabbitmq-discuss] Crash with RabbitMQ 3.1.5
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#31056">[ date ]</a>
              <a href="thread.html#31056">[ thread ]</a>
              <a href="subject.html#31056">[ subject ]</a>
              <a href="author.html#31056">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 17 October 2013 01:29, Tim Watson &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">tim at rabbitmq.com</A>&gt; wrote:

&gt;<i> Hello David!
</I>&gt;<i>
</I>&gt;<i>
</I>Hey Tim, thanks for replying so quickly!


&gt;<i> On 16 Oct 2013, at 15:14, David Harrison wrote:
</I>&gt;<i> &gt; Hoping someone can help me out.  We recently experienced 2 crashes with
</I>&gt;<i> our RabbitMQ cluster.  After the first crash, we moved the Mnesia
</I>&gt;<i> directories elsewhere, and started RabbitMQ again.  This got us up and
</I>&gt;<i> running.  Second time it happened, we had the original nodes plus an
</I>&gt;<i> additional 5 nodes we had added to the cluster that we were planning to
</I>&gt;<i> leave in place while shutting the old nodes down.
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> What version of rabbit are you running, and how was it installed?
</I>&gt;<i>
</I>
3.1.5, running on Ubuntu Precise, installed via deb package.


&gt;<i>
</I>&gt;<i> &gt; During the crash symptoms were as follows:
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; - Escalating (and sudden) CPU utilisation on some (but not all) nodes
</I>&gt;<i>
</I>&gt;<i> We've fixed at least one bug with that symptom in recent releases.
</I>&gt;<i>
</I>
I think 3.1.5 is the latest stable ??


&gt;<i>
</I>&gt;<i> &gt; - Increasing time to publish on queues (and specifically on a test queue
</I>&gt;<i> we have setup that exists only to test publishing and consuming from the
</I>&gt;<i> cluster hosts)
</I>&gt;<i>
</I>&gt;<i> Are there multiple publishers on the same connection/channel when this
</I>&gt;<i> happens? It wouldn't be unusual, if the server was struggling, to see flow
</I>&gt;<i> control kick in and affect publishers in this fashion.
</I>&gt;<i>
</I>
Yes in some cases there would be, for our test queue there wouldn't be --
we saw up to 10s on the test queue though


&gt;<i>
</I>&gt;<i> &gt; - Running `rabbitmqctl cluster status` gets increasingly slow (some
</I>&gt;<i> nodes eventually taking up to 10m to return with the response data - some
</I>&gt;<i> were fast and took 5s)
</I>&gt;<i>
</I>&gt;<i> Wow, 10m is amazingly slow. Can you provide log files for this period of
</I>&gt;<i> activity and problems?
</I>&gt;<i>
</I>
I'll take a look, we saw a few &quot;too many processes&quot; messages,

&quot;Generic server net_kernel terminating&quot; followed by :

** Reason for termination ==
** {system_limit,[{erlang,spawn_opt,
[inet_tcp_dist,do_setup,
[&lt;0.19.0&gt;,'<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b02.internal</A>',normal,
'<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b00.internal</A>',longnames,7000],
[link,{priority,max}]]},
{net_kernel,setup,4},
{net_kernel,handle_call,3},
{gen_server,handle_msg,5},
{proc_lib,init_p_do_apply,3}]}


=ERROR REPORT==== 15-Oct-2013::16:07:10 ===
** gen_event handler rabbit_error_logger crashed.
** Was installed in error_logger
** Last event was: {error,&lt;0.8.0&gt;,{emulator,&quot;~s~n&quot;,[&quot;Too many processes\n&quot;]}}
** When handler state == {resource,&lt;&lt;&quot;/&quot;&gt;&gt;,exchange,&lt;&lt;&quot;amq.rabbitmq.log&quot;&gt;&gt;}
** Reason == {aborted,
                 {no_exists,
                     [rabbit_topic_trie_edge,
                      {trie_edge,
                          {resource,&lt;&lt;&quot;/&quot;&gt;&gt;,exchange,&lt;&lt;&quot;amq.rabbitmq.log&quot;&gt;&gt;},
                          root,&quot;error&quot;}]}}


=ERROR REPORT==== 15-Oct-2013::16:07:10 ===
Mnesia(<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">nonode at nohost</A>): ** ERROR ** mnesia_controller got unexpected
info: {'EXIT',
&lt;0.97.0&gt;,
shutdown}

=ERROR REPORT==== 15-Oct-2013::16:11:38 ===
Mnesia('<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b00.internal</A>'): ** ERROR ** mnesia_event got
{inconsistent_database, starting_partitioned_network,
'<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b01.internal</A>'}




&gt;<i> &gt; - When trying to shut down a node, running `rabbitmqctl stop_app`
</I>&gt;<i> appears to block on epmd and doesn't return
</I>&gt;<i>
</I>&gt;<i> Again, we've fixed bugs in that area in recent releases.
</I>&gt;<i>
</I>&gt;<i> &gt; --- When that doesn't return we eventually have to ctrl-c the command
</I>&gt;<i> &gt; --- We have to issue a kill signal to rabbit to stop it
</I>&gt;<i> &gt; --- Do the same to the epmd process
</I>&gt;<i>
</I>&gt;<i> Even if you have to `kill -9' a rabbit node, you shouldn't need to kill
</I>&gt;<i> epmd. In theory at least. If that was necessary to fix the &quot;state of the
</I>&gt;<i> world&quot;, it would be indicative of a problem related to the erlang
</I>&gt;<i> distribution mechanism, but I very much doubt that's the case here.
</I>&gt;<i>
</I>&gt;<i> &gt; Config / details as follows (we use mirrored queues -- 5 hosts, all disc
</I>&gt;<i> nodes, with a global policy that all queues are mirrored &quot;ha-mode:all&quot;),
</I>&gt;<i> running on Linux
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> How many queues are we talking about here?
</I>&gt;<i>
</I>
~30


&gt;<i>
</I>&gt;<i> &gt; [
</I>&gt;<i> &gt;         {rabbit, [
</I>&gt;<i> &gt;                 {cluster_nodes, {['<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b05.internal</A>',
</I>&gt;<i> '<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b06.internal</A>','<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b07.internal</A>','<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b08.internal</A>
</I>&gt;<i> ','<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at b09.internal</A>'], disc}},
</I>&gt;<i> &gt;                 {cluster_partition_handling, pause_minority}
</I>&gt;<i>
</I>&gt;<i> Are you sure that what you're seeing is not caused by a network partition?
</I>&gt;<i> If it were, any nodes in a minority island would &quot;pause&quot;, which would
</I>&gt;<i> certainly lead to the kind of symptoms you've mentioned here, viz
</I>&gt;<i> rabbitmqctl calls not returning and so on.
</I>&gt;<i>
</I>
There was definitely a network partition, but the whole cluster nose dived
during the crash


&gt;<i>
</I>&gt;<i> &gt; The erl_crash.dump slogan error was : eheap_alloc: Cannot allocate
</I>&gt;<i> 2850821240 bytes of memory (of type &quot;old_heap&quot;).
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> That's a plain old OOM failure. Rabbit ought to start deliberately paging
</I>&gt;<i> messages to disk well before that happens, which might also explain a lot
</I>&gt;<i> of the slow/unresponsive-ness.
</I>&gt;<i>
</I>
These hosts aren't running swap, we give them a fair bit of RAM (gave them
even more now as part of a possible stop gap)


&gt;<i>
</I>&gt;<i> &gt; System version : Erlang R14B04 (erts-5.8.5) [source] [64-bit] [smp:2:2]
</I>&gt;<i> [rq:2] [async-threads:0] [kernel-poll:false]
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> I'd strongly suggest upgrading to R16B02 if you can. R14 is pretty ancient
</I>&gt;<i> and a *lot* of bug fixes have appeared in erts + OTP since then.
</I>&gt;<i>
</I>&gt;<i>
</I>ok good advice, we'll do that


&gt;<i> &gt; When I look at the Process Information it seems there's a small number
</I>&gt;<i> with ALOT of messages queued, and the rest are an order of magnitude lower:
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> That's not unusual.
</I>&gt;<i>
</I>&gt;<i> &gt; when I view the second process (first one crashes erlang on me), I see a
</I>&gt;<i> large number of sender_death events (not sure if these are common or highly
</I>&gt;<i> unusual ?)
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; {'$gen_cast',{gm,{sender_death,&lt;2710.20649.64&gt;}}}
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> Interesting - will take a look at that. If you could provide logs for the
</I>&gt;<i> participating nodes during this whole time period, that would help a lot.
</I>&gt;<i>
</I>&gt;<i> &gt; mixed in with other more regular events:
</I>&gt;<i> &gt;
</I>&gt;<i>
</I>&gt;<i> Actually, sender_death messages are not &quot;irregular&quot; as such. They're just
</I>&gt;<i> notifying the GM group members that another member (on another node) has
</I>&gt;<i> died. This is quite normal with mirrored queues, when nodes get partitioned
</I>&gt;<i> or stopped due to cluster recovery modes.
</I>&gt;<i>
</I>&gt;<i> Cheers,
</I>&gt;<i> Tim
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> rabbitmq-discuss mailing list
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbitmq-discuss at lists.rabbitmq.com</A>
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20131017/b44ef053/attachment.htm">http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20131017/b44ef053/attachment.htm</A>&gt;
</PRE>









































































<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="031049.html">[rabbitmq-discuss] Crash with RabbitMQ 3.1.5
</A></li>
	<LI>Next message: <A HREF="031059.html">[rabbitmq-discuss] Crash with RabbitMQ 3.1.5
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#31056">[ date ]</a>
              <a href="thread.html#31056">[ thread ]</a>
              <a href="subject.html#31056">[ subject ]</a>
              <a href="author.html#31056">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
