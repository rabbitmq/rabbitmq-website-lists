<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] lost message due to binding delay
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=%5Brabbitmq-discuss%5D%20lost%20message%20due%20to%20binding%20delay&In-Reply-To=AANLkTinVOvmO0dH0H5Odw5gydA__p-UT6yg11kzVjdwF%40mail.gmail.com">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="007361.html">
   <LINK REL="Next"  HREF="007394.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] lost message due to binding delay</H1>
    <B>Simon MacMullen</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=%5Brabbitmq-discuss%5D%20lost%20message%20due%20to%20binding%20delay&In-Reply-To=AANLkTinVOvmO0dH0H5Odw5gydA__p-UT6yg11kzVjdwF%40mail.gmail.com"
       TITLE="[rabbitmq-discuss] lost message due to binding delay">simon at rabbitmq.com
       </A><BR>
    <I>Wed May 26 16:42:18 BST 2010</I>
    <P><UL>
        <LI>Previous message: <A HREF="007361.html">[rabbitmq-discuss] lost message due to binding delay
</A></li>
        <LI>Next message: <A HREF="007394.html">[rabbitmq-discuss] lost message due to binding delay
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7392">[ date ]</a>
              <a href="thread.html#7392">[ thread ]</a>
              <a href="subject.html#7392">[ subject ]</a>
              <a href="author.html#7392">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi Aaron. Interesting problem! There are a few possibilities for what 
could be happening, see below...

On 21/05/10 20:57, Aaron Westendorf wrote:
&gt;<i> We tracked down an interesting bug today in a 1.7.2 cluster.  Our
</I>&gt;<i> setup is as follows:
</I>&gt;<i>
</I>&gt;<i> cluster: 4 hosts, 1 node each  (&quot;rabbit1&quot;, &quot;rabbit2&quot;, &quot;rabbit3&quot;, &quot;rabbit4&quot;)
</I>&gt;<i> clients: 2 services, &quot;serviceA&quot; and &quot;serviceB&quot;; 1 or more processes each
</I>&gt;<i>
</I>&gt;<i> In this situation both services are connected to rabbit1.  Both
</I>&gt;<i> services have a standard queue and binding setup that is built into
</I>&gt;<i> our application stack so that they can receive messages from our HTTP
</I>&gt;<i> bridge, and also receive messages that we send between services.  The
</I>&gt;<i> queues, bindings and consumers are all declared when the services
</I>&gt;<i> start, and the queues are by service name to distribute messages
</I>&gt;<i> between all instances of each service.
</I>&gt;<i>
</I>&gt;<i> The bug occurs when we use the part of our stack that allows serviceA
</I>&gt;<i> to query serviceB and receive a response.  To be sure that the
</I>&gt;<i> response ends up with the right process, each process sets up a queue
</I>&gt;<i> that resolves to its host and pid. The queue and its bindings are not
</I>&gt;<i> allocated at startup but instead on-demand when services interact.
</I>&gt;<i>
</I>&gt;<i> Our python driver is a fork of py-amqplib which uses libevent for all
</I>&gt;<i> IO and scheduling.  The driver has been in use for awhile now, though
</I>&gt;<i> it needs a lot of documentation before it is ready to be released into
</I>&gt;<i> the wild (we promise, we're working on it).  What this means is that
</I>&gt;<i> if we have multiple AMQP messages sent during the same event loop
</I>&gt;<i> cycle, the bytes reside locally in a buffer until the current cycle
</I>&gt;<i> completes.  When a write event is processed, we push as many bytes
</I>&gt;<i> into the socket buffer as possible, and in this case, likely all of
</I>&gt;<i> the bytes would be able to fit into the socket buffer.
</I>
I'm not completely sure I understand but if you're saying that your 
driver pipelines synchronous methods (e.g. sends queue.bind in the 
dialogue below before receiving queue.declare-ok from the broker) then 
that's very bad. We don't attempt to detect this in the broker (as any 
such detection would be very racy) but it violates the spec and could 
cause any sort of weird behaviour. So that's possibility #1.

&gt;<i> So, when we perform inter-service communication, the first time it
</I>&gt;<i> occurs in that process, the bytes for setting up our subscription are
</I>&gt;<i> immediately followed by the message sent to the second service.  For
</I>&gt;<i> example:
</I>&gt;<i>
</I>&gt;<i> serviceA: exchange_declare('response', 'topic')     # already exists
</I>&gt;<i> serviceA: queue_declare('serviceA.response.host.pid')
</I>&gt;<i> serviceA: queue_bind('serviceA.response.host.pid', 'response',
</I>&gt;<i> routing_key='serviceA.response.host.pid')
</I>&gt;<i> serviceA: basic_consume( 'serviceA.response.host.pid' )
</I>&gt;<i> serviceA: tx_select()
</I>&gt;<i> serviceA: basic_publish(&lt;a message to serviceB&gt;  )
</I>&gt;<i> serviceA: tx_commit()
</I>&gt;<i> serviceB: on receipt, do a DB query and send response back to serviceA
</I>&gt;<i>
</I>&gt;<i> Using our passive listener application, we can confirm that serviceB
</I>&gt;<i> writes a response with the correct routing keys, but serviceA never
</I>&gt;<i> receives it.  Subsequent messages skip everything before
</I>&gt;<i> basic_publish() and work as expected.
</I>
I note you don't talk about channels here. In AMQP the only real 
ordering guarantees are within channels. So if each service is using 
more than one channel in the dialogue above then that's possibility #2.

&gt;<i> We are unable to reproduce this bug if we're running a single Rabbit
</I>&gt;<i> node.  The reason I suspect that it's a problem with the
</I>&gt;<i> exchange-queue binding is that all of the messages are flowing, which
</I>&gt;<i> means that Rabbit is handling the queue_* and tx_* methods in the
</I>&gt;<i> order in which we expect them to be processed.  Because we're running
</I>&gt;<i> this in a cluster, it is necessary for all nodes to register the
</I>&gt;<i> binding of the exchange to the queue.  I suspect that this is an
</I>&gt;<i> asynchronous operation, and that &quot;rabbit1&quot; has not confirmed that the
</I>&gt;<i> binding is in place by the time serviceB writes its response.  We
</I>&gt;<i> don't have exact timings, but the round-trip time for the request and
</I>&gt;<i> response is between 1 and 5ms.
</I>
We have recently found and fixed a bug which could cause messages 
(internal to Rabbit, so they could be basic.publish, tx.commit or 
various other things) to overtake each other in certain (we thought) 
theoretical circumstances. So this could be possibility #3.

*However*, queue.bind is not one of the messages that could be 
overtaken. And I'm afraid binding to a queue is in fact a synchronous 
operation anyway. So the explanation as presented can't be quite right. 
And I can't immediately see any other way for something to get overtaken 
and cause the results you're seeing, although it would help to see the 
dialogue expanded to show the bit where serviceB replies.

But you might want to try compiling from hg (default branch) and see if 
that fixes your problem.

Cheers, Simon

</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="007361.html">[rabbitmq-discuss] lost message due to binding delay
</A></li>
	<LI>Next message: <A HREF="007394.html">[rabbitmq-discuss] lost message due to binding delay
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7392">[ date ]</a>
              <a href="thread.html#7392">[ thread ]</a>
              <a href="subject.html#7392">[ subject ]</a>
              <a href="author.html#7392">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
