<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures when recovering from cluster partition
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20RabbitMQ%203.1.0%20lost%20messages%20and%20autoheal%0A%20failures%20when%20recovering%20from%20cluster%20partition&In-Reply-To=%3C7F7BFE76D0F5234F9258D88C37C9EC6B151F43A4%40VALVCSMBX001PH.val.vlss.local%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="027612.html">
   <LINK REL="Next"  HREF="027294.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures when recovering from cluster partition</H1>
    <B>Maslinski, Ray</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20RabbitMQ%203.1.0%20lost%20messages%20and%20autoheal%0A%20failures%20when%20recovering%20from%20cluster%20partition&In-Reply-To=%3C7F7BFE76D0F5234F9258D88C37C9EC6B151F43A4%40VALVCSMBX001PH.val.vlss.local%3E"
       TITLE="[rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures when recovering from cluster partition">MaslinskiR at valassis.com
       </A><BR>
    <I>Fri May 31 20:04:02 BST 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="027612.html">[rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures when recovering from cluster partition
</A></li>
        <LI>Next message: <A HREF="027294.html">[rabbitmq-discuss] Amazon EC2 spurious cluster timeouts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#27618">[ date ]</a>
              <a href="thread.html#27618">[ thread ]</a>
              <a href="subject.html#27618">[ subject ]</a>
              <a href="author.html#27618">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>The net_ticktime is using the default value, and event sequence seems consistent with the associated timeout.

Roughly,

Start test

Block cluster connection at some point -&gt; trace data shows that only publish events are occurring after this happens

After a bit more than a minute, RabbitMQ server log shows lost cluster connection on both nodes, along with the death of the mirrors and promotion of slave to master.

Test continues and completes publish phase, since these appeared to be progressing normally, with no errors visible to client

Test then starts to verify that all messages were received by polling the contents of a buffer used to store incoming messages.  In a normal scenario without faults, this generally means that the first poll finds all the messages expected to be present and test completes successfully.  Otherwise, the poll rechecks for any missing messages once per second, and gives up if 30 seconds elapse without any new messages showing up (timer is reset if any messages arrived during polling window).

The slow trickle of messages occurs during the last phase, as one appears to arrive every 14 seconds and the polling loop continues to retry.  Trace data doesn't appear to log any deliver events during this stage, but console shows a slow increment in the number of unacknowledged messages that lines up with the delivery rate observed by the client.

Ray Maslinski
Senior Software Developer, Engineering
Valassis / Digital Media
Cell: 585.330.2426
<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">maslinskir at valassis.com</A>
www.valassis.com

Creating the future of intelligent media delivery to drive your greatest success

_____________________________________________________________________________

This message may include proprietary or protected information. If you are not the intended&#160;
recipient, please notify me, delete this message and do not further communicate the information&#160;
contained herein without my express consent.


-----Original Message-----
From: Simon MacMullen [mailto:<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">simon at rabbitmq.com</A>] 
Sent: Friday, May 31, 2013 12:33 PM
To: Maslinski, Ray
Cc: Discussions about RabbitMQ
Subject: Re: [rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures when recovering from cluster partition

Hi. The behaviour you describe doesn't really match what I would expect to see. In the beginning of a partition I would expect:

1) Partition starts

2) Things behave slowly for approximately net_ticktime (see
<A HREF="http://www.rabbitmq.com/partitions.html#net_ticktime">http://www.rabbitmq.com/partitions.html#net_ticktime</A>) as nodes attempt to contact each other and time out

3) On each node, Erlang decides the other nodes are down. Things speed up again, HA queues fail over. Split-brain has begun.

It sounds like you were stuck in 2) for an extended period of time. Have you changed net_ticktime?

Alternatively, when testing partitions I have see really odd behaviour by blocking network traffic in one direction only with iptables. You might want to check if you've done that by mistake.

Cheers, Simon

On 30/05/13 22:14, Maslinski, Ray wrote:
&gt;<i> Follow-up question ...
</I>&gt;<i>
</I>&gt;<i> I tried some experiments to gain some understanding of how the cluster 
</I>&gt;<i> behaved with clients attached during a network partition event.  
</I>&gt;<i> Essentially, I repeated the previous tests described below for 
</I>&gt;<i> autohealing and automatic queue synchronization, but left the cluster 
</I>&gt;<i> communications port blocked while the client test completed.
</I>&gt;<i> One oddity I noticed was that while the consumer connected to the 
</I>&gt;<i> slave appeared to receive an indication that something was amiss 
</I>&gt;<i> (client log showed a consumer cancel exception being handled by the 
</I>&gt;<i> Spring AMQP framework, and other monitoring logs appeared to show the 
</I>&gt;<i> client restarting a connection, which seems to be consistent with 
</I>&gt;<i> documentation), the consumer connected to the master seemed to remain 
</I>&gt;<i> oblivious to any possible issues.  That consumer continued to receive 
</I>&gt;<i> messages, but at an extremely slow rate (test published at 16/sec 
</I>&gt;<i> fixed rate, but the remaining consumer began to receive messages at 
</I>&gt;<i> the rate of about 1 every 14 seconds).
</I>&gt;<i>
</I>&gt;<i> Since the test client waits around for expected message deliveries 
</I>&gt;<i> with a resettable 30 second timeout, it continued to run for an 
</I>&gt;<i> extended period of time (longer than I waited around for).  In 
</I>&gt;<i> addition, the admin console showed a relatively small number of 
</I>&gt;<i> unacked messages on that server, with the unacked count increasing 
</I>&gt;<i> with each actual delivery (client should always be acknowledging in 
</I>&gt;<i> the test setup, and reported no errors).  Eventually unblocking the 
</I>&gt;<i> cluster port released a bunch of messages in a short interval (albeit 
</I>&gt;<i> with some lost, as described previously).
</I>&gt;<i>
</I>&gt;<i> I also saw  producer connections go into flow control during the 
</I>&gt;<i> outage and remain there during the slow consumer delivery (though the 
</I>&gt;<i> test had long since completed delivering all its messages).
</I>&gt;<i>
</I>&gt;<i> Does this sound like expected behavior during a partition?
</I>&gt;<i>
</I>&gt;<i> Ray Maslinski Senior Software Developer, Engineering Valassis / 
</I>&gt;<i> Digital Media Cell: 585.330.2426 <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">maslinskir at valassis.com</A> 
</I>&gt;<i> www.valassis.com
</I>&gt;<i>
</I>&gt;<i> Creating the future of intelligent media delivery to drive your 
</I>&gt;<i> greatest success
</I>&gt;<i>
</I>&gt;<i> ______________________________________________________________________
</I>&gt;<i> _______
</I>&gt;<i>
</I>&gt;<i>  This message may include proprietary or protected information. If you 
</I>&gt;<i> are not the intended recipient, please notify me, delete this message 
</I>&gt;<i> and do not further communicate the information contained herein 
</I>&gt;<i> without my express consent.
</I>&gt;<i>
</I>&gt;<i> -----Original Message----- From: Simon MacMullen 
</I>&gt;<i> [mailto:<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">simon at rabbitmq.com</A>] Sent: Monday, May 20, 2013 6:30 AM To:
</I>&gt;<i> Discussions about RabbitMQ Cc: Maslinski, Ray Subject: Re:
</I>&gt;<i> [rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures 
</I>&gt;<i> when recovering from cluster partition
</I>&gt;<i>
</I>&gt;<i> On 17/05/13 20:38, Maslinski, Ray wrote:
</I>&gt;&gt;<i> Hello,
</I>&gt;<i>
</I>&gt;<i> Hi!
</I>&gt;<i>
</I>&gt;&gt;<i> To simulate a network partition failure, I've been using iptables to 
</I>&gt;&gt;<i> temporarily block inbound and outbound access on one of the nodes to 
</I>&gt;&gt;<i> the single port configured for cluster communications through 
</I>&gt;&gt;<i> inet_dist_listen_min and inet_dist_listen_max settings (min = max). 
</I>&gt;&gt;<i> Client access is not blocked during a simulated partition fault.
</I>&gt;<i>
</I>&gt;<i> Sounds reasonable.
</I>&gt;<i>
</I>&gt;&gt;<i> I've observed two anomalies during testing that I wasn't expecting 
</I>&gt;&gt;<i> based on the documentation I've read:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> -At a sufficiently high message rate, some number of messages will be 
</I>&gt;&gt;<i> lost during the fault sequence, with the number lost tending to 
</I>&gt;&gt;<i> increase with message rate.  No indication of a send error has been 
</I>&gt;&gt;<i> observed by the client program. Based on results obtained from test 
</I>&gt;&gt;<i> logs and an independent monitor listening on trace messages from each 
</I>&gt;&gt;<i> node, it appears that as soon as the port is blocked, both nodes 
</I>&gt;&gt;<i> continue to accept published messages, but (temporarily) stop 
</I>&gt;&gt;<i> delivering messages until the cluster heartbeat failure is detected, 
</I>&gt;&gt;<i> at which point the cluster is partitioned and the slave promotes 
</I>&gt;&gt;<i> itself to become master. In the sequences I've looked at, the 
</I>&gt;&gt;<i> messages that are lost all appear to be published to the original 
</I>&gt;&gt;<i> master (and final master after a winner is selected during autoheal).  
</I>&gt;&gt;<i> Neither the start nor the end of the lost message window appear to 
</I>&gt;&gt;<i> line up with any events in the logs, other than the start occurring 
</I>&gt;&gt;<i> sometime after the port connection is blocked but before the cluster 
</I>&gt;&gt;<i> heartbeat failure is detected, and the end occurring sometime after 
</I>&gt;&gt;<i> the detection of the cluster heartbeat failure and before the 
</I>&gt;&gt;<i> detection of the partitioned cluster after the connection is 
</I>&gt;&gt;<i> unblocked.  Is message loss to be expected in this scenario?
</I>&gt;<i>
</I>&gt;<i> I would expect to see message loss in a cluster heal scenario.
</I>&gt;<i>
</I>&gt;<i> It's important to remember that a cluster partition is still a 
</I>&gt;<i> substantial problem, and the healing process involves throwing state 
</I>&gt;<i> away. Autoheal mode just means you get through this process faster, 
</I>&gt;<i> and hopefully spend much less time accepting messages that will end up 
</I>&gt;<i> being lost.
</I>&gt;<i>
</I>&gt;<i> I would expect intuitively that only messages from the losing 
</I>&gt;<i> partitions would be lost. But I am not entirely surprised if messages 
</I>&gt;<i> from the winner are lost too; there is a period after the partitions 
</I>&gt;<i> have come back together but before autoheal kicks in during which we 
</I>&gt;<i> will have multiple masters for a queue, and behaviour can be 
</I>&gt;<i> unpredictable.
</I>&gt;<i>
</I>&gt;&gt;<i> -Occasionally the autoheal loser node fails to rejoin the cluster 
</I>&gt;&gt;<i> after restart.  I don't have a lot of data points on this one since 
</I>&gt;&gt;<i> it's only happened a handful of times during overnight test 
</I>&gt;&gt;<i> iterations.  During one failure, the autoheal winner showed the log 
</I>&gt;&gt;<i> message below during recovery:
</I>&gt;<i>
</I>&gt;<i> Ah, that looks like a bug in autoheal. I think the stack trace you 
</I>&gt;<i> posted should contain enough information to fix it. Thanks.
</I>&gt;<i>
</I>&gt;<i> Cheers, Simon
</I>&gt;<i>
</I>&gt;<i> -- Simon MacMullen RabbitMQ, Pivotal
</I>&gt;<i>
</I>

--
Simon MacMullen
RabbitMQ, Pivotal
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="027612.html">[rabbitmq-discuss] RabbitMQ 3.1.0 lost messages and autoheal failures when recovering from cluster partition
</A></li>
	<LI>Next message: <A HREF="027294.html">[rabbitmq-discuss] Amazon EC2 spurious cluster timeouts
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#27618">[ date ]</a>
              <a href="thread.html#27618">[ thread ]</a>
              <a href="subject.html#27618">[ subject ]</a>
              <a href="author.html#27618">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
