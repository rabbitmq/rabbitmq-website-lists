<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] RabbitMQ failure under high load
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20RabbitMQ%20failure%20under%20high%20load&In-Reply-To=%3CCAHFJS_XLY0ajinZ1%3DKEbmB%2BA5eCr%2Bfe71MLay0O4j-6GAQyRqw%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="020858.html">
   <LINK REL="Next"  HREF="020863.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] RabbitMQ failure under high load</H1>
    <B>Micha&#322; Ki&#281;dy&#347;</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20RabbitMQ%20failure%20under%20high%20load&In-Reply-To=%3CCAHFJS_XLY0ajinZ1%3DKEbmB%2BA5eCr%2Bfe71MLay0O4j-6GAQyRqw%40mail.gmail.com%3E"
       TITLE="[rabbitmq-discuss] RabbitMQ failure under high load">michal at kiedys.net
       </A><BR>
    <I>Wed Jun 27 12:19:04 BST 2012</I>
    <P><UL>
        <LI>Previous message: <A HREF="020858.html">[rabbitmq-discuss] RabbitMQ failure under high load
</A></li>
        <LI>Next message: <A HREF="020863.html">[rabbitmq-discuss] [Q] Cluster configuration is not retained by	node after restart
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#20859">[ date ]</a>
              <a href="thread.html#20859">[ thread ]</a>
              <a href="subject.html#20859">[ subject ]</a>
              <a href="author.html#20859">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Dear Simon,

My tool uses company internal libraries, so I can not publish it.
Would you like to get more details of this test to be able to play it on
your own?

Regards,
MK

2012/6/27 Simon MacMullen &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">simon at rabbitmq.com</A>&gt;

&gt;<i> Hi Micha&#322; - please can you keep rabbitmq-discuss on CC?
</I>&gt;<i>
</I>&gt;<i> So as I said, the limit is only the point at which Rabbit stops accepting
</I>&gt;<i> new messages. In the general case this should be enough to stop further
</I>&gt;<i> memory consumption - but in your case it looks like it isn't. If you were
</I>&gt;<i> able to post your test tool in a way that would make it easy for us to run,
</I>&gt;<i> then that might be the easiest way for us to help you. At the moment we
</I>&gt;<i> just don't have enough information.
</I>&gt;<i>
</I>&gt;<i> Cheers, Simon
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> On 27/06/12 09:36, Micha&#322; Ki&#281;dy&#347; wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Simon,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> My question becomes from fact, that Rabbit can consume even more than
</I>&gt;&gt;<i> 4GB when limit is set to 1.6GB.
</I>&gt;&gt;<i> At this scenario raports usage at 2.7GB but real usage is more than 4GB.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at arch-task-mq</A> -8
</I>&gt;&gt;<i> &lt;<A HREF="http://arch-task-mq-7:55672/#**/nodes/rabbit%40arch-task-mq-8&lt;http://arch-task-mq-7:55672/#/nodes/rabbit%40arch-task-mq-8">http://arch-task-mq-7:55672/#**/nodes/rabbit%40arch-task-mq-8&lt;http://arch-task-mq-7:55672/#/nodes/rabbit%40arch-task-mq-8</A>&gt;
</I>&gt;&gt;<i> **&gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> 734 / 1024
</I>&gt;&gt;<i> 701 / 829
</I>&gt;&gt;<i> 5795 / 1048576
</I>&gt;&gt;<i> 2.7GB (?)
</I>&gt;&gt;<i> _1.6GB high watermark
</I>&gt;&gt;<i> 49.6GB
</I>&gt;&gt;<i> _4.0GB low watermark 12m 33sRAM
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> After a while kernel kills Rabbit process:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Mem-info:
</I>&gt;&gt;<i> DMA per-cpu:
</I>&gt;&gt;<i> cpu 0 hot: high 186, batch 31 used:8
</I>&gt;&gt;<i> cpu 0 cold: high 62, batch 15 used:48
</I>&gt;&gt;<i> cpu 1 hot: high 186, batch 31 used:108
</I>&gt;&gt;<i> cpu 1 cold: high 62, batch 15 used:55
</I>&gt;&gt;<i> cpu 2 hot: high 186, batch 31 used:118
</I>&gt;&gt;<i> cpu 2 cold: high 62, batch 15 used:53
</I>&gt;&gt;<i> cpu 3 hot: high 186, batch 31 used:89
</I>&gt;&gt;<i> cpu 3 cold: high 62, batch 15 used:55
</I>&gt;&gt;<i> DMA32 per-cpu: empty
</I>&gt;&gt;<i> Normal per-cpu: empty
</I>&gt;&gt;<i> HighMem per-cpu: empty
</I>&gt;&gt;<i> Free pages:       12076kB (0kB HighMem)
</I>&gt;&gt;<i> Active:0 inactive:741324 dirty:0 writeback:9 unstable:0 free:3023
</I>&gt;&gt;<i> slab:101876 mapped:3649 pagetables:2586
</I>&gt;&gt;<i> DMA free:12092kB min:8196kB low:10244kB high:12292kB active:0kB
</I>&gt;&gt;<i> inactive:2965168kB present:4202496kB pages_scanned:32 all_unreclaimable?
</I>&gt;&gt;<i> no
</I>&gt;&gt;<i> lowmem_reserve[]: 0 0 0 0
</I>&gt;&gt;<i> DMA32 free:0kB min:0kB low:0kB high:0kB active:0kB inactive:0kB
</I>&gt;&gt;<i> present:0kB pages_scanned:0 all_unreclaimable? no
</I>&gt;&gt;<i> lowmem_reserve[]: 0 0 0 0
</I>&gt;&gt;<i> Normal free:0kB min:0kB low:0kB high:0kB active:0kB inactive:0kB
</I>&gt;&gt;<i> present:0kB pages_scanned:0 all_unreclaimable? no
</I>&gt;&gt;<i> lowmem_reserve[]: 0 0 0 0
</I>&gt;&gt;<i> HighMem free:0kB min:128kB low:128kB high:128kB active:0kB inactive:0kB
</I>&gt;&gt;<i> present:0kB pages_scanned:0 all_unreclaimable? no
</I>&gt;&gt;<i> lowmem_reserve[]: 0 0 0 0
</I>&gt;&gt;<i> DMA: 172*4kB 533*8kB 170*16kB 41*32kB 11*64kB 1*128kB 1*256kB 1*512kB
</I>&gt;&gt;<i> 0*1024kB 1*2048kB 0*4096kB = 12632kB
</I>&gt;&gt;<i> DMA32: empty
</I>&gt;&gt;<i> Normal: empty
</I>&gt;&gt;<i> HighMem: empty
</I>&gt;&gt;<i> Swap cache: add 4358, delete 4243, find 0/0, race 0+0
</I>&gt;&gt;<i> Free swap  = 1031136kB
</I>&gt;&gt;<i> Total swap = 1048568kB
</I>&gt;&gt;<i> Free swap:       1031136kB
</I>&gt;&gt;<i> 1050624 pages of RAM
</I>&gt;&gt;<i> 26588 reserved pages
</I>&gt;&gt;<i> 17300 pages shared
</I>&gt;&gt;<i> 83 pages swap cached
</I>&gt;&gt;<i> Out of Memory: Kill process 2213 (rabbitmq-server) score 14598295 and
</I>&gt;&gt;<i> children.
</I>&gt;&gt;<i> Out of memory: Killed process 2227 (beam.smp).
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> This is Ok?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Regards,
</I>&gt;&gt;<i> MK
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> 2012/6/22 Simon MacMullen &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">simon at rabbitmq.com</A> &lt;mailto:<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">simon at rabbitmq.com</A>
</I>&gt;&gt;<i> &gt;&gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Hi Micha&#322;.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    This is quite vague - if we can't see the source of your test tool
</I>&gt;&gt;<i>    it's hard to see what it's actually doing.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    The server can use more memory than the high watermark; that's just
</I>&gt;&gt;<i>    the point at which it stops accepting new messages from the network.
</I>&gt;&gt;<i>    This should greatly cut the extent to which it can consume more
</I>&gt;&gt;<i>    memory, but will not eliminate it.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    There is an existing issue where the processes used by connections
</I>&gt;&gt;<i>    do not close when the connection is closed and memory use is above
</I>&gt;&gt;<i>    the watermark. When the memory use drops the processes will go.
</I>&gt;&gt;<i>    Could your test application be opening new connections?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Also, you say:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        The readers has been disconnected by the server ahead of time.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    does this mean that huge numbers of messages are building up in the
</I>&gt;&gt;<i>    server? Note that in the default configuration there is a
</I>&gt;&gt;<i>    per-message cost in memory of a hundred bytes or so even when the
</I>&gt;&gt;<i>    message has been paged out to disc, so that might explain why so
</I>&gt;&gt;<i>    much memory is being used.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    I hope this helps explain what you are seeing. But I'm not exactly
</I>&gt;&gt;<i>    sure what you are doing...
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Cheers, Simon
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    On 22/06/12 14:09, Micha&#322; Ki&#281;dy&#347; wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Hi,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Software version: 2.8.2
</I>&gt;&gt;<i>        The cluster has been stressed with 1000 writers and 100 readers.
</I>&gt;&gt;<i>        Message
</I>&gt;&gt;<i>        size is 100kB.
</I>&gt;&gt;<i>        Test configuration:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        _readers node #1_
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        test.ConnectionPerWorker=true
</I>&gt;&gt;<i>        test.WritersCount=0
</I>&gt;&gt;<i>        test.ReadersCount=33
</I>&gt;&gt;<i>        test.Durable=true
</I>&gt;&gt;<i>        test.QueuesCount=1
</I>&gt;&gt;<i>        test.AutoAck=false
</I>&gt;&gt;<i>        test.ExchangeType=direct
</I>&gt;&gt;<i>        test.QueueNamePrefix=direct
</I>&gt;&gt;<i>        test.Host=arch-task-mq-7.atm
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        _readers node #2_
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        test.ConnectionPerWorker=true
</I>&gt;&gt;<i>        test.WritersCount=0
</I>&gt;&gt;<i>        test.ReadersCount=33
</I>&gt;&gt;<i>        test.Durable=true
</I>&gt;&gt;<i>        test.QueuesCount=1
</I>&gt;&gt;<i>        test.AutoAck=false
</I>&gt;&gt;<i>        test.ExchangeType=direct
</I>&gt;&gt;<i>        test.QueueNamePrefix=direct
</I>&gt;&gt;<i>        test.Host=arch-task-mq-8.atm
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        _readers node #3_
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        test.ConnectionPerWorker=true
</I>&gt;&gt;<i>        test.WritersCount=0
</I>&gt;&gt;<i>        test.ReadersCount=33
</I>&gt;&gt;<i>        test.Durable=true
</I>&gt;&gt;<i>        test.QueuesCount=1
</I>&gt;&gt;<i>        test.AutoAck=false
</I>&gt;&gt;<i>        test.ExchangeType=direct
</I>&gt;&gt;<i>        test.QueueNamePrefix=direct
</I>&gt;&gt;<i>        test.Host=arch-task-mq-8.atm
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        _writers node #4_
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        test.ConnectionPerWorker=true
</I>&gt;&gt;<i>        test.WritersCount=333
</I>&gt;&gt;<i>        test.ReadersCount=0
</I>&gt;&gt;<i>        test.Durable=true
</I>&gt;&gt;<i>        test.QueuesCount=1
</I>&gt;&gt;<i>        test.AutoAck=false
</I>&gt;&gt;<i>        test.ExchangeType=direct
</I>&gt;&gt;<i>        test.QueueNamePrefix=direct
</I>&gt;&gt;<i>        test.BodySize=102400
</I>&gt;&gt;<i>        # available units: s(seconds), m(minutes), h(hours) d(days)
</I>&gt;&gt;<i>        test.TestDuration=3h
</I>&gt;&gt;<i>        test.Host=arch-task-mq-8.atm
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        writers node #5
</I>&gt;&gt;<i>        test.ConnectionPerWorker=true
</I>&gt;&gt;<i>        test.WritersCount=333
</I>&gt;&gt;<i>        test.ReadersCount=0
</I>&gt;&gt;<i>        test.Durable=true
</I>&gt;&gt;<i>        test.QueuesCount=1
</I>&gt;&gt;<i>        test.AutoAck=false
</I>&gt;&gt;<i>        test.ExchangeType=direct
</I>&gt;&gt;<i>        test.QueueNamePrefix=direct
</I>&gt;&gt;<i>        test.BodySize=102400
</I>&gt;&gt;<i>        # available units: s(seconds), m(minutes), h(hours) d(days)
</I>&gt;&gt;<i>        test.TestDuration=3h
</I>&gt;&gt;<i>        test.Host=arch-task-mq-7.atm
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        writers node #6
</I>&gt;&gt;<i>        test.ConnectionPerWorker=true
</I>&gt;&gt;<i>        test.WritersCount=334
</I>&gt;&gt;<i>        test.ReadersCount=0
</I>&gt;&gt;<i>        test.Durable=true
</I>&gt;&gt;<i>        test.QueuesCount=1
</I>&gt;&gt;<i>        test.AutoAck=false
</I>&gt;&gt;<i>        test.ExchangeType=direct
</I>&gt;&gt;<i>        test.QueueNamePrefix=direct
</I>&gt;&gt;<i>        test.BodySize=102400
</I>&gt;&gt;<i>        # available units: s(seconds), m(minutes), h(hours) d(days)
</I>&gt;&gt;<i>        test.TestDuration=3h
</I>&gt;&gt;<i>        test.Host=arch-task-mq-8.atm
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        _Actual tests state:_
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Running worker-1000w-100r-100kB
</I>&gt;&gt;<i>        Preparing tests on arch-task-mq-1
</I>&gt;&gt;<i>        Preparing tests on arch-task-mq-2
</I>&gt;&gt;<i>        Preparing tests on arch-task-mq-3
</I>&gt;&gt;<i>        Preparing tests on arch-task-mq-4
</I>&gt;&gt;<i>        Preparing tests on arch-task-mq-5
</I>&gt;&gt;<i>        Preparing tests on arch-task-mq-6
</I>&gt;&gt;<i>        Preparations done, starting testing procedure
</I>&gt;&gt;<i>        Start tests on arch-task-mq-1
</I>&gt;&gt;<i>        Start tests on arch-task-mq-2
</I>&gt;&gt;<i>        Start tests on arch-task-mq-3
</I>&gt;&gt;<i>        Start tests on arch-task-mq-4
</I>&gt;&gt;<i>        Start tests on arch-task-mq-5
</I>&gt;&gt;<i>        Start tests on arch-task-mq-6
</I>&gt;&gt;<i>        Waiting for tests to finish
</I>&gt;&gt;<i>        Tests done on arch-task-mq-5
</I>&gt;&gt;<i>        Tests done on arch-task-mq-6
</I>&gt;&gt;<i>        Tests done on arch-task-mq-4
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        The readers has been disconnected by the server ahead of time.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        _Actual cluster state (data from Management Plugin view):_
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Name                   File descriptors (?)           Socket
</I>&gt;&gt;<i>        descriptors
</I>&gt;&gt;<i>        (?)           Erlang processes      Memory
</I>&gt;&gt;<i>  Disk
</I>&gt;&gt;<i>        space     Uptime     Type
</I>&gt;&gt;<i>                                            (used / available)
</I>&gt;&gt;<i>           (used
</I>&gt;&gt;<i>        / available)                  (used / available)
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabit at arch-task-mq-7</A>    392 / 1024                     334 / 829
</I>&gt;&gt;<i>                              2885 / 1048576        540.2MB
</I>&gt;&gt;<i>          49.6GB          21h 14m  Disc Stats *
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>          1.6GB high watermark 4.0GB low watermark
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at arch-task-mq-8</A>  692 / 1024                     668 / 829
</I>&gt;&gt;<i>                               5522 / 1048576        1.8GB (?)
</I>&gt;&gt;<i>          46.1GB          21h 16m  RAM
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>          1.6GB high watermark 4.0GB low watermark
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Number of processes is growing all the time even though no
</I>&gt;&gt;<i>        messages are
</I>&gt;&gt;<i>        not published or received.
</I>&gt;&gt;<i>        All publishers has been blocked. After some time I killed the
</I>&gt;&gt;<i>        publisher processes, but RabbitMQ still sees them as connected and
</I>&gt;&gt;<i>        blocked. :)
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Some logs:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">mkiedys at arch-task-mq-8</A>:/var/__**log/rabbitmq$ cat
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at arch-task-mq-8.log</A>
</I>&gt;&gt;<i>        |grep vm_memory_high|tail -n 20
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1709148224
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2135174984
</I>&gt;&gt;<i>        &lt;tel:2135174984&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1593121728
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2043534608
</I>&gt;&gt;<i>        &lt;tel:2043534608&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1681947128
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2088225952
</I>&gt;&gt;<i>        &lt;tel:2088225952&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1710494800
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2208875080
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1713902032
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2122564032
</I>&gt;&gt;<i>        &lt;tel:2122564032&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1663616264
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2098909664
</I>&gt;&gt;<i>        &lt;tel:2098909664&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1712666136
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2088814360
</I>&gt;&gt;<i>        &lt;tel:2088814360&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1640273568
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2116966952
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1715305176
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2186572648
</I>&gt;&gt;<i>        &lt;tel:2186572648&gt; allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        vm_memory_high_watermark clear. Memory used:1716620504
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>        vm_memory_high_watermark set. Memory used:2180898440
</I>&gt;&gt;<i>        allowed:1717986918
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">mkiedys at arch-task-mq-8</A>:/var/__**log/rabbitmq$ cat
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbit at arch-task-mq-8.log</A>
</I>&gt;&gt;<i>        |grep vm_memory_high|wc -l
</I>&gt;&gt;<i>        2935
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Why does the server consumes more memory than 1.6GB limit?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        Regards,
</I>&gt;&gt;<i>        MK
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        ______________________________**___________________
</I>&gt;&gt;<i>        rabbitmq-discuss mailing list
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbitmq-discuss at lists.__rabbi</A>**tmq.com &lt;<A HREF="http://rabbitmq.com">http://rabbitmq.com</A>&gt;
</I>&gt;&gt;<i>        &lt;mailto:rabbitmq-discuss@**lists.rabbitmq.com&lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbitmq-discuss at lists.rabbitmq.com</A>&gt;
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i>        <A HREF="https://lists.rabbitmq.com/__**cgi-bin/mailman/listinfo/__**">https://lists.rabbitmq.com/__**cgi-bin/mailman/listinfo/__**</A>
</I>&gt;&gt;<i> rabbitmq-discuss&lt;<A HREF="https://lists.rabbitmq.com/__cgi-bin/mailman/listinfo/__rabbitmq-discuss">https://lists.rabbitmq.com/__cgi-bin/mailman/listinfo/__rabbitmq-discuss</A>&gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>        &lt;<A HREF="https://lists.rabbitmq.com/**cgi-bin/mailman/listinfo/**">https://lists.rabbitmq.com/**cgi-bin/mailman/listinfo/**</A>
</I>&gt;&gt;<i> rabbitmq-discuss&lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>&gt;
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    --
</I>&gt;&gt;<i>    Simon MacMullen
</I>&gt;&gt;<i>    RabbitMQ, VMware
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i> --
</I>&gt;<i> Simon MacMullen
</I>&gt;<i> RabbitMQ, VMware
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20120627/f6acb0b7/attachment.htm">http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20120627/f6acb0b7/attachment.htm</A>&gt;
</PRE>























































<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="020858.html">[rabbitmq-discuss] RabbitMQ failure under high load
</A></li>
	<LI>Next message: <A HREF="020863.html">[rabbitmq-discuss] [Q] Cluster configuration is not retained by	node after restart
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#20859">[ date ]</a>
              <a href="thread.html#20859">[ thread ]</a>
              <a href="subject.html#20859">[ subject ]</a>
              <a href="author.html#20859">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
