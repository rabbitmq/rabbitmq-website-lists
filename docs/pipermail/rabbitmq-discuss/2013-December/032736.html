<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [rabbitmq-discuss] Lower delivery rate than publish rate - why?
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Lower%20delivery%20rate%20than%20publish%20rate%20-%20why%3F&In-Reply-To=%3CCAMcAHLXioBFJV0G2M63myb-z92%2BaW0YJcKFJT13bH_tuatbttw%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="032734.html">
   <LINK REL="Next"  HREF="032522.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[rabbitmq-discuss] Lower delivery rate than publish rate - why?</H1>
    <B>Alvaro Videla</B> 
    <A HREF="mailto:rabbitmq-discuss%40lists.rabbitmq.com?Subject=Re%3A%20%5Brabbitmq-discuss%5D%20Lower%20delivery%20rate%20than%20publish%20rate%20-%20why%3F&In-Reply-To=%3CCAMcAHLXioBFJV0G2M63myb-z92%2BaW0YJcKFJT13bH_tuatbttw%40mail.gmail.com%3E"
       TITLE="[rabbitmq-discuss] Lower delivery rate than publish rate - why?">videlalvaro at gmail.com
       </A><BR>
    <I>Sat Dec 21 21:10:55 GMT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="032734.html">[rabbitmq-discuss] Lower delivery rate than publish rate - why?
</A></li>
        <LI>Next message: <A HREF="032522.html">[rabbitmq-discuss] ant build SimpleAmqpClient
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#32736">[ date ]</a>
              <a href="thread.html#32736">[ thread ]</a>
              <a href="subject.html#32736">[ subject ]</a>
              <a href="author.html#32736">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi,

RabbitMQ will always prioritise sending messages to consumers that are
ready. So a slow consumer won't block others from receiving messages,
provided there's a basic.qos set, ie: not unlimited.

There's a better explanation of consumers ready here:
<A HREF="http://www.rabbitmq.com/consumer-priority.html">http://www.rabbitmq.com/consumer-priority.html</A>

Regards,

Alvaro


On Sat, Dec 21, 2013 at 6:02 PM, MikeTempleman &lt;<A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">mike at meshfire.com</A>&gt; wrote:

&gt;<i> We are still seeing the cycling.
</I>&gt;<i>
</I>&gt;<i> I wanted to ask if the round-robing scheduling could be a factor.
</I>&gt;<i>
</I>&gt;<i> For example, if I have 4 servers with 5 channels each on the same topic
</I>&gt;<i> queue. Each channel has a prefetch setting of 50. When a channel consumer
</I>&gt;<i> becomes blocked (slow db operation, overloaded with other requests, etc.),
</I>&gt;<i> does that block delivery of messages to the other channels? If that
</I>&gt;<i> delivery to fill the prefetch cache is delayed, is there any impact on the
</I>&gt;<i> client (i.e. does it become blocked)?
</I>&gt;<i>
</I>&gt;<i> iow, is there a way for all channels on all servers to become blocked in a
</I>&gt;<i> high load situation? Even if their prefetch caches are full?
</I>&gt;<i>
</I>&gt;<i> --
</I>&gt;<i>
</I>&gt;<i> *Mike Templeman*
</I>&gt;<i> *Head of Development*
</I>&gt;<i>
</I>&gt;<i> T: @talkingfrog1950 &lt;<A HREF="http://twitter.com/missdestructo">http://twitter.com/missdestructo</A>&gt;
</I>&gt;<i> T: @Meshfire &lt;<A HREF="http://twitter.com/meshfire">http://twitter.com/meshfire</A>&gt;
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> On Wed, Dec 18, 2013 at 10:32 AM, Mike Templeman &lt;[hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32259&amp;i=0">http://user/SendEmail.jtp?type=node&amp;node=32259&amp;i=0</A>&gt;
</I>&gt;<i> &gt; wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Well, multi-ack didn't help very much. We can see some, but not enough to
</I>&gt;&gt;<i> matter.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> We cannot use auto-ack because consumers (multiple/server) die
</I>&gt;&gt;<i> unexpectedly as the app servers are autoscaled. We have not built a fully
</I>&gt;&gt;<i> separated service yet (too hard to debug on development machines right
</I>&gt;&gt;<i> now). But could Publisher confirms resolve the issue of servers dying with
</I>&gt;&gt;<i> n messages in their prefetch buffers?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> --
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> *Mike Templeman *
</I>&gt;&gt;<i> *Head of Development*
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> T: @talkingfrog1950 &lt;<A HREF="http://twitter.com/missdestructo">http://twitter.com/missdestructo</A>&gt;
</I>&gt;&gt;<i> T: @Meshfire &lt;<A HREF="http://twitter.com/meshfire">http://twitter.com/meshfire</A>&gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On Sun, Dec 15, 2013 at 5:02 AM, Alvaro Videla-2 [via RabbitMQ] &lt;[hidden
</I>&gt;&gt;<i> email] &lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32259&amp;i=1">http://user/SendEmail.jtp?type=node&amp;node=32259&amp;i=1</A>&gt;&gt; wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Hi Mike,
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Yes, RabbitMQ queues are designed for fast delivery of messages and for
</I>&gt;&gt;&gt;<i> being as empty as possible, as that blog post explains.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Another interesting blog post, about consumer strategies and basic.qos
</I>&gt;&gt;&gt;<i> settings is this one:
</I>&gt;&gt;&gt;<i> <A HREF="http://www.rabbitmq.com/blog/2012/05/11/some-queuing-theory-throughput-latency-and-bandwidth/#more-276">http://www.rabbitmq.com/blog/2012/05/11/some-queuing-theory-throughput-latency-and-bandwidth/#more-276</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> re multi ack: yes, that might help.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Regards,
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Alvaro
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> On Sat, Dec 14, 2013 at 2:15 AM, MikeTempleman &lt;[hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32095&amp;i=0">http://user/SendEmail.jtp?type=node&amp;node=32095&amp;i=0</A>&gt;
</I>&gt;&gt;&gt;<i> &gt; wrote:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> I realized that was a bad interpretation. Sorry. The exchange is just
</I>&gt;&gt;&gt;&gt;<i> successfully routing all the messages to the target queues.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> After reading a number of posts and this blog entry (
</I>&gt;&gt;&gt;&gt;<i> <A HREF="http://www.rabbitmq.com/blog/2011/09/24/sizing-your-rabbits/">http://www.rabbitmq.com/blog/2011/09/24/sizing-your-rabbits/</A>) I wonder
</I>&gt;&gt;&gt;&gt;<i> if the issue is that each message is ack'd. It seemed that the issue
</I>&gt;&gt;&gt;&gt;<i> occurred when I had a large backlog in the queues. When Rabbit is empty,
</I>&gt;&gt;&gt;&gt;<i> performance is fine. When the consumers tried to run at much higher speeds,
</I>&gt;&gt;&gt;&gt;<i> we encountered this cycling.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> We have run a brief test with no-ack, not on production, and the
</I>&gt;&gt;&gt;&gt;<i> performance is excellent even under load. But this is not a viable solution
</I>&gt;&gt;&gt;&gt;<i> (appservers crash, autoscaling shuts servers down that have prefetched
</I>&gt;&gt;&gt;&gt;<i> messages and are still connected to rabbit) without a full redesign.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Assuming each queue is only one thread (I assume it handles both
</I>&gt;&gt;&gt;&gt;<i> receipt, delivery, and ack cleanup) then I can understand what might happen
</I>&gt;&gt;&gt;&gt;<i> when the consumers generate ~500 acks/s while new messages are coming in at
</I>&gt;&gt;&gt;&gt;<i> a low 50-100/s rate on a specific queue. I will move out some events that
</I>&gt;&gt;&gt;&gt;<i> tend to generate peaks into their own queue and accept that queue
</I>&gt;&gt;&gt;&gt;<i> processing more slowly. As for separating the real worker queue, I suppose
</I>&gt;&gt;&gt;&gt;<i> I could create 2 or so static queues to divide the load up.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> So what I think I can do is:
</I>&gt;&gt;&gt;&gt;<i> 1. bump the default tcp buffer from 128KB to around 10MB. The added
</I>&gt;&gt;&gt;&gt;<i> buffering may help a little
</I>&gt;&gt;&gt;&gt;<i> 2. see if I can find out how to set the multiple ack flag. We are using
</I>&gt;&gt;&gt;&gt;<i> Grails so maybe that is just creating a custom bean. I don't know
</I>&gt;&gt;&gt;&gt;<i> 3. create a couple of queues for lower-priority events specifically
</I>&gt;&gt;&gt;&gt;<i> events chosen to be less time critical.
</I>&gt;&gt;&gt;&gt;<i> 4. if all that doesn't work then probably create 4 queues for the high
</I>&gt;&gt;&gt;&gt;<i> priority events, randomly publish to those queues, and put consumers for
</I>&gt;&gt;&gt;&gt;<i> each queue.
</I>&gt;&gt;&gt;&gt;<i> 5. Also, upgrade the server to the latest version.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Mike Templeman
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> --
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> *Mike Templeman*
</I>&gt;&gt;&gt;&gt;<i> *Head of Development*
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> T: @talkingfrog1950 &lt;<A HREF="http://twitter.com/missdestructo">http://twitter.com/missdestructo</A>&gt;
</I>&gt;&gt;&gt;&gt;<i> T: @Meshfire &lt;<A HREF="http://twitter.com/meshfire">http://twitter.com/meshfire</A>&gt;
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> On Fri, Dec 13, 2013 at 1:42 PM, Mike Templeman &lt;[hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=0">http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=0</A>&gt;
</I>&gt;&gt;&gt;&gt;<i> &gt; wrote:
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> I noticed something else very odd.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Currently, one queue has 43,000 messages backed up in its queue. But
</I>&gt;&gt;&gt;&gt;&gt;<i> when I look at the exchange (there is only one exchange) I see that the
</I>&gt;&gt;&gt;&gt;&gt;<i> message rate in exactly matches the message rate out.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> With such a huge backlog, why would that be? I would have thought that
</I>&gt;&gt;&gt;&gt;&gt;<i> the consumers (there are 16 total distributed across 4 systems for that
</I>&gt;&gt;&gt;&gt;&gt;<i> queue with a prefetch of 100) would run at a much higher steady state.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> This exchange also seems to cycle regularly. It appears to run from a
</I>&gt;&gt;&gt;&gt;&gt;<i> low of around 60/s in and out to 500+ a second in and out.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>  --
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> *Mike Templeman*
</I>&gt;&gt;&gt;&gt;&gt;<i> *Head of Development*
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> T: @talkingfrog1950 &lt;<A HREF="http://twitter.com/missdestructo">http://twitter.com/missdestructo</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;<i> T: @Meshfire &lt;<A HREF="http://twitter.com/meshfire">http://twitter.com/meshfire</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> On Fri, Dec 13, 2013 at 10:40 AM, Mike Templeman &lt;[hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=1">http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=1</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;<i> &gt; wrote:
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> Also, from observing the Connections screen on the web UI shows that
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> no flow control has been recently turned on for any of the four current
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> connections (four app servers).
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> --
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> *Mike Templeman *
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> *Head of Development*
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> T: @talkingfrog1950 &lt;<A HREF="http://twitter.com/missdestructo">http://twitter.com/missdestructo</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> T: @Meshfire &lt;<A HREF="http://twitter.com/meshfire">http://twitter.com/meshfire</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> On Fri, Dec 13, 2013 at 10:17 AM, Mike Templeman &lt;[hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=2">http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=2</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; wrote:
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Hi Alvaro
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> I would be more than happy to provide logs. But all they have in
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> them is connection and shutdown information. Nothing more. I have just
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> enabled tracing on the vhost and will send the logs shortly. We encounter
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> this issue when under load every day now.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Let me tell you our architecture and deployment:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> rabbitMQ:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - m1.large ec2 instance. Version: RabbitMQ 3.1.5,  Erlang R14B04
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - 23 queues (transaction and direct)
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - 3 exchanges used, two fanout and one topic exchange
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - topic exchange
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - Topic exchange overview is attached.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - 46 total channels.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> AppServers
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - m1.large tomcat servers running grails application
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - 2-7 servers at any one time.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - Consume + publish
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - On busy queues, each server has 16 consumers with prefetch at
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    100
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - message sizes on busy queues are ~4KB.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    - publishing rates on busiest queue ranges from 16/s to &gt;100/s.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>    (We need to be able to support 1000/s).
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Each AppServer connects to a sharded mongodb cluster of 3 shards.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Our first suspicion was that something in Mongo or AWS was causing the
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> periodic delay but AWS techs looked into our volume use and said we were
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> only use 25% of available bandwidth.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> At this moment, we have a modest publish rate (~50-60/s) but a
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> backlog of 50,000 messages for the queue &quot;user&quot;. You can see a 10 minute
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> snapshot of the queue and see the cycling.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> I turned on tracing but the results don't seem to becoming into the
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> log. Is there another way to enable reporting of flow control?
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Mike Templeman
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>  --
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> *Mike Templeman*
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> *Head of Development*
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> T: @talkingfrog1950 &lt;<A HREF="http://twitter.com/missdestructo">http://twitter.com/missdestructo</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> T: @Meshfire &lt;<A HREF="http://twitter.com/meshfire">http://twitter.com/meshfire</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> On Fri, Dec 13, 2013 at 6:03 AM, Alvaro Videla-2 [via RabbitMQ] &lt;[hidden
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> email] &lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=3">http://user/SendEmail.jtp?type=node&amp;node=32089&amp;i=3</A>&gt;&gt; wrote:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Mike,
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Would you be able to provide information more information to help
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> us
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> debug the problem?
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Tim (from the rabbitmq team) requested more info in order to try to
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> find answers for this.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> For example, when consumption drops to zero, are there any logs on
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> the
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> rabbitmq server that might tell of a flow control mechanism being
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> activated?
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Regards,
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Alvaro
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> On Fri, Dec 13, 2013 at 2:19 AM, MikeTempleman &lt;[hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32063&amp;i=0">http://user/SendEmail.jtp?type=node&amp;node=32063&amp;i=0</A>&gt;&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> wrote:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; Tyson
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; Did you ever find an answer to this question? We are encountering
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> virtually
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; the exact same problem.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; We have a variable number of servers setup as producers and
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> consumers and
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; see our throughput drop to zero on a periodic basis. This is most
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> severe
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; when there are a few hundred thousand messages on rabbit.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; Did you just drop Rabbit? Ours is running on an m1.large instance
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> with RAID0
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; ephemeral drives, so size and performance of the disk subsystem
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> is not an
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; issue (we are still in beta). We have spent untold hours tuning
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> our sharded
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; mongodb subsystem only to find out that it is only being 25%
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> utilized (at
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; least it will be blazing fast if we ever figure this out).
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; --
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; View this message in context:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32040.html">http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32040.html</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; Sent from the RabbitMQ mailing list archive at Nabble.com.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; _______________________________________________
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; rabbitmq-discuss mailing list
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt; [hidden email]&lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32063&amp;i=1">http://user/SendEmail.jtp?type=node&amp;node=32063&amp;i=1</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> rabbitmq-discuss mailing list
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> [hidden email] &lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32063&amp;i=2">http://user/SendEmail.jtp?type=node&amp;node=32063&amp;i=2</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> ------------------------------
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>  If you reply to this email, your message will be added to the
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> discussion below:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32063.html">http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32063.html</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>  To unsubscribe from Lower delivery rate than publish rate - why?, click
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> here.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> NAML&lt;<A HREF="http://rabbitmq.1065348.n5.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&amp;id=instant_html%21nabble%3Aemail.naml&amp;base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&amp;breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml">http://rabbitmq.1065348.n5.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&amp;id=instant_html%21nabble%3Aemail.naml&amp;base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&amp;breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml</A>&gt;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> ------------------------------
</I>&gt;&gt;&gt;&gt;<i> View this message in context: Re: Lower delivery rate than publish
</I>&gt;&gt;&gt;&gt;<i> rate - why?&lt;<A HREF="http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32089.html">http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32089.html</A>&gt;
</I>&gt;&gt;&gt;&gt;<i>  Sent from the RabbitMQ mailing list archive&lt;<A HREF="http://rabbitmq.1065348.n5.nabble.com/">http://rabbitmq.1065348.n5.nabble.com/</A>&gt;at Nabble.com.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;&gt;<i> rabbitmq-discuss mailing list
</I>&gt;&gt;&gt;&gt;<i> [hidden email] &lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32095&amp;i=1">http://user/SendEmail.jtp?type=node&amp;node=32095&amp;i=1</A>&gt;
</I>&gt;&gt;&gt;&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;<i> rabbitmq-discuss mailing list
</I>&gt;&gt;&gt;<i> [hidden email] &lt;<A HREF="http://user/SendEmail.jtp?type=node&amp;node=32095&amp;i=2">http://user/SendEmail.jtp?type=node&amp;node=32095&amp;i=2</A>&gt;
</I>&gt;&gt;&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> ------------------------------
</I>&gt;&gt;&gt;<i>  If you reply to this email, your message will be added to the
</I>&gt;&gt;&gt;<i> discussion below:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> <A HREF="http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32095.html">http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32095.html</A>
</I>&gt;&gt;&gt;<i>  To unsubscribe from Lower delivery rate than publish rate - why?, click
</I>&gt;&gt;&gt;<i> here.
</I>&gt;&gt;&gt;<i> NAML&lt;<A HREF="http://rabbitmq.1065348.n5.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&amp;id=instant_html%21nabble%3Aemail.naml&amp;base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&amp;breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml">http://rabbitmq.1065348.n5.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&amp;id=instant_html%21nabble%3Aemail.naml&amp;base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&amp;breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml</A>&gt;
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ------------------------------
</I>&gt;<i> View this message in context: Re: Lower delivery rate than publish rate -
</I>&gt;<i> why?&lt;<A HREF="http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32259.html">http://rabbitmq.1065348.n5.nabble.com/Lower-delivery-rate-than-publish-rate-why-tp29247p32259.html</A>&gt;
</I>&gt;<i> Sent from the RabbitMQ mailing list archive&lt;<A HREF="http://rabbitmq.1065348.n5.nabble.com/">http://rabbitmq.1065348.n5.nabble.com/</A>&gt;at Nabble.com.
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> rabbitmq-discuss mailing list
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">rabbitmq-discuss at lists.rabbitmq.com</A>
</I>&gt;<i> <A HREF="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss</A>
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20131221/32bc60a1/attachment.html">http://lists.rabbitmq.com/pipermail/rabbitmq-discuss/attachments/20131221/32bc60a1/attachment.html</A>&gt;
</PRE>





<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="032734.html">[rabbitmq-discuss] Lower delivery rate than publish rate - why?
</A></li>
	<LI>Next message: <A HREF="032522.html">[rabbitmq-discuss] ant build SimpleAmqpClient
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#32736">[ date ]</a>
              <a href="thread.html#32736">[ thread ]</a>
              <a href="subject.html#32736">[ subject ]</a>
              <a href="author.html#32736">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss">More information about the rabbitmq-discuss
mailing list</a><br>
</body></html>
